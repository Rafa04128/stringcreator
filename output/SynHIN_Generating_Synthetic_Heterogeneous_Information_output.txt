SynHIN Generating Synthetic Heterogeneous Information Network for Explainable AI MingYi Hong 1 YiHsiang Huang 2 YouChen Teng 2 ChihYu Wang 3 Che Lin 124 1 Graduate Program of Data Science National Taiwan University and Academia Sinica Taiwan 2 Graduate Institute of Communication Engineering National Taiwan University Taiwan 3 Research Center for Information Technology Innovation Academia Sinica Taiwan 4 Department of Electrical Engineering National Taiwan University Taiwan Abstract Graph Neural Networks GNNs excel in various domains from detecting ecommerce spam to social network classifi cation problems However the lack of public graph datasets hampers research progress particularly in heterogeneous in formation networks HIN The demand for datasets for fair HIN comparisons is growing due to advancements in GNN interpretation models In response we propose SynHIN a unique method for generating synthetic heterogeneous in formation networks SynHIN identifies motifs in realworld datasets summarizes graph statistics and constructs a syn thetic network Our approach utilizes InCluster and Out Cluster Merge modules to build the synthetic HIN from pri mary motif clusters After InOurCluster mergers and a post pruning process fitting the real dataset constraints we en sure the synthetic graph statistics align closely with the refer ence one SynHIN generates a synthetic heterogeneous graph dataset for node classification tasks using the primary mo tif as the explanation ground truth It can adapt and address the lack of heterogeneous graph datasets and motif ground truths proving beneficial for assessing heterogeneous graph neural network explainers We further present a benchmark dataset for future heterogeneous graph explainer model re search Our work marks a significant step towards explainable AI in HGNNs 1 Introduction Graph neural networks GNNs have demonstrated impres sive performance in various domains such as community analysis Shchur and Gunnemann 2019 chemical bond analysis Stokes et al 2020 and recommendation systems Cui et al 2020 They effectively address node classifica tion and link prediction challenges within graphs Chami et al 2022 However compared to other machine learning domains like natural language processing NLP and com puter vision CV the available public datasets for GNNs are limited The scarcity of public datasets is especially notice able in heterogeneous information network graph datasets making it more challenging for GNN models to generalize and prone to overfitting due to the small number of avail able datasets Palowitch et al 2022 Many researches have been focused on enhancing the interpretability of GNNs The first two authors contributed equally to this work Corresponding author Email chelinntuedutw They aim to develop methods that provide insights into the decisionmaking processes of GNNs These efforts encom pass various aspects such as selfexplainable classification models Dai and Wang 2021 and posthoc interpretation techniques Ying et al 2019 Luo et al 2020 Lin et al 2022 However one of the significant obstacles in inves tigating interpretable GNNs is the need for datasets with explanatory ground truths Furthermore a limited amount of real datasets with similar structures and attributes lack diverse features potentially causing overfitting and biased results in research and hindering progress in this field as mentioned by Lin et al Lin et al 2022 Simulating real datasets proves beneficial to surmount these challenges and enhance interpretable GNNs An effective synthetic dataset should mirror vital attributes of the actual data including graph size edge distribution node features and label disper sion Essential ground truth explanations like primary mo tifs should also be included The current synthetic dataset generation method Ling Yang and Zhao 2021 demands substantial human involvement Therefore an automated process is highly desirable for scalability offering an effi cient approach to generating datasets In light of the above issues we develop a novel approach to generate synthetic graphs We extract primary motifs from realworld datasets and apply a Merge strategy creating clusters with explana tory ground truths and combining meaningful clusters into a full synthetic heterogeneous graph dataset as benchmark datasets for future explanatory models Our main contribu tions are summarized as follows We proposed SynHIN with a novel approach of In Cluster Merge and OutCluster Merge for generating synthetic heterogeneous graph datasets It closely emu lates heterogeneous realworld graph structures and char acteristics enabling realistic synthetic generation We are the first to generate heterogeneous graphs with explanatory ground truths and devise benchmark datasets for heterogeneous GNN explainers to the best of our knowledge We provide a modularized synthetic graph dataset gen eration framework including motif extraction subgraph building InCluster Merge OutCluster Merge and pruning Each module can be substituted or expanded ac cording to the requirements arXiv240104133v1 csLG 7 Jan 20242 Related Work Synthetic Graph Generation There are some existing tools to generate graph datasets ar tificially Snijders et al Snijders and Nowicki 1997 pro posed to generate a graph edge between two nodes based on the clusters they belong Other approaches involve ex tracting subgraphs from realworld graphs and using them as predefined graphs to evaluate the capability of graph neu ral networks in identifying specific substructures within ran dom Stochastic Block Model SBM graphs Dwivedi et al 2020 SBM is one of the most popular methods used to gen erate clustering Snijders and Nowicki 1997 The concept of SBM is to maintain a high correlation between nodes with the same premarked label in the graph structure By consid ering the characteristics of the graph structure it generates a graph dataset that aligns with these specifications SBM has many variants including unsupervised Tsit sulin et al 2020 and semisupervised ones Rozember czki et al 2021 GraphWorld Palowitch et al 2022 de signs these graph dataset benchmark based on the Degree Corrected Stochastic Block Model DCSBM model Abbe 2017 and uses different parameter adjustments to design diverse graph datasets Therefore different graph neural network classification models can be evaluated clearly and fairly without overfitting a small number of datasets Currently these works all focus on homogeneous graphs and most synthetic graph methods do not provide the ability for interpretation lack the ground truths of interpretation and cannot be easily extended to heterogeneous graphs Explainer for Graph Neural Networks The field of GNN explainable models has seen a burgeon ing growth focusing primarily on node classification and graph classification Ying et al 2019 Luo et al 2020 Yuan et al 2021 Lin et al 2022 These models can be divided into inherent selfexplanation and posthoc interpretation Selfexplanatory models such as ProtGNN Dai and Wang 2021 use topK similarity to identify analogous subgraphs contributing to predictions with these similar subgraphs serving as the explanation itself On the other hand posthoc interpretation models aim to capture significant subgraphs Luo et al 2020 Yuan et al 2021 and key features Ying et al 2019 necessitating a further layer of interpretation after the model has been applied These models utilize a va riety of techniques and definitions to isolate subgraphs For instance GNNExplainer Ying et al 2019 is trained on each instance generating an output that masks the graph structure as its explanation It strives to identify the most consequen tial portions of the node feature simultaneously Similarly PGExplainer Luo et al 2020 uses mutual information to maximize the similarity between the masked graphs in the prediction results SubgraphX Yuan et al 2021 employs Monte Carlo Tree Search MCTS to locate vital subgraphs Graph Explanatory Evaluation Issues Conventional evaluation of graph explanatory models heav ily depended on molecular datasets or artificial community datasets which were often scarce Consequently overfitting was a common occurrence in these datasets Datasets like BAShape and MUTAG achieved remarkable accuracies ex ceeding 98 Luo et al 2020 Lin et al 2022 reached a plateau where further improvements are difficult to attain leading to a loss of generalizability These synthetic explana tory datasets are composed of artificially defined motifs and random graphs in advance In BAShape the motif com ponent comprises elements like houses grids cycles and more Ying et al 2019 The generation process involves defining the desired motif structure and quantity and gen erating the random graph portion Moreover the motif is randomly connected to the random graph part with edge addition or removal occurring randomly Since the graphs generated through this method can be considered random noise it becomes challenging to distinguish the significant elements from them As a result it becomes difficult to de fine the explanatory ground truths in the graph Furthermore only evaluating the motif parts could lead to overestimating the performance of explanatory models 3 Methodology In this research we proposed SynHIN a novel synthetic heterogeneous graph generation approach We utilize real world datasets as the reference to extract the primary motif Then use the proposed Merge technologies to create syn thetic heterogeneous graph datasets that mimic realworld ones with primary motifs that serve as the explanatory ground truths specifically designed for evaluating heteroge neous GNN explainers of node classification The synthetic heterogeneous datasets generated by SynHIN can relieve the lacking of datasets for explainable AI studies of HINs This study includes several critical modules including Mo tif Extraction Subgraph Building InOut Cluster Merge and Pruning The module detail is introduced in the follow ing chapters For clarity we use the IMDB 1 heterogeneous graph dataset as a reference dataset to explain our proposed algorithm and implementation details and illustrate the ef fectiveness of SynHIN IMDB is a movie dataset with infor mation such as movies keywords actors and directors The goal is to predict the movie genre tags The graph schema is shown on the left of Figure 2 Motif Extraction To extract the primary motif as explanatory ground truths we use the graphlet searching method Milo et al 2002 to capture common subgraphs on the real graph The graphlet model captures the number of occurrences on the IMDB dataset We organize the most frequently occurring graphlets as follows G9 G10 G11 and G20 The graphlet nota tions follow the paper Milo et al 2002 Figure 3 shows the shape of the graphlets The first three shapes G9 sim ple straight lines G10 Yshape and G11 crossed shape are prevalent across the datasets Therefore we did not use them as primary motifs However G20 is a unique graphlet in the dataset a three 2hop path with the same node type for start and end Figure 2 For IMDB the start and end 1httpswwwkagglecomdatasetskarrrimbamovie metadatacsvFigure 1 Overall SynHIN Structure Figure 2 IMDB Schema and a Primary Motif Figure 3 Graphlet G9 G10 G11 and G20 nodes are movies and the nodes between two movies could be directors actors or keywords G20 would be a stronger connection between two target nodes and more interpretable to humans Coincidentally this connection method is con sistent with studies based on metapaths and metagraphs Zhang et al 2020 used to capture the critical paths in the connection between two target nodes Hence G20 is espe cially suitable as an explanatory motif In practice to imi tate the situation in the real world and increase flexibility for explanatory ground truths The number of nontarget nodes within G20 connections between two target nodes could be sampled through a Poisson distribution to generate primary motifs Subgraph Building After designing the primary motifs we randomly add non target nodes to the motifs as secondary nodes Adding these secondary nodes can make the motifs more in line with the realworld target graph because a target node would not only be connected to nodes that are connected to other tar get nodes Many nontarget nodes might only be connected to one target node in the realworld dataset Additionally we leverage the actual degree distribution of target nodes from the realworld reference graph considering different edge types as a basis for sampling the number of secondary nodes These secondary nodes are subsequently introduced and attached to target nodes For example most movies are connected with one director three actors and five keywords in the graph This process results in a subgraph that attaches to the degree distribution of the realworld dataset ensur ing that each target node is connected to a nontarget node following the realworld dataset degree distribution After adding the secondary node to the primary motif all the target nodes in subgraphs will be marked with labels Furthermore the two target nodes in the same primary motif will be assigned the same label The subgraphs would be organized into multiple clusters Motifs in different clusters can be of different labels In addition to agreeing with the graph datasets most of the nodes with the same label are strongly connected and it is also more in line with human interpretation Connect or Merge the Subgraphs We can observe from the graph schema that HIN has a spe cific connection structure not every edge type between two nodes is legal Suppose we directly applied traditional graph generation methods BA and SBM to generate HIN Illegal connections may appear in the generated graph because the connection method does not provide such a guarantee Ad ditionally different edge types on the graph will also have different distributions It is challenging to determine how to connect the nodes via edges Moreover conventional meth ods for generating homogeneous graphs restrict the ability to apply distinctive processing for different edge types To resolve these issues we approach the problem with a brandnew perspective we consider the operation of Merge between subgraphs and nodes instead Specifically we pro pose a novel approach wherein two nodes of the same type are merged into a single node It offers two distinct ad vantages Firstly it ensures all edges generated during the process adhere to the established constraints thereby main taining legality Secondly it upholds a heterogeneous merge probabilitythresholdratio effectively emulating the inher ent variability in the distribution of different node types in the realworld reference graph Concurrently it pre serves the intrinsic structure and characteristics of the mo tif thereby ensuring a faithful representation of the target dataset Two types of Merge are described below InCluster Merge and OutCluster Merge where each type has a differ ent use and purposeInCluster Merge We perform the InCluster Merge IC Merge for the sub graphs in the same label In an IC Merge we randomly arrange the subgraphs with the same label and merge the subgraphs into the cluster one by one in a greedy manner When merging the nontarget nodes in the merged sub graphs will be sampled via a uniform distribution probability U0 1 Only the nodes that are sampled in the process will be merged Within the sampled nodes we randomly merge these nodes into the same type of nodes in the cluster The target node type will not participate in the IC Merge to avoid destroying the structure of the primary motifs In addition we avoid merging two motifs with the same label to avoid multiple explanatory ground truths for the same target node Also the greedy method allows subgraphs to have differ ent opportunities to be merged When every time a subgraph merges into a cluster the subgraphs added earlier in the clus ter will have more opportunity to be sampled to become the merged object This mimics the Superstar phenomenon which is common in actual community graphs Abbe 2017 Albert and Barabasi 2002 Figure 4 In and OutCluster Merges OutCluster Merge In contrast to the IC Merge method we employ a distinct approach for OutCluster Merge OC Merge in different labels considering the different characteristics of merging between clusters If we still use the greedy method of IC Merge for OC Merge some clusters that join the graph first will have more chances to merge making the clusters eas ier to connect with others The clusters that join later are less likely to be closely connected with others The Super star phenomenon should not be observed in outcluster be haviors In the actual graph connections between clusters of different labels generally have a more uniform distribu tion Therefore we adopt a onetime method approach to OC Merge Our approach carefully segregates all nodes based on their type creating distinct clusters Subsequently a proba bility of merging is computed for each pair of nodes from different clusters M U0 1 1 where M represents a symmetric N N matrix and N cor responds to the total number of nodes in a node type The element in the ith row and jth column of matrix M signi fies the probability of a OC Merge operation between node i and node j In our OC Merge step we ensure that node pairs originating from the same cluster have a zero merge proba bility thereby avoiding any potential remerging within the same cluster Our approach views the OC Merge threshold as the proportion of nodes involved in the merging process We can determine the number of node pairs to be merged based on the OC Merge threshold and subsequently perform Merge operations on the node pairs with the highest proba bilities according to the Merge probability matrix M k 1 OC Merge Threshold N 2 where k represents the count of node pairs undergoing Merge and N corresponds to the total number of nodes By adopting this strategy we strike a balance between the OC Merge thresholds influence and the IC Merge thresholds interpretation This method enables us to merge the graph dataset comprehensively accurately capturing the intricate connections between various node types In the OC Merge phase we set whether the target node type participates in the merge according to different task ob jectives In a multilabel classification task the target node is allowed to be merged now After the OC Merge this node can be marked with the labels from the original two nodes Different motifs can be used as explanatory ground truths according to different labels in the multilabel As such a multilabel dataset can be created Pruning Pruning is to regularize the synthetic graph to fit the restric tions or constraints of the reference realworld graph Most realworld graph databases are derived from nongraphical raw databases There are some restrictions from the nature of the raw dataset that limit the graph schema and edgetype degree distributions The pruning step can help synthetic graphs satisfy such raw data constraints For instance due to raw data constraints the public IMDB dataset only con nects one movie to at most three actors In ACM Wang et al 2019 and DBLP 2 one paper should be published in one conference and thus there will be only one connection to the conference In the pruning step we remove edges according to the node degree upper limit from raw data constraints with a welldesigned approach If pruning is done randomly it will easily destroy the primary motif structure affecting the explanatory ground truths Therefore we identify the nodes with degrees exceeding the upper limit and randomly drop the nonprimary motif edges until the degree fits the upper limit degree If the node degree is still too high we allow some flexibility in the node degree to ensure the primary motif can maintain a complete structure Keeping the com plete structures of primary motifs is more crucial since they serve as the essential explanation of ground truths Node Feature Generation Following the node features generation approach Tsitsulin et al 2022 Palowitch et al 2022 we generate the node fea 2httpwebcsuclaedu yzsundatatures from a withincluster multivariate normal distribution The features in the same cluster will be sampled from the same prior multivariate normal distribution with unit covari ance The feature center of each group will first be sampled from a multivariate normal distribution with the feature cen ter distance defined by the user The ratio of feature center distance to cluster covariance can be regarded as a signal tonoise ratio In the multilabel node classification task we first overlay the probability distribution functions of the la bels of the node Afterward we draw samples from this com bined distribution to determine the features of the nodes The effect of the feature center distance parameter will be dis cussed in the experiment Fidelity for Heterogeneous Graph Fidelity is a metric commonly used to evaluate the perfor mance of the explanation model Yuan et al 2021 Li et al 2022 It measures how closely related the explanations are to the models predictions The idea behind fidelity is that the performance of GNN models should be reduced if im portant characteristics or subgraphs are removed If the criti cal information is included in the subgraph the classification model prediction probability should be close to the original prediction We use fidelity as the evaluation metric to sup port that the primary motifs can be excellent explanations of ground truths Nevertheless we extend fidelity scoring to multilabel tasks which to our knowledge has not been done before The following are the details of the fidelity score Fidelity 1 N N i1 1 L L l1 fGiyi f Giyi 3 where fGiyi and f Giyi denote the predition probability of yi of the original graph Gi and motifs Gi respectively We denote N as the total number of target node samples and L as the number of node labels i 4 Experiment Settings As the SynHIN framework is flexible and can be highly customized according to the characteristic of the reference datasets we conduct a series of experiments to examine the influence of different tunable parameters on the syn thetic IMDB dataset generated by SynHIN Following the prior graph generation research Palowitch et al 2022 we delve into the interplay between tunable parameters clas sification model efficacy and fidelity We employ microF1 and macroF1 for the HGNN model accuracy and fidelity for interpretations SynHIN Parameter Settings The tunable parameters of the SynHIN framework for gen erating the synthetic graphs include the following Number of primary motifs It affects the number of tar get nodes and the size of the final generated graph In the experiments we set it similar to the reference dataset Number of clusters We determine the cluster number depending on the reference dataset Also we generate data in a balanced manner removing class imbalance is sues In the experiment we set it as the reference dataset InCluster Merge threshold The threshold is the prob ability of sampling from a uniform distribution It can be seen as the proportion of nodes that are not merged The lower the IC Merge threshold the more nodes are merged and links within the cluster will be stronger We grid search IC Merge threshold from 01 to 08 with an interval of 01 OutCluster Merge threshold The OC Merge affects the closeness between clusters If the threshold is lower the links between different clusters will be denser We grid search from 02 to 09 with an interval of 01 Feature dimension The dimension size of the feature is set as the same as the reference realworld dataset feature dimension Feature center distance Feature center distance deter mines the information richness of the features which can be regarded as the signaltonoise ratio of the fea ture due to the use of the unit covariance We perform a grid search from 01 to 10 with an interval of 01 Heterogeneous Graph Neural Network Models We used three different concept HGNN models to validate the synthetic graphs Model parameters follow paper rec ommendations The following briefly introduces the mod els 1 HGT Hu et al 2020 adopted a transformerbased design for handling different node and edge types without manually defining the metapath for the HGNN model 2 SimpleHGN Lv et al 2021 introduce the attention mech anism project different nodetype features to the share fea ture space and then use GAT as the HGNN backbone 3 TreeXGNN Hong et al 2023 leverage the decision tree based model XGBoost to enhance the node feature extrac tion assisting the HGNN model in getting more prosperous and meaningful information 5 Experiment Results We first experimented with the effect of the combination of IC Merge and OC Merge thresholds on the performance of the node classification model We set the OC Merge thresh old larger than the IC Merge threshold This assumption is from the observations of realworld graph datasets that the proportion connected to the same group must be more than different groups Palowitch et al 2022 The results are shown in Table 1 We also visualize the results in Fig ure 5 When the IC Merge threshold decreases or the OC Merge threshold increases the performance of the predic tion model would be better The less the noise of the corre sponding connection the better the performance Moreover when the feature center distance is larger the higher the in formation contained in the node feature the better the pre diction performance We adjust the number of different mo tifs thereby affecting the number of target nodes and the size of the synthetic graph dataset Since we follow the original parameter setting of the HGNNs rather than finetune for each synthetic graph dataset when we halve the referencedataset size the model tends to be overfitted and declines in performance Figure 5 HGT MacroF1 in different ICOC Merge thresh olds Fidelity Figures 6a and 6b illustrate the fidelity comparison between HGT SimpleHGN and TreeXGNN under the same set tings SimpleHGN and TreeXGNN exhibit lower fidelity indicating higher model stability than HGT Figure 6b is evident that the fidelity decreases as the OC Merge thresh old increases while maintaining a fixed IC Merge threshold This decrease can be attributed to noise reduction resulting in higher predicted performance Consequently the models become more stable leading to a corresponding decrease in fidelity Figure 6a presents the fidelity results for a fixed OC Merge threshold Notably the fidelity remains stable re gardless of changes in the IC Merge threshold The obser vation indicates that the model predominantly relies on the primary motif designed as the prediction foundation While the amount of noise in the dataset remains constant differ ent incluster connections are formed based on varying IC Merge thresholds It implies the presence of additional effec tive information beyond the designed primary motif How ever despite the presence of this additional information the fidelity of the model does not significantly change This find ing confirms that the primary motif designed indeed serves as the primary cause for the models predictions When the number of nodes surpasses the reference datasets count 2000 motifs the models prediction perfor a InCluster Merge b OutCluster Merge Figure 6 Threshold to Fidelity results a MacroF1 score b Fidelity Figure 7 Different graph size results a MacroF1 score b Fidelity Figure 8 Different center distance results mance remains steady shown in Figure 7a though the clas sification models hyperparameters were not finetuned for this condition Similarly changing the number of motifs re flective of the graphs size maintains a relatively consistent signaltonoise ratio across the graph Consequently Figure 7b shows fidelity performance remains stable Figure 8a presents the relationship between fidelity and feature center distance It can be observed that as the feature center distance increases the amount of information asso ciated with all node features also increases Consequently when the graph structure and primary motif design remain unchanged the primary motif incorporates more node fea ture information While the models performance improves due to the increased information Figure 8b shows that the fidelity decreases as well It can be attributed to more infor mation present in the node features The increased feature center distance introduces more informative node features into the dataset and primary motif decreasing fidelity Visualization We visualize the examples of synthetic graphs with varied parameters Dark blue represents nontarget nodes shown in Figures 9a 9b and 9c while other colors indicate differ ent label target nodes Figures 9a and 9b show that when the IC Merge threshold is constant an increase in the OC Merge threshold reduces noise enhancing groupings and re laxing divisions in the graph When the OC Merge threshold is constant shown in Figures 9b and 9c raising the IC Merge threshold leads to reduced information resulting in looser connections within the group and less conspicuous grouping in the graph Applying SynHIN to Other Datasets We demonstrate the adaptability of SynHIN to different ref erence datasets ACM and DBLP The prediction object of DBLP is different from ACM the graph schema of ACMHGT MacroF1 OutCluster Threshold 02 03 04 05 06 07 08 09 InCluster Threshold 01 7205 7337 7814 8143 8802 9063 9423 9705 02 7051 7213 7716 7828 8661 8871 9583 03 6811 6984 7610 8067 8618 9345 04 7097 7142 7942 8465 8980 05 6672 7350 8041 8807 06 7055 7680 8664 07 7095 8038 08 8140 Table 1 HGT MacroF1 in different ICOC Merge thresholds a IC Merge Threshold 03 OC Merge Threshold 06 b IC Merge Threshold 03 OC Merge Threshold 09 c IC Merge Threshold 06 OC Merge Threshold 09 Figure 9 SynHIN synthetic graph example ACM MicroF1 MacroF1 Fidelity HGT 9290 9285 03525 SimpleHGN 9639 9637 01452 TreeXGNN 9824 9823 01606 DBLP HGT 8489 8495 01246 SimpleHGN 9229 9233 00920 TreeXGNN 9050 9053 01347 Table 2 MicroMacroF1 and Fidelity in ACMDBLP Figure 10 ACM and DBLP graph schema and DBLP are shown in Figure 10 We perform a similar process to IMDB Notably we avoid merging the target node during the OC Merge for the singlelabel datasets The performance is shown in Table 2 Under the same parameter settings compared to the multilabel dataset IMDB singlelabel ACM and DBLP perform better in micromacroF1 because a singlelabel task is relatively sim ple Moreover the number of labels in ACM and DBLP is less than in IMDB making the task easier In addition the degree distribution in ACM and DBLP also has a higher av erage making the graph structure more closely connected Based on the above reasons the classification models per form better in ACM and DBLP than IMDB 6 Conclusion We present SynHIN a novel method for generating syn thetic graph datasets that address the limitations of exist ing approaches We leverage realworld datasets and em ploy the graphlet model to identify primary motifs that serve as ground truth explanations These primary motifs are then connected with additional nodes to form subgraphs ensur ing the degree distribution aligns with the reference dataset Subgraphs with the same predefined label are clustered through the IC Merge step Later the OC Merge step com bines different clusters to form a complete synthetic graph Finally pruning is applied to align the output graph with the limitations of the reference realworld dataset resulting in a HIN dataset with explicit ground truth explanations In this work we address the scarcity of heterogeneous graph datasets and overcome the need for such datasets in the do main of GNN explanations Furthermore our approach re solves the challenges faced by GNN explanations in homo geneous graphs such as the absence of explanatory ground truths for many nodes in most explanation datasets and the repetitive and limited nature of primary motif design To the best of our knowledge our work introduces the first syn thetic heterogeneous graph dataset with ground truth expla nations offering a multilabel functionality previously un available in generated graph datasets Additionally we de sign a comprehensive and diverse benchmark that provides a solid foundation for future research on heterogeneous GNN explanationsReferences Abbe E 2017 Community detection and stochastic block models recent developments The Journal of Machine Learning Research 181 64466531 Albert R and Barabasi AL 2002 Statistical mechanics of complex networks Reviews of modern physics 741 47 Chami I AbuElHaija S Perozzi B Re C and Mur phy K 2022 Machine learning on graphs A model and comprehensive taxonomy Journal of Machine Learning Re search 2389 164 Cui Z Xu X Fei X Cai X Cao Y Zhang W and Chen J 2020 Personalized recommendation system based on collaborative filtering for IoT scenarios IEEE Transac tions on Services Computing 134 685695 Dai E and Wang S 2021 Towards selfexplainable graph neural network In Proceedings of the 30th ACM Interna tional Conference on Information Knowledge Manage ment 302311 Dwivedi V P Joshi C K Laurent T Bengio Y and Bresson X 2020 Benchmarking graph neural networks Hong MY Chang SY Hsu HW Huang YH Wang CY and Lin C 2023 TreeXGNN can gradient boosted decision trees help boost heterogeneous graph neu ral networks In ICASSP 20232023 IEEE International Conference on Acoustics Speech and Signal Processing ICASSP 15 IEEE Hu Z Dong Y Wang K and Sun Y 2020 Heteroge neous graph transformer In Proceedings of the web confer ence 2020 27042710 Li Y Zhou J Verma S and Chen F 2022 A survey of explainable graph neural networks Taxonomy and evalua tion metrics arXiv preprint arXiv220712599 Lin W Lan H Wang H and Li B 2022 Orphicx A causalityinspired latent variable model for interpreting graph neural networks In Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition 1372913738 Ling C Yang C and Zhao L 2021 Deep generation of heterogeneous networks In 2021 IEEE International Con ference on Data Mining ICDM 379388 IEEE Luo D Cheng W Xu D Yu W Zong B Chen H and Zhang X 2020 Parameterized explainer for graph neu ral network Advances in neural information processing sys tems 33 1962019631 Lv Q Ding M Liu Q Chen Y Feng W He S Zhou C Jiang J Dong Y and Tang J 2021 Are we really making much progress revisiting benchmarking and refin ing heterogeneous graph neural networks In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery data mining 11501160 Milo R ShenOrr S Itzkovitz S Kashtan N Chklovskii D and Alon U 2002 Network motifs simple building blocks of complex networks Science 2985594 824827 Palowitch J Tsitsulin A Mayer B and Perozzi B 2022 Graphworld Fake graphs bring real insights for gnns In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining 36913701 Rozemberczki B Englert P Kapoor A Blais M and Perozzi B 2021 Pathfinder discovery networks for neu ral message passing In Proceedings of the Web Conference 2021 25472558 Shchur O and Gunnemann S 2019 Overlapping commu nity detection with graph neural networks arXiv preprint arXiv190912201 Snijders T A and Nowicki K 1997 Estimation and pre diction for stochastic blockmodels for graphs with latent block structure Journal of classification 141 75100 Stokes J M Yang K Swanson K Jin W Cubillos Ruiz A Donghia N M MacNair C R French S Car frae L A BloomAckermann Z et al 2020 A deep learn ing approach to antibiotic discovery Cell 1804 688702 Tsitsulin A Palowitch J Perozzi B and Muller E 2020 Graph clustering with graph neural networks arXiv preprint arXiv200616904 Tsitsulin A Rozemberczki B Palowitch J and Perozzi B 2022 Synthetic Graph Generation to Benchmark Graph Learning arXiv preprint arXiv220401376 Wang X Ji H Shi C Wang B Ye Y Cui P and Yu P S 2019 Heterogeneous Graph Attention Network In The World Wide Web Conference WWW 19 20222032 New York NY USA Association for Computing Machin ery ISBN 9781450366748 Ying Z Bourgeois D You J Zitnik M and Leskovec J 2019 Gnnexplainer Generating explanations for graph neural networks Advances in neural information processing systems 32 Yuan H Yu H Wang J Li K and Ji S 2021 On ex plainability of graph neural networks via subgraph explo rations In International Conference on Machine Learning 1224112252 PMLR Zhang W Fang Y Liu Z Wu M and Zhang X 2020 mg2vec Learning relationshippreserving hetero geneous graph representations via metagraph embedding IEEE Transactions on Knowledge and Data Engineering 343 13171329