A Universal Training Algorithm for Quantum Deep Learning Guillaume Verdon1 2 4 Jason Pye1 2 4 and Michael Broughton3 1Department of Applied Mathematics University of Waterloo Waterloo Ontario N2L 3G1 Canada 2Institute for Quantum Computing University of Waterloo Waterloo Ontario N2L 3G1 Canada 3School of Computer Science University of Waterloo Waterloo Ontario N2L 3G1 Canada 4Perimeter Institute for Theoretical Physics Waterloo Ontario N2L 2Y5 Canada Dated June 27 2018 We introduce the Backwards Quantum Propagation of Phase errors Baqprop principle a cen tral theme upon which we construct multiple universal optimization heuristics for training both parametrized quantum circuits and classical deep neural networks on a quantum computer Baqprop encodes error information in relative phases of a quantum wavefunction dened over the space of net work parameters it can be thought of as the unication of the phase kickback principle of quantum computation and of the backpropagation algorithm from classical deep learning We propose two core heuristics which leverage Baqprop for quantumenhanced optimization of network parameters Quantum Dynamical Descent QDD and Momentum Measurement Gradient Descent MoMGrad QDD uses simulated quantum coherent dynamics for parameter optimization allowing for quan tum tunneling through the hypothesis space landscape MoMGrad leverages Baqprop to estimate gradients and thereby perform gradient descent on the parameter landscape it can be thought of as the quantumclassical analogue of QDD In addition to these core optimization strategies we propose various methods for parallelization regularization and metalearning as augmentations to MoMGrad and QDD We introduce several quantumcoherent adaptations of canonical classical feedforward neural networks and study how Baqprop can be used to optimize such networks We develop multiple applications of parametric circuit learning for quantum data and show how to per form Baqprop in each case One such application allows for the training of hybrid quantumclassical neuralcircuit networks via the seamless integration of Baqprop with classical backpropagation Finally for a representative subset of these proposed applications we demonstrate the training of these networks via numerical simulations of implementations of QDD and MoMGrad CONTENTS I Introduction 2 II Background 5 A Continuous Quantum Registers 5 B Discrete Simulation of Continuous Quantum Registers 6 1 Quantum Phase Estimation 8 C Quantum Phase Kickback 8 1 Quantum Gradients 10 III Quantum Parametric Optimization 10 A Basic Principles 10 1 Quantum Feedforward and Baqprop 10 2 Fullbatch Eective Phase Kicks 12 3 Eective Forces 14 B Quantum Dynamical Descent 15 1 Core Algorithm 15 2 Heisenberg Picture Update rule 17 3 Connections to QAOA 17 4 Adiabatic Limit 18 C Momentum Measurement Gradient Descent 20 D Phase Space Visualization 22 IV Further Quantum Descent Methods 23 A Batching Parallelization 23 1 Quantum Stochastic Descent 23 2 Sequential MiniBatching 24 3 Coherently Accumulating Momentum Parallelization 25 4 Quantum Random Access Memory Minibatching 27 B Discrete Parametric Optimization 28 1 Kicking Hybrid DiscreteContinuous Parameters 28 2 ContinuousDiscrete Hybrid QDD 29 3 ContinuousDiscrete Hybrid Momentum Measurement Gradient Descent 30 4 ContinuumEmbedded Discrete Optimization 30 5 Estimating Continuum Gradients with Single Qubits 31 C Regularization Variants 32 1 ParameterWeight Decay 32 2 Metanetworked Interacting Swarm Optimization 32 3 Dropout 34 D Quantum MetaLearning 36 1 Overview 36 2 Quantum hyperparameter Descent 37 3 Network Architecture Optimization 40 V Quantum Neural Network Learning 41 A QuantumCoherent Neural Networks 41 1 ClassicaltoQuantum Computational Embedding 41 2 Classical Data Phase Kicking 42 3 Abstract Quantum Neuron 43 arXiv180609729v1 quantph 25 Jun 20182 4 Quantum Neural Network Feedforward Baqprop 44 B Quantum Phase Error Backpropagation Layerwise Analysis 46 1 Operator Chain Rule 48 C Implementations of Quantum Coherent Neurons 49 1 Hybrid CVDV Neurons 49 2 CVonly 50 VI Quantum Parametric Circuit Learning 50 A Parametric Ansatze Error Backpropagation 51 1 From Classically to QuantumlyParametrized Ansatze 51 2 Quantum Parametric Circuit Error Backpropagation 52 B Quantum State Exponentiation 53 1 Single state exponentiation 53 2 Sequential Exponential Batching 54 3 QRAM Batching 55 C Quantum State Learning 55 1 Quantum Pure State Learning 55 2 Quantum Mixed State Learning 56 D Quantum Unitary Channel Learning 56 1 Supervised Unitary Learning 56 2 Supervised Channel Learning 57 3 Unsupervised Unitary Learning 57 4 Unsupervised Channel Learning 58 E Quantum ClassicationRegressionMeasurement Learning 59 1 Overview 59 2 Output Encodings Implementation Options 59 F Quantum Code Learning 60 1 Quantum Autoencoders Compression Code Learning 60 2 Denoising Quantum Autoencoder 63 3 Quantum Error Correcting Code Learning 63 G Generative Adversarial Quantum Circuits 65 1 Classical Generative Adversarial Networks Review 65 2 Generative Adversarial Quantum Circuits 65 H Parametric Hamiltonian Optimization 68 1 HamiltonianParallelized Gradient Accumulation 68 I Hybrid Quantum NeuralCircuit Networks 70 1 Fully Coherent Hybrid Networks 70 2 Hybrid QuantumClassical Networks 71 VII Numerical Experiments 72 A Quantum Neural Deep Learning 72 1 Application Methods 72 2 Implementation Results 73 B Quantum Parametric Hamiltonian Optimization 74 1 Application Methods 75 2 Implementation Results 75 C Quantum Unitary Learning 76 1 Application Methods 76 2 Implementation Results 76 D Hybrid NeuralCircuit Learning 77 1 Application Methods 77 2 Implementation Results 77 VIII Discussion Outlook 78 1 Nearterm considerations 78 2 Furtherterm considerations 79 IX Conclusion 80 X Acknowledgements 81 References 81 I INTRODUCTION The eld of classical deep learning 1 has seen a rapid expansion in interest and number of breakthrough appli cations in recent years 211 Deep learning consists of a class of algorithms within the broader class of machine learning algorithms which are mostly employed either to identify patterns in a given dataset andor generate new data mimicking such pat terns At their core many machine learning algorithms consist of three main components First is the model a parametrized hypothesis class of functions usually ar ranged in a network of layered compositions of simpler parametric functions Networks with multiple layers are called deep and are the subclass of models considered in deep learning Second is a cost function a metric to eval uate how well specic hypotheses model the data The third and nal key component is the optimizer an algo rithmic strategy used to search over the space of hypothe ses in order to minimize the cost function to a sucient degree A central concept in the optimization of such networks of compositions is the principle of backwards propagation of errors also known as the backpropagation algorithm Typically the cost function error of such a layered net work is a function strictly of the output nal layer of the network or occasionally of the output of certain sub sets of the network The backpropagation algorithm is a means for information about the gradient of the cost function with respect to the network parameters to spread eciently throughout the network beginning at the output and propagating backwards through the com positional layers Since the negative gradient provides the direction of steepest descent in the landscape of hy potheses this propagation can be leveraged to optimize the network parameters in order to nd a local minimum3 of the cost function Many if not all canonical network optimization methods employ the backpropagation prin ciple in some manner 3 12 It is often deemed that the recent resurgence and successes of classical deep learning can be traced back to the rst demonstrations of imple mentations backpropagation algorithm 12 13 In this paper we introduce a quantumnative back propagation principle Sec III A 1 called the backwards quantum propagation of phase errors Baqprop This Baqprop principle allows for the ecient optimization of quantumlyparametrized networks on a quantum com puter Previously such quantum networks typically con sisted of classicallyparametrized quantum operations By considering versions of these networks using quan tum parameters we can exploit the quantum mechan ical properties of the wavefunction over the hypothesis space to aid in the propagation of gradient information throughout the network More specically Baqprop em ploys the phase kickback principle of quantum comput ing to induce relative phases between dierent branches of the wavefunction in the superposition over hypoth esis space These relative phases will contain the de sired gradient information Baqprop will thus allow for quantumenhanced optimization over multiple types of quantum parametric network hypothesis classes Note that the technique of leveraging phase kickback for gra dients was originally pioneered by Jordan 14 and later improved upon in Ref 15 In our background section Sec II we show how this gradient technique is related to phase estimation in the context of both continuous variable quantum information and quditsqubits There fore in the context of training quantumparametric net works Baqprop provides a unied view of both classical backpropagation and quantum phase estimation Further included in this work is the introduction of two main Baqpropbased parameter optimization strategies Sec III B III C Both approaches leverage the cost function error signal encoded in the relative phases of the quantum parameter wavefunction but provide dierent means of using this error signal to update the parameters during an iteration of the optimization The rst of these strategies is a fully quantumcoherent method of opti mization called Quantum Dynamical Descent QDD This method is motivated by the recognition that these relative phases can be seen as induced by an eective po tential acting on the parameters The QDD algorithm is then a descent of the parameter optimization landscape via quantum simulation of the Schrodinger dynamics un der the inuence of this eective potential The sec ond method is a quantumclassical approach which in volves a quantum measurement of the Baqpropinduced relative phase shifts in the wavefunction of the param eters This allows for the estimation of the local gradi ent of the cost function in the parameter space which can then be used in gradient descent to descend the cost landscape Since these relative phase shifts can be in terpreted as kicks in the momenta of the parameters we call this approach Momentum Measurement Gradient Descent MoMGrad The broad aim of this work is to bridge classical and quantum deep learning theory within a unied framework for optimization on quantum computers Establishing this bridge between theories allows for an exchange of powerful tools across elds as well as the possibility to mutually improve understanding of both topics In this spirit in Section IV we introduce multiple techniques as augmentations of the core optimization methods of Section III QDD and MoMGrad which are directly in spired from methods of classical deep learning 1618 For example we introduce methods for parallelization Sec IV A 3 regularization Sec IV C 1 and hyper parameter optimization metalearning Sec IV D In addition to these various augmented optimization strate gies in Sections V and VI we explore ways of leverag ing Baqprop in numerous applications of quantum para metric transformation learning for classical and quantum data modelling In particular for classical data learning we examine quantumcoherent analogues of traditional classical neural networks Sec V while for quantum data we discuss the training of a number of applications of Quantum Parametric Circuits Sec VI We later test the ecacy of training some of these proposed applica tions with QDD and MoMGrad via numerical simulations of quantum computation in Section VII To provide further context for this work let us briey describe how it ts into the current state of quantum ma chine learning literature Inspired by classical machine learning the eld of quantum machine learning began as an exploration of the possibility of using quantum algo rithms to identify patterns in either quantum or classical data using a quantum computer 19 Early quantum ma chine learning work took a similar path as early classical machine learning before the advent of the connectionist approach deep learning the focus lied mostly on so called analogizertype algorithms 13 Such early quan tum algorithms include Quantum Principal Component Analysis 20 Quantum Support Vector Machines 21 and other kernel methods 22 23 Many of these algo rithms focused on the analysis of classical data embedded into a quantum wavefunction via a theoretical quantum computer component called a Quantum Random Access Memory 24 The goal of such an embedding was to ex ploit the exponential dimensionality of the Hilbert space to encode data in the probability amplitudes of a multi qubit wavefunction in order to potentially gain an expo nential speedup over classical algorithms The feasibility and practicality of this dataloading scheme with realis tic noise conditions and error correction overheads taken into account remains a debated topic to this day 25 Beyond the data loading issue part of the quantum ma chine learning eld has moved away from analogizertype methods 13 towards parametric networks resembling deep learning for similar reasons to those responsible for the eventual dominance of classical deep learning over classical kerneltype methods 2629 Namely the rea soning being exibility and modelling capacity not all4 data is linearly separable using SVMs thus requiring a handpicked kernel and not all data is wellsuited to a Principal Component Analysis Before we delve into the more recent literature on quantum parametric networks we will rst mention ear lier work involving deep learning on quantum comput ers Similar to the progression of classical deep learning the rst forms of quantum neural networks to be studied were Boltzmann machines In classical machine learn ing some of the work rst incorporating backpropaga tion was in the context of deep networks of coupled spin like neurons called Deep Boltzmann Networks 30 On the quantum side analog quantum computers allowed for a physical implementation of networks of qubits whose statistics mimic those of Boltzmann machines 3134 This general avenue of research focused on determining whether quantum computers can accelerate the training of classical neural network models Due to the possibility of superpositions of the joint state of the neurons and thereby of quantum tunneling in the energy landscape it was hoped that Quantum Annealing could provide a speedup over classical annealing optimization methods for such neural network models Despite early claims of a speedup 35 certain bottlenecks such as the embedding problem qubit quality and thermal noise 36 would ob scure whether there could truly be a quantum advantage for Quantum Annealing especially with the advent of quantuminspired classical algorithms designed to com pete with these machines 37 The question thus remained is there a way to leverage the quantum properties of superposition entanglement and tunneling in order to gain an optimization advantage for a classical neural network Later work continued on this avenue of research 38 but most work pivoted to quantum parametric circuits which we will return to be low In this paper we provide a comprehensive approach to training classical neural networks on a quantum com puter for the purposes of classical data learning Sec V All techniques make use of superposition and entangle ment Sec III and some techniques employ tunneling directly Sec III B IV D We also provide an indepth analysis of quantum backpropagation of the error sig nal in these quantumcoherent neural networks thus ex plicitly relating quantum and classical backpropagation This bridging of the theories allows for further exchange of insights and techniques as well as a merging of both the classical and quantum backpropagation principles see Sec VI I 2 Furthermore not only can the network parameters be optimized but so can the network archi tecture and hyperparameters in a quantum tunneling procedure in the space of trained networks which we call Quantum MetaLearning QMetaL Section IV D Although we do not directly claim a general speedup for training classical neural nets in Section III B we explicitly relate Quantum Dynamical Descent QDD to the Quantum Approximate Optimization Algorithm QAOA 3941 and the Quantum Adiabatic Algorithm QAA 4244 QAA is the predecessor to Quantum An nealing the latter of which is considered to be the open quantum system analogue of QAA The QAOA is akin to a variationallyoptimized temporally coarsegrained approximate quantum simulation of the QAA Both the QAA and the QAOA have been shown to exhibit a quan tum advantage in some optimization scenarios 40 45 As such the possibility may be open to show a speedup for Quantum Dynamical Descent andor Quantum Meta Learning for certain types of networks and optimization scenarios We leave further analysis of such advantages for future work More recent approaches to quantum deep learning have moved away from attempting to train classical models on a quantum computer and have rather involved a quantumnative model called quantum parametric cir cuits QPCs 2629 As their name implies QPCs con sist of multiple parametric quantum operations arranged in a sequential layered structure in a similar fashion to a neural network In the literature QPCs are sometimes called Quantum Variational Algorithms 29 or Quantum Neural Networks 26 27 To avoid confusion with the quantumcoherent neural networks from Section V we will exclusively use the term Quantum Parametric Cir cuits QPCs can learn from either classical data or quantum data and have been shown to be able to do so on near term noisy quantum hardware 46 47 mainly through the use of classicalquantum hybrid optimization schemes 48 Such optimization schemes rst execute multiple runs of a parametric circuit on a quantum processing unit for a certain set of parameters Through these runs the expectation value of certain observables at the output of the circuit are obtained and fed to a classical process ing unit The classical computer is then tasked with the extremization of this expectation value subject to varia tions in the parameters using the quantum computer as a black box Thus the classical and quantum computer work in tandem to variationally optimize over the space of parametric circuits hence the name quantumclassical hybrid optimization Despite a recent rapid expansion of this body of work the question remains open as to whether there can be a more ecient means to optimize over the space of quan tum networks either in the short term Noisy Interme diate Scale Quantum Devices 49 era and long term post FaultTolerance and Error Correction 50 Fur thermore a backpropagation principle could provide a unied approach to the optimization of quantum net works and provide insights as to which ansatze are e ciently trainable 51 The work presented in this paper tackles these is sues We show explicitly how to use Baqprop for a number of applications of Quantum Parametric Cir cuits in Section VI A main draw using Baqprop is that it requires only on the order of one query feedfor ward per optimization step This can be compared to the abovementioned classical nitedierence quantum5 classical methods which usually require a number of queries which scales at least linearly with the number of parameters The applications featured in Section VI either build upon previously existing work 52 53 or relate to works released during the writing of this pa per 26 27 54 In particular we study the follow ing tasks quantum state learning Sec VI C quan tum unitary learning Sec VI D 1 VI D 3 quantum channel learning Sec VI D 2 VI D 4 quantum clas sicationregression Sec VI E quantum compression code Sec VI F 1 VI F 2 and quantum error correcting code learning Sec VI F 3 quantum generative adver sarial learning Sec VI G as well as parametric Hamil tonian optimization Sec VI H Finally we propose an application which combines both classical neural net works and quantum parametric circuits in a hybrid net work Sec VI I 2 We show how to leverage Baqprop to train these hybrid neuralcircuit networks either ex clusively on a quantum processing unit or in a hy brid quantumclassical fashion combining classical back propagation with Baqprop and allowing for the seam less backpropagation of error information through the classicalquantum interface As this paper is intended for a broad audience we be gin with an introduction of core background quantum computing concepts in Section II including continuous variable CV and discrete variable DV phase space phase estimation basic operations and gradient estima tion Although not essential to understanding this paper a knowledge of standard deep learning may be useful to the reader who would like to compare classical versions of certain protocols to their respective quantum versions in troduced in this paper We encourage the reader looking to fully connect concepts of classical and quantum deep learning to consult one of many possible references which cover the basics such as gradient descent stochastic gra dient descent minibatch gradient descent and hyper parameter optimization 1 II BACKGROUND A Continuous Quantum Registers A quantum register that stores a real number is dened by an observable with a spectrum consisting of R which we will denote here by R R d The Hilbert space upon which this operator acts is L2R Shifts be tween eigenstates of the operator are generated by a conjugate momentum operator denoted which satis es i where throughout we set 1 Addition and multiplication of real numbers are com mon operations performed during a computation In or der to implement these operations as unitaries on quan tum registers they need to be reversible This is typi cally achieved by retaining one or more of the inputs in the output of the computation For example addition of two quantum real numbers in eigenstates 1 and 2 respectively can be achieved with ei1 2 1 2 7 1 1 2 1 Here we have used one of the output registers to store one of the inputs and the other to store the sum of the two input values Note that this implementation of addition can be thought of as a von Neumann measurement of the rst register by the second Addition can also be achieved somewhat less eciently by retaining both input values and storing the output in a third register which is initialized to the state 3 0 ei1 3ei2 3 1 2 0 7 1 2 1 2 2 Enacting multiplication of two quantum real numbers typically involves using a third register for the output initialized to 3 0 The unitary for multiplication is ei1 2 3 1 2 0 7 1 2 12 3 These basic operations will be used extensively in the paper An illustration of addition of two continuous registers is provided in Fig 1 The gure consists of the plots of the Wigner functions before and after the rst version of addition for two registers initialized to Gaussian states Here we will only employ Wigner functions for purposes of illustration but for completeness we have dened the Wigner function of a continuous register in state to be Wcx p Z dxdp 2 tr D cx peixppxpx2 4 where Dcx p 1 2 eipeix 5 One of the simplest physical systems with a quantum real number as its degree of freedom is the quantum har monic oscillator dened by a Hamiltonian H 1 2m 2 m 2 2 02 6 where m and are the oscillator mass and frequency re spectively and 0 simply sets the location of the min imum of the potential term A collection of continuous quantum variables will often be arranged as a vector of operators nN n1 Throughout we will be us ing the notation a ann to denote a vector whose nth component is an A set of coupled quantum harmonic oscillators with degrees of freedom in the simplest case of equal masses are associated with a Hamiltonian H 1 2m T 1 2 0T K 0 7 where K is a positivedenite matrix which encodes the couplings It is a basic fact of such a system that its6 Figure 1 CV adder In plots a and b we have two initial Gaussian pointer states initialized with centered at 1 and 2 respectively In plots c and d we show these two reg isters after the addition where plot c retains the input of the rst register and plot d shows the output on the second register We see that as expected the Gaussian in d is cen tered at the value 3 Here we have employed Gaussian states of nite variance since eigenstates are unphysical hence there is some noise in the input and output values for these registers Also in plot c we have that the Wigner function is broadened in the direction due to phase kickback which we will treat in Subsection II C In plot d the variance in the direction is increased due to uncertainty in of the two registers before the addition ground state 0 is a Gaussian wavefunction when rep resented in the joint eigenbasis of 0 detmW 14e m 2 0T W 0 8 where W q 1 mK recall K is positivedenite B Discrete Simulation of Continuous Quantum Registers If one has access to a discrete system for example a collection of qubits on a quantum computer then it is possible to approximate the behavior of a continuous register A register which stores a qudit of dimension d is dened by the operator Jd d1 X j0 j jj 9 acting on the Hilbert space H Cd If one has access to N qubits it is possible to construct Jd for d 2N as J2N N X n1 2n2In 2 Zn 2 2N 1 2 N X n1 2n2 Zn 2 10 where In 2 and Zn 2 are the identity and the PauliZ operator respectively for the nth qubit Note that in the above equation and throughout this paper constant terms added to operators should be treated as propor tional to the identity The operator Jd can be used to simulate a continuous operator on a nite interval a b R by identify ing the eigenvalues of Jd with discrete samples on the interval Then one can write the simulated continuous variable on the interval a b as d b a d 1 Jd aId 11 where Id is the identity operator for the qudit One means of dening a momentum operator is as the gener ator of shifts in the value of the continuous or discrete register Such an operator can be written as the Fourier transform continuous or discrete respectively of the ob servable corresponding to the value of the register For a continuous register storing a quantum real number the Fourier transform is Fc x Z R dy 2 eixy y 12 The momentum operator from the previous section can thus alternatively be dened as F c Fc 13 From this denition it is straightforward to show that generates shifts in the register ei x x for any R In analogy with the continuum one can dene a dis crete Fourier transform by Fd j 1 d d1 X k0 jk d k 14 where d e2id and an analogous discrete momen tum operator by Kd F d Jd Fd 15 It is easy to show that this operator also generates shifts in the eigenbasis of Jd Explicitly for some a Zd a d Kd j j a mod d 167 Although at times we will nd it necessary to work with Jd and Kd directly often it is more convenient to work with the exponentiated operators Zd d Jd and Xd d Kd Notice that we could also write Xd F d Zd Fd These are the HeisenbergWeyl operators which satisfy the relation Zd Xd 1 d Xd Zd in analogy with the displacement operators used in the Weyl relations for continuous systems Simulation of a continuous momentum operator on the interval a b using a qudit can be achieved with d d 1 b a Kd 17 Note that this is not simply the discrete Fourier trans form of d since in the continuum we should have a Schrodinger representation of as i If has units of length then should have units of inverse length therefore the scaling of the two operators should be dierent Also we do not have a constant oset for the momentum operator so that it is centered at zero momentum Now let us examine the exponentiated versions of these simulated position and momentum operators First it will be useful to derive an expression for Z d and X d for arbitrary R note that the case where Zd is straightforward Because the map z 7 z is locally analytic we can use the Riesz functional calculus to dene Z d as Z d d1 X j0 j d jj 18 and continue to write X d F d Z d Fd Note that al though we can write Zd P jZd j d jj with a sum over Zd the sum for Z d is taken over the set 0 d 1 This is because for arbitrary R in order to determine j d e logj d we must choose a branch of the complex logarithm which breaks the pe riodic structure of the phases ie jd d j d for Zd There is an alternative form of the operator Z d which we will occasionally nd convenient Due the local an alyticity of the map z 7 z one can in principle write a local power series for Z d in terms of integer powers of Zd However since we also have that Zd d Id this series will collapse in order to give a representation of Z d in terms of a superposition of nite powers Zk d with k 0 d1 To obtain such a form explicitly let us rst dene the following orthonormal basis for operators on Cd Ddq p 1 d Zp d Xq d 19 where q p Zd These operators are orthonormal with respect to the HilbertSchmidt inner product A B trAB To demonstrate that they are a basis we note that jk 1 d X pZd jp d Ddp j k 20 ie they can be used to represent an arbitrary matrix element of an operator acting on Cd Using this relation it is simple to show that for arbitrary R one can write Z d X kZd k Zk d 21 X d X kZd k Xk d 22 where 1 d d1 X j0 j d 1 dd12 d sin sind 23 Hence Z d and X d can be represented as a superposition of phases and displacements respectively Notice that is peaked around 0 Therefore for example if we apply X d to a computational basis state j then the value of the register is shifted to a digital approximation of with errors if Zd We demonstrate this fact explicitly in Fig 2 below in the context of phase estimation With this in hand let us consider the exponentiated versions of our simulated continuous operators d and d Using the above one can write d d P kZd d1 ba k Xk d 24 d d P kZd ba d1 k Zk d 25 Consider for example beginning with the state j In the simulated interval this corresponds to an eigenvector of the simulated position operator d with eigenvalue ba d1 j a a b Now suppose we apply a simulated continuous displacement d d to this state Then we arrive at the state d d j X kZd d 1 b a k j k 26 This state is peaked around a digital approximation to the value j k j d1 ba Thus it is approximately an eigenvalue of the simulated position operator d with eigenvalue ba d1 j a That is the value of d has been shifted by approximately hence the operator d d is a simulation of a displacement of the eigenstates of d8 Of course the error in approximation of the shift in is due to the fact that one can only get a certain preci sion for a nite d Thus for a given d ideally we would like the shift of the discrete register to be the closest integer k to d1 ba However here we have a prob abilistic distribution over dierent integer values so we will not obtain the closest integer approximation with certainty If using qubits one technique for suppress ing the probability of error is to use more qubits than the desired precision and ignore these extra qubits when performing a measurement or subsequent computations It is a standard result see for example Ref 55 that in order to obtain an approximation accurate to n bits of precision with probability 1 one must use at least N n log2 12 qubits for the simulation Other issues which can arise during a computation with the simulated continuous operators are overow and underow errors An overow error is simply a shift of the register which goes beyond the range a b An underow error is a shift that is too small to be resolved by the discretization scale So far in this section we have seen that it is possi ble to simulate the kinematic structure of a continuous quantum system using a suciently large digital system Insofar as the dynamics is concerned in Ref 56 it was shown that it is possible to simulate the dynamics of a quantum harmonic oscillator using a Hamiltonian Hd 1 2 p2 d 1 2 x2 d 27 where in our notation these operators can be written xd r 2 d d 1 b a d a d 2 pd r 2 d b a d 1 d d 2 28 1 Quantum Phase Estimation The nal tool we will review in this subsection is the phase estimation algorithm which can be seen as a hy brid continuousdiscrete variable operation As above let and be the continuous variable and its conju gate momentum and let d and d be corresponding simulated continuous operators The phase estimation algorithm is a von Neumann measurement of the contin uous variable by the simulated discrete variable and can be summarized by the operator d d 29 We see that this is a straightforward extension of the continuous shifts of the discrete registers from before but now the magnitude of the shift is controlled by a continuous quantum register Often this algorithm is im plemented by taking the discrete Fourier transforms out of the exponential and applying a controlled phase oper ator d d F d d1 ba 2 da d Fd This is due to the fact that it is straightforward to construct d from Pauli Z qubit operators for d 2N and then the controlled phase gate breaks into a product of 2local controlled phase gates 2N 2N 2N 12 2ba 2N F 2N N Y n1 2N 1 ba 2n2 Zn 2 2N F2N 30 Note that throughout this paper all products of oper ators will be ordered as N Y n1 Un UN U2 U1 31 An illustration of the phase estimation algorithm is provided in Fig 2 The plots are of the Wigner func tions of the continuous and discrete registers before and after the algorithm The continuous Wigner function is dened as in the previous subsection The denition of the discrete Wigner function used here for odd d is Wdq p 1 d X qpZd tr D dq pqppq21pq d 32 where 21 d 12 is the multiplicative inverse of 2 in Zd for odd d cf Ref 57 More generally the shift could be controlled by an ar bitrary observable A from any type of register For example this observable could be a Hamiltonian If this register is in an eigenstate of the observable then the phase estimation algorithm provides a digital approxi mation to the eigenvalue of that observable Explicitly suppose A then d Ad 0 X kZd d 1 b a k k 33 where a b should be chosen so that the interval a b con tains the spectrum of A or at least the desired eigenvalue C Quantum Phase Kickback To every action there is an equal and opposite reac tion Although this archaic dogma is no longer a central tenet of modern physics remnants of Newtons third law make occasional appearances particularly in quantum computing In more general terms this law makes the point that generically a coupling between two physical systems causes them to react to one another Concretely this is due to the coupling inducing generalized forces appearing in the equations of motion of both systems In quantum mechanics these coupling terms appear in9 Figure 2 Phase Estimation In plots a and b we have the Wigner functions of the initial states of both the continuous and discrete registers respectively The discrete register was chosen to be a qudit of dimension d 63 The continuous register is initialized to a highly squeezed Gaussian pointer state with centered at 2 The discrete register is initialized to a null state Plot c demonstrates the phase kickback on the momentum of the continuous register discussed in the next subsection while plot d is the result of the von Neumann measurement Here we chose a 0 and b 5 so that the discretecontinuous conversion factor is d 1b a 124 Hence for a von Neumann measurement of a value of 2 the discrete register is centered at q 25 which is the closest integer approximation to 2d1ba 248 the generators of the unitary operators which evolve the interacting systems although the eect on one or both systems may not be apparent For example consider the case of a controlled displacement on the position basis of two continuous reg isters eic t c t 7 c t c 34 It would seem that the operation had no eect on the control register However we can see this is not so by examining the action on the momentum basis eic t c t 7 c t t 35 Hence the control register is only left unchanged if in a position eigenstate and the target register is only left unchanged if in a momentum eigenstate Often this backaction is more easily visualized in the Heisenberg picture We see that under this controlled displacement the operators of the two registers evolve as Adeic tc c Adeic tc c t Adeic tt t c Adeic tt t 36 where Ad U A U A U An analogous eect plays a prominent role in quantum computing where it goes by the name of phase kickback Suppose we consider a controlledNOT gate CNOT 00c It 11c Xt 37 acting on two qubits In the Zbasis this acts as CNOT zc zt 7 zc zt zc 38 and in the Xbasis CNOT xc xt 7 xc xt xt 39 where the X eigenstates are identied as x 0 and x 1 In the Heisenberg picture this looks like Ad CNOT Zc Zc Ad CNOT Xc Xc Xt Ad CNOT Zt Zc Zt Ad CNOT Xt Xt 40 We see that the operation not only aects the target qubit but also the control qubit More precisely the operation changes the computational value of the target qubit zt and the phase of the control qubit xc hence the name phase kickback The point here is that phase kickback may seem like odd quantum behavior since it is not intuitive to think of a controlled operation aecting the state of the con trol register However it is simply the fact that we are including the physics of the control register in our model and that the backaction on the control register is simply due to a remnant of Newtons third law within quan tum mechanics In particular we are keeping track of the eects these operations have in the conjugate of the computational basis This backaction could also be seen with classical bit strings however one does not typically consider the momentum of bit strings in classical com puting However that is not to say that we are only doing clas sical physics In the realm of quantum mechanics these phases can interfere with each other to produce highly nonclassical phenomena This eect is used throughout quantum computing eg in Shors algorithm In this paper we will use controlledunitaries of the form U X U 4110 which are generally more sophisticated than the controlleddisplacement and controlledNOT gates just described but the phase kickback behaves similarly We will be using this kickback in the following to train ma chine learning algorithms parametrized by quantum reg isters 1 Quantum Gradients An application of phase kickback that we will use throughout the paper is for computing the gradient of an oracle for a function f whose input is a continuous or simulated continuous register This phase estimation of gradients technique for a black box oracle was rst pio neered by Jordan 14 and later improved upon by Wiebe et al 15 Consider a von Neumann measurement of the output of this function described abstractly by eif12 1 2 7 1 2 f1 42 We can think of this operation as computing the function f on the rst register and storing the result in the second In the Heisenberg picture one can view this operation as Adeif122 2 f1 43 The phase kickback appears as a shift in the momentum of the rst register Adeif121 1 f 12 44 where f is the derivative of f Thus we see that if the second register begins in a state of small uncertainty in 2 the momentum of the rst register is shifted propor tional to the gradient of f Of course if there is large un certainty in either of the quadratures of the rst register then this shift will provide little information However we see that it will possible to extract some information about the gradient by measuring the momentum 1 Be low we will show how to make use of this observation to implement backpropagation in a variety of contexts III QUANTUM PARAMETRIC OPTIMIZATION This section will be devoted to explaining abstractly the training algorithm used to accomplish quantum pa rameter optimization for general parametrized quan tum algorithms Then Sections V and VI will exam ine more concretely how these techniques can be used to train quantumcoherent neural networks as well as parametrized quantum algorithms for performing various quantum information tasks A Basic Principles 1 Quantum Feedforward and Baqprop Machine learning consists of the task of nding a suit able algorithm among a parametrized class of algorithms On a quantum computer an algorithm consists of a uni tary operator acting on a collection of registers Nat urally one can consider parametrizing a quantum algo rithm unitary operator with some collection of param eters nn Abstractly we can denote this algo rithm as U For example the algorithm may consist of a set of single qubit rotations along with controlled NOT gates and the parameters could be taken as the Euler angles parametrizing each rotation In such considerations the algorithm is quantum but the parameters remain externallycontrolled and classi cal Here we will extend this by using parameters which are quantized To this end we introduce registers to store the parameters in addition to those upon which the algo rithm is performing its computation Let us denote the full Hilbert space as H Hc where H is the param eter Hilbert space and Hc is the computational Hilbert space The combined unitary operator will be denoted U X U 45 Note that the sum over is only formal we also include the case where this consists of integrals over continu ous variables Every xed set of parameters applies a parametrized algorithm U Allowing for quantum parameters allows us to apply a superposition of quantum algorithms Of course when the state of the parameters is such that the uncertainty in is small then we recover the case of a quantum algorithm controlled by classical parameters Including the parameters as a part of the system under consideration will allow us to analyze the computation of the class of algorithms and the training of the parameters as a closed dynamical system Furthermore the quantum mechanical nature of the parameters can allow for some advantages in training as we will explore throughout this paper Note that although the parameters have been pro moted to quantum operators in this paper we will only consider seeking a classical value for the parameters for the purposes of inference at the end of the training In one of the optimization strategies we will present below Quantum Dynamical Descent Subsection III B the end of the training will result in a wavefunction over the set of parameters At this stage one could perform a mea surement of the parameters or multiple measurements to obtain an expectation value and use the result as the classical value The second optimization strategy is semiclassical Momentum Measurement Gradient De scent Subsection III C and the end of the training will directly result in a set of classical parameters to be used for inference11 Once the parametrized class of algorithms is xed ie the hypothesis space next is to provide an appraisal of each algorithm according to some metric and then search for a set of parameters which optimizes this met ric In machine learning this parameter search is guided by training data The basic element of training is to feed a single example input into the algorithm and evaluate a loss function at the output Typically the gradient with respect to the parameters of the loss function for multi ple training examples are combined to update the values of the parameters using some variant of gradient descent If the algorithm is comprised of many parametrized com ponents as in deep learning then the gradients of the loss function at the output need to be propagated back to the relevant component in order perform this update In this section we will explain the use of quantum phase kick back to obtain a gradient of the loss function for a single training example The following sections will elaborate upon various schemes for making use of these gradients as well as combining the phase kicks for multiple training examples The remainder of this section will be used to describe abstractly the Quantum Feedforward and Baqprop QFB algorithm which evaluates the gradient of the loss function for a single training example and stores it in the momenta of the parameter registers via an eective phase kick To this end let us begin by denoting Hc as the input associated with a single training example to the quantum algorithm U For example this state could denote the encoding of a classical number or set of num bers in a continuous or discrete quantum register How ever this could be any state in Hc for a general quantum algorithm Further discussion of the structure of input states will be provided below for particular applications Let us also suppose the parameters are initialized in an arbitrary state 0 H expressed in the parameter eigenbasis as 0 P 0 The algorithm then acts on this joint initial state to produce a superposition of parametrized algorithms on the example input state and yields X 0 U 46 This will be called the feedforward step of the QFB al gorithm The next step in a machine learning training algorithm is to evaluate the performance of the algorithm using a loss function based on the output for a particular in put In this case the loss function will be an operator which will be denoted L which acts on the computational Hilbert space Hc and acts as the identity on the param eters After the feedforward step of the QFB training algorithm we apply the exponential of the loss function as a phase gate I ei L 47 where is the phase kicking rate which will inuence the learning rate of the algorithm Methods for exponentiat ing various loss functions will be described below when discussing particular applications Finally after evaluat ing the loss function we transmit the eect of the phase gate back to the parameters of the algorithm by perform ing a backpropagation step consisting of the application of the inverse of the feedforward step namely U This backward quantum propagation of phase errors will be referred to as Baqprop In all the quantum feedforward and Baqprop QFB circuit is UQFB U ei L U ei L 48 where L U L U can be seen as the loss function operator evolved under the feedforward unitary U Applied to the joint initial state of the parameters and the training example input state 0 we get UQFB 0 X 0 ei L 49 We can view this output state as a superposition of ancillaassisted phase gates on the parameters by decom posing the operator L for xed into its eigen basis which we will denote as L Then if we decompose the input state in this basis P we have UQFB 0 X ei0 50 We see that the QFB algorithm acts as a nonlinear phase gate for the parameters in each branch of Notice that if is an eigenstate of the operator L in the sense that L for all then the QFB algorithm acts as a pure phase kick In particular we will show in Section V A 2 that this generally occurs when training neural networks for classical data learning on a quantum computer In the generic case the QFB algorithm UQFB causes the parameter and computational registers to become en tangled Since the purpose of the training data is to play an auxiliary role to guide the training of the parameters let us focus solely on the eect the QFB algorithm has on the parameters Not only will the momenta be shifted but the entanglement between the parameters and the computational registers will cause the parameter wave function to decohere as can be seen from the channel trC h ei L0 ei Li X i Ai0 A i 51 where 0 00 Ai i ei L are the Kraus operators for the channel and ii is an ar bitrary basis for HC We see that the decoherence can be12 interpreted as due to a noisy measurement of the param eters by the computational registers which causes them to become entangled Generically this will have the eect of causing phase decoherence in the parameter eigenba sis and increasing the uncertainty in the values of the parameters To minimize the eect of this decoherence one must train the algorithm slowly ie tune the learn ing rate to be suciently small Then if we expand the above channel perturbatively in we see that it is unitary to rst order trC h 1 i L 0 1 i L i 0 iL 0 O2 52 with an eective Hamiltonian L L 53 Therefore we see that insofar as the parameters are con cerned the QFB algorithm acts as an eective unitary phase gate eiL to rst order in Any decoherence does not occur until higher orders in For the conve nience of notation in the following we will use eiL to denote the eect of the QFB algorithm on the param eters and it should be understood that it is valid only to rst order in Now let us examine how the momenta of the parame ters are aected by this eective phase gate 7 eiL eiL O2 L O2 54 We see that to rst order in the momenta are kicked according to the gradient of the loss function This gra dient update can be interpreted as an eective force on the parameters in the direction of decreasing values of the loss function We will elaborate upon this analogy in the Section III A 3 We leave as future work a more careful analysis of the open systems nature of the parameterdata interactions In particular of interest would be to elaborate upon the decoherence at higher orders in and frame the problem in terms of repeated interactions between the parameters with multiple data points 2 Fullbatch Eective Phase Kicks Above we considered eective phase kicks for an ab stract state which is associated with an input ex ample on the computational space and an abstract loss function L Now let us examine this again with some more emphasis on the machine learning aspects of these phase kicks without yet examining the details of par ticular applications Specically we will consider how multiple loss function phase kicks can be batched over a dataset in order to induce a cost function as an eec tive phase Here we will only consider batching the full dataset whereas later Section IV A we will discuss more rened techniques for combining kicks from multiple data points We illustrate the concept for inputoutput pairs of data which would occur in supervised learning but as we will show in later sections it can extend to many other cases including unsupervised scenarios and Hamiltonian optimization A classical dataset for a supervised learning prob lem consists of a collection of inputoutput pairs xj yjjB which we will assume to be real vectors In this setting we consider the computational Hilbert space Hc to be partitioned into an input space as well as an auxiliary work space so that respectively Hc Hi Hw If one is training via superpositions of classi cal data points it would be necessary to assign a Hilbert space for the outputs as well Before the QFB procedure is applied for a single data point xj yj the input state on Hi must be prepared in a computational basis state corresponding to the input xj For an initially blank in put register 0i we can apply the classicallycontrolled unitary Uixj eixjpi 0i 7 xji Once this state is prepared we apply the QFB algorithm as above to the combined parameter and computational spaces H Hc with the parameters initialized to some state 0 P 0 Because in a supervised learning problem the loss function will depend on the output data point yj the exponentiated loss function occurring after the feedforward operation will generally be a classically controlled unitary where the classical control registers are those which store the desired output yj We will la bel the classicallycontrolled loss function as Lyj Af ter the uncomputation step we can also uncompute the state preparation by acting U i xj eixjpi It turns out that this will indeed uncompute the state prepara tion because as we mentioned above in the case of an embedded classical machine learning problem the com putational registers are left unchanged at the end of the QFB algorithm so we get a perfect unitary phase kick of the loss function and hence the parameter and computa tional registers are left unentangled Again the details of this fact will be provided in V A 2 As a whole this procedure applied onto the initial state 0 0c yields U i xj U ei Lyj U Uixj 0 0c eiLxjyj 0 0c 55 where Lxj yj U i xj U Lyj U Uixj In Figure 3 we represent this classicaldatainduced phase kick for a single data point In the same gure we repre sent pictorially how the eective phase kick amounts to an operation strictly on the parameters using the com putational registers eectively as an auxiliary space to assist the phase kick13 Figure 3 Quantum Feedforward and Baqprop procedure for a backpropagating loss phase error of a single classical data point The eective phase kick is an exact unitary on the parameter registers Note that for this diagram and throughout this paper the rate hyperparameters will be labelled as in this case the phase kicking rate is considered a hyperparameter Repeating this procedure for subsequent data points is straightforward rst prepare the input state to a computational state representing the input data point xj1 then apply the QFB algorithm with the classically controlled loss function Lyj1 and nally uncompute the input state preparation As one proceeds through the dataset xj yjjB the phase kicks on the parameter registers accumulate resulting in a total phase kick eiJ 0 0c 56 where we have dened the average cost function for clas sical data J 1 B X jB Lxj yj 57 Here we have also redened to be the total phase kick ing rate of the batch and B the phase kicking rate normalized by the batch size which would appear in the exponentiated loss functions for the individual data points This accumulation of phase kicks for the dataset is illustrated in Figure 4 For quantum data the procedure is slightly dier ent due to the fact that the state preparation at the input of the parametrized algorithm and the exponen tiated loss function must be controlled from quantum data sources rather than classical Consider for illus tration a case of supervised quantum data learning where we are handed a set of inputoutput pairs of quan tum states i j o j jB which are states on respective Hilbert spaces Hij Hoj We consider beginning with a blank computational register 0c and swap in an input state i j from the dataset using the input preparation unitary Uii j Once again we being with a certain state for the parameters 0 P 0 and apply the parametric unitary U onto the compute and parame ter Hilbert space jointly After this a certain exponential of a loss operator dependent on the desired output state Ljo j is applied In general generating an exponenti ated loss depending on the state o j can consume multiple copies of o j We refer the reader to Section VI for specic examples of quantum data learning problems and to Sec tion VI B for particular examples of statedependent loss Figure 4 Phase kick batching By sequentially applying the Quantum Feedforward and Baqprop for multiple data points in the full batch B we can act an eective phase kick accord ing to the cost function for the full data set by accumulating the loss exponentials Here B is the phase kicking pa rameter normalized by the batch size The registers labelled as HP P C and D correspond to the hyperparameters the parameters the compute and the data registers respectively Note that most diagrams in Sections III and IV will use clas sical data registers but all protocols also work for quantum data kicking functions After the loss function is applied the para metric unitary is uncomputed and the quantum data preparation unitary is uncomputed in order to establish a fresh compute register for the next iteration The data registers Hi j Ho j are also discarded after generating the eective phase kick In all the algorithm schematically consists of the transformation U i i j U ei Ljo j U Uii j 0 0c trc 7 eiLi jo j 0 O2 58 where eiLi jo j 0c U i i j U ei Ljo j U Uii j 0c 5914 Figure 5 Quantum Feedforward and Baqprop procedure for a backpropagating loss phase of a single quantum data point The eective phase kick is eectively a unitary phase kick on the parameters to rst order in We use a swapcontrol symbol for both the input and output for the input the data is fully swapped onto the compute registers whereas for application of an outputdependent phase kick the swapcontrol symbolizes the consumption of multiple copies from state exponentiation which we treat indepth in Subsection VI B This swapping out of the computational register is im portant for quantum data problems since the QFB proce dure generally does not entirely disentangle the compu tational and parameter registers By swapping out and discarding the output of the computational register we are tracing out this register and as such on average to rst order in the expected value of the phase kick is that of 53 We present a quantum data QFB procedure pictorially in Figure 5 With this procedure it is again straightforward to accumulate phase kicks for multiple data points Af ter applying the algorithm for the full data batch i j o j jB we obtain a total eective phase kick eiJ 0 0c 60 where we have dened the average eective cost function for quantum data as J 1 B X jB Li j o j 61 and where is the total phase kicking rate Again we emphasize that this phase kick is only valid to rst order in Now that we have an abstract method for obtain ing the gradient of the eective cost function for the parametrized quantum algorithm next is to examine methods for making use of these gradients to update the weights 3 Eective Forces In the previous section we saw that for any parametrized quantum programs we can consider using a set of quantum registers for the parameters We can ini tiate the parameters in a superposition of values and by applying a forward computation a loss function phase kick and an uncomputation we can push the wavefunc tion in the momentum basis according to the gradient of the eective loss function Let us consider how these momentum kicks can be in terpreted from the perspective of classical Hamiltonian dynamics Consider the Hamiltonian Hq p function where q is a position variable and p its conjugate mo mentum In the simplest cases the Hamiltonian is com posed of a sum of two terms the kinetic term T and the potential term V In many physical scenarios the kinetic term is strictly a function of the momentum eg Tp pp 2m where m is the mass parameter while the potential term V q is strictly a function of position Hence we can write Hq p Tp V q In canoni cal Hamiltonian mechanics the change in the momentum per unit time is given by p qH hence p qV q for the case above For a suciently small unit of time the change in the momentum vector is proportional to the negative gradient of the potential p qV q t We can compare this to the nitedierence version of Newtons second law where the change in momentum per unit time is dened as a force p F t and we see the force is then equal to the negative gradient of the potential F qV q Notice that in 54 the change in momentum for a single data point is propor tional to the kicking rate times the negative gradient of the eective loss function L After batching multiple data points the momentum kicks accumulate to change the momentum according to the negative gradient of the total eective cost function 7 J O2 62 Thus in our physics analogy the cost function acts as a potential for the weights with parameters and conju gate momenta playing the respective roles of position and momentum q and p Therefore we see that the momentum shifts induced by the batched quantum feedforward and Baqprop pro cedure QFB can be seen as analogous to applying a force onto the parameters with playing the role of the time step parameter and J the potential Note that a momentum kick is not a step of gradient de scent just as a forceinduced momentum update does15 not alone cause a particle to move For this one must also take into account the remaining Hamilton equa tions which in canonical scenarios give the change in the position per unit time as q pH In the case above where Hq p Tp V q this is given by q pTp 1 mp For a small time step t the po sition is updated according to q 1 mpt Therefore any update in the momentum is channelled to the posi tion via the kinetic term We do not yet have an analogue of this kinetic term for training quantum machine learn ing programs In the next section we will introduce such terms in order to use the force induced by the batched QFB algorithm to update the values of the parameters B Quantum Dynamical Descent The idea behind Quantum Dynamical Descent QDD is to simulate Schrodinger timeevolution of the weights as they would evolve if they were particles moving un der the inuence of the potential induced by the cost function Dynamical timeevolution of a quantum me chanical system is governed by a Hamiltonian operator similar to classical Hamiltonian mechanics but instead of a scalar function we have an operator for the Hamilto nian For a timeindependent Hamiltonian H the time evolution operator is simply U exp i H while for a timedependent Hamiltonian H with as the time parameter the Schrodinger equation dictates that the time evolution operator is a timeordered exponential of the form U T expi R 0 d H Now in the previous subsection we established that our cost function J was analogous to a potential term V in the Hamiltonian as derived from the momentum update rule In fact since the cost function J is an operator on the Hilbert space of the parameters it is more akin to an operatorvalued potential term which would appear in the Hamiltonian operator H T V Thus for QDD we want to introduce a kinetic term along with the cost function potential in order to construct a Hamiltonian operator under which we can evolve the pa rameter wavefunction Let us consider a timedependent Hamiltonian H 1 2m 2 J 63 where is the time parameter and 2 T No tice that we consider a standard kinetic term strictly de pendent on the momentum but with a timedependent mass parameter This is a standard Hamiltonian for Schrodinger dynamics of a single particle in N spatial di mensions apart from the timedependent mass where N is the number of parameters Using a timevarying mass is less standard but it will allow us to control the rate of descent of the potential landscape as the descent proceeds The optimization of how to initialize this pa rameter and its rate of change will fall into the category of hyperparameter optimization tasks We will exam ine a few approaches for the design of the mass function one inspired by the Quantum Approximate Optimization Algorithm and another by the Quantum Adiabatic Al gorithm 1 Core Algorithm The Quantum Dynamical Descent QDD algorithm consists of applying alternating pulses as in the Trot terization of the time evolution generated by a time dependent Hamiltonian of the form H T V In other words the algorithm consists of applying opera tions which mimic a nitetimestep quantum simulation of the Hamiltonian in 63 which has the cost function as the potential and a timevariable mass kinetic term More explicitly the timeordered exponential gener ated by the Hamiltonian 63 is approximated with a product of single timestep exponentials To describe this mathematically rst partition the time interval of interest I R into subintervals Ik k k1 I with k1 k k1 for all k and I kIk The evo lution associated with each subinterval will be called an epoch since it corresponds to a gradient and parameter update for the full batch of data Alternative batching schemes will be discussed in Subsection IV A With this partitioning of the time interval we can approximate the timeevolution operator by decomposing it into a prod uct of timeevolution operators generated by averaged operators on each subinterval U T exp i Z I d H Y k eik Hk 64 where k k1 k is the length of subinterval Ik and Hk R Ikd H is the averaged Hamiltonian for the kth time interval Note that the above expression is approximate since the timedependent mass prevents the Hamiltonian from commuting with itself at dier ent times We proceed by using the LieSuzukiTrotter formula to divide the exponential of the Hamiltonian in each subinterval into that of the cost potential and ki netic terms U Y k eik Hk Y k eik 22mkeikJ 65 where mk R Ikdm11 is the inverse averaged inverse mass In a following subsubsection we argue that a small time step ne temporal partition with a slowly decreas ing mass parameter can yield the minimum of J through the adiabatic theorem In general it is up to the discretion of the practitioner to determine an appli cable time step and mass parameter schedule In clas sical machine learning this process of nding the opti mal initializations and optimal learning rates are a part of a process called hyperparameter optimization Algo rithms for hyperparameter optimization are often called16 metalearning algorithms In Subsubsection IV D we of fer a set of quantum metalearning algorithms for Quan tum Dynamical Descent Since we will generally optimize the parameters for each pulse we can write the unitary corresponding to the quantum dynamical descent as Uqdd Y k eik 2eikJ 66 where we call each parameter k the phase kicking rate and k the kinetic rate for epoch k We will argue in Subsubsection III B 4 that a heuristic one may wish to use for choosing the phase kicking rate and kinetic rate is to begin with k k for small k early time and for large k late time shift towards k k Later we will discuss how beginning with a large kinetic rate and transitioning to a relatively larger phase kicking rate aids in converging to a local minimum Recall that the cost function for each epoch is the loss function for each data point averaged over the en tire batch dataset That is for a batch set B J 1 B X jB Lj 67 Above we wrote the loss function in supervised classical learning for a data point xj yj as Lxj yj Simi larly for supervised quantum data learning the loss func tion was denoted Li j o j for a data point i j o j Here we will simply denote the loss function as Lj for a data point classical or quantum indexed by points in the data batch j B For each epoch we can split the exponential of the cost function into a product of exponentiated loss functions for each data point in the batch eikJ Y jB ei k B Lj 68 Recall that each of these exponentials in turn consist of applying an iteration of QFB feedforward classical or quantumcontrolled loss pulse and backpropaga tionuncomputation The above decomposition pre sumes an application of QFB for each data point sequen tially Later in Section IV A we will examine methods for parallelizing this phase accumulation Now for Quantum Dynamical Descent as seen in 66 we need to alternate between phase kicks and kinetic term exponentials Recall that is a vector of param eters with nth component n and the conjugate mo mentum with components n F n n F n where we use F n to denote the Fourier transform on the nth component For the kinetic term exponentials be cause of the commutation relation n n 0 hence n n 0 one can apply all kinetic terms in parallel on the dierent parameter registers eik 2 O n eikn2 O n F neikn2 F n 69 Therefore the depth of this part of the circuit is strictly dependent upon the qubital precision of the parameter registers in the case of simulated continuous parameters and on the speed of the analog Fourier transform in the case of continuous parameters Recall that the Hilbert space upon which this algo rithm is being run is a tensor product of the parameter and computational Hilbert spaces H Hc The initial state on the computational space is the blank state 0c whereas the parameters will generically be initialized to some Gaussian pointer state 0 1 det1420 ei0e 1 4 0T 1 0 0 70 where 0 is the initial mean for the wavefunction 0 its initial momentum and 0 any initial correlations be tween the parameters often they will be chosen as uncor related hence 0 will be diagonal Some implications of dierent choices for the initial wavefunction parame ters 0 0 0 will be discussed below in the context of the adiabatic theorem We will nd it useful when discussing hyperparameter optimization to also include preparation of this initial parameter wavefunction as a part of the QDD algorithm Hence we will write a uni tary Up where 0 0 0 to denote the preparation of the parameter wavefunction 0 from some initial reference state This preparation unitary can be thought of as classically controlled by the hyper parameters A circuit diagram for two iterations of QDD including the initial state preparation is provided in Figure 6 Figure 6 Two iterations of Quantum Dynamical Descent The descent rate hyperparameters are represented by the classical vector while the preparation hyper parameters are represented by 0 0 0 The uni tary Up prepares the pointer state of the parameters as in equation 70 and is thus dependent on these preparation hyperparameters In this diagram and in most diagrams for Section III we use classical registers to represent the data but one may as well use quantum data registers the latter of which might require a fresh swapin after every phase kick Additionally we use HP P C and D to denote the hyper parameter parameter compute and data registers respec tively17 2 Heisenberg Picture Update rule We can now derive a Heisenberg picture update rule for singleepoch update for the parameters under QDD Re call we derived the update rule for the momentum under conjugation by the cost exponential eiJ eiJ J O2 71 ie the momentum is shifted by the negative gradient of the cost function up to second order error in the kicking rate Conversely the exponential kinetic term shifts the value of the parameter operator ei 22 ei 22 72 We see that the parameter operator is updated according to the momentum times the kinetic rate The QDD algorithm applies 71 followed by 72 hence we can derive how the parameters are updated according to the gradient to rst order in the kicking rate eiJ ei 22 ei 22eiJ J O2 73 We thus see from the Heisenberg picture that the pa rameters get updated according to the momentum and gradient of the cost function at each epoch Note that attempting to recursively conjugate the al ternating operators of QDD analytically in the Heisen berg picture rapidly becomes intractable unless the cost function can be approximated as a secondorder polyno mial This is due to the fact that the QFB unitary phase kick is a nonGaussian operation and keeping track of the operators as they evolve through such a set of non Gaussian operations gets exponentially hard It is well known that Gaussian operations are classically eciently simulatable 58 whereas nonGaussian operations gen erally are not Letting the parameters coherently descent the cost landscape leads to a state of complexity which can be seen as classically hard to simulate As an ad ditional inclination to suspect that Quantum Dynamical Descent is also classically hard to simulate as we describe immediately below it can be seen as a type of Quantum Approximate Optimization Algorithm QAOA and this class of algorithms has been proven to be classically hard to simulate 40 3 Connections to QAOA Here we will relate the Quantum Dynamical Descent QDD approach to the Quantum Approximate Opti mization Algorithm QAOA 39 and its most general form the Quantum Alternating Operator Ansatz 41 First we will review the QAOA algorithm QAOA is a quantumclassical hybrid algorithm in which a classical optimizer is used to optimize the hyperparameters for a sequence of alternating operators generated by non commuting Hamiltonians One usually denes a cost Hamiltonian Hc and a mixer Hamiltonian Hm The goal of QAOA is to nd states of low cost expectation value Hc The algorithm begins with a simple initial state eg the ground state of Hm and applying the sequence of alternating unitaries Uqaoa P Y k1 eik Hmeik Hc 74 where the number of pulses P is xed The set of pa rameters kP k1 and kP k1 are optimized classically to minimize the expectation Hc which is usually estimated using multiple runs and measurements The optimization is done in a feedback loop between the quantum and classical computers the quantum computer is seen as a blackbox in which one inputs parameters and obtains a scalar corresponding to the expectation value As such the classical optimizer is always using a nite dierence algorithm whether it is using NelderMead gradient descent or particle swarms Typically the cost Hamiltonian is diagonal in the com putational basis and the mixer Hamiltonian is a genera tor of shifts of computational basis states For example a typical cost Hamiltonian seen for QAOA with qubits 39 would be of the form Hc P jk jk Zj Zj P j j Zk which is a coupling of standard basis observables This choice of cost function is typical for any Quadratic Un constrained Binary Optimization QUBO problem A typical choice of mixer Hamiltonian would be Hm P j Xj ie an uncoupled sum of generators of shifts of the standard basis In our case for machine learning the parameters we are optimizing are often continuous real values hence the choice of cost and mixer Hamiltonian has to be adapted but we can repeat this same pattern of using a cost that is a polynomial of standardbasis diago nal operators and a mixer which is the sum of generators of shifts of each register Comparing 66 to 74 we see that the Quantum Dy namical Descent unitary is of the form of the Quantum Alternating Operator Ansatz with cost Hamiltonian be ing the eective phase shift induced on the parameters by the QFB Hc J and the mixer Hamiltonian made up of generators of shifts of each register Hm 2 ie the kinetic term we pulse to perform parameter shifts Since the QAOA algorithm consists both of applying the unitaries and also optimizing the pulse lengths we see that we can consider the hyperparameter optimization of Quantum Dynamical Descent to be a case of a QAOA problem where optimizing the kinetic rates kP k1 and phase kicking rates kP k1 would count as a metalearning We discuss in depth how to perform meta learning in Section IV D As mentioned in the last subsubsection III B 2 in certain cases classically simulating the QDD for certain superquadratic eective cost functions becomes rapidly18 intractable since this would eectively be a task of sim ulating nonGaussian operations As we have detailed above QDD is like a continuousvariable QAOA prob lem and QAOA itself has been shown to potentially demonstrate a quantum advantage for certain optimiza tion problems 40 We leave as future work a formal proof of whether this quantum advantage result can be extended to QDD and whether this is achieved only in the cases where the eective potential is nonGaussian Finally QAOA is technically inspired from the Quan tum Adiabatic Algorithm it can be considered as a tem porally coarsegrained variationally optimized quantum simulation of adiabatic evolution In the limit of many pulses one can show that there exists a solution for the hyperparameters which converges to a quantum simula tion of adiabatic evolution hence eectively a quantum simulated implementation of the Quantum Adiabatic Al gorithm In the next subsection III B 4 we establish a similar limit for the QDD algorithm ie a limit where many pulses are applied and show how it can be seen as a quantum simulation of a continuousquantumvariable adiabatic evolution This will be useful to discuss con vergence and to use the physical intuition to derive a heuristic for the initialization of hyperparameters 4 Adiabatic Limit Before we can connect to the adiabatic theorem let us briey review the Quantum Adiabatic Algorithm QAA Suppose we would like to nd the ground state of a certain cost Hamiltonian Hc We begin in a ground state g0 of some simpler Hamiltonian H0 Then we evolve the system with the unitary generated by a time dependent interpolating Hamiltonian of the form H 1 T H0 T Hc where 0 T is the time pa rameter Explicitly this evolution operator is given by the timeordered exponential T expi R T 0 d H In this scenario the adiabatic theorem states that for su ciently long time T the evolution will track the instan taneous ground state g for all times 0 T More precisely the condition for T is given by T 12 where g is the smallest ground state energy gap of the interpolating Hamiltonian g min g There exist more general statements of the adiabatic theorem 59 We leave an examination of the connection of QDD to these more general versions to the zealous reader We can consider a set of cases where our Quantum Dynamical Descent obeys the adiabatic theorem So far we have yet to specify which initial states are best for the quantum parameter wavefunction however to ap ply the adiabatic theorem we will have to pick an initial Hamiltonian and hence a corresponding ground state We will work in the continuousvariable formalism but much transfers over to the qubital case through quan tum simulation theory We can pick an initial Hamiltonian for N quantum harmonic oscillators for N parameters with hyper parameters m0 k0 and 0 H0 1 2m0 2 k0 2 02 75 The corresponding ground state is the Gaussian wave function given by 019 time interval 0 T into subintervals Ik k k1 with k1 k k1 for all k and I kIk For convenience let us also denote k k1 k k k1 k2 and the timeaveraged Hamiltonian on the subinterval Ik as Hk R Ik d H Then the evolution operator over the interval 0 T generated by the interpolating Hamiltonian is UT Y k eik Hk Y k eik 2eikJ eik02 80 where k k 1 kT 2m0 kT 2mc 81 k kkT 82 k k1 kTk0 2 83 We see that the adiabatic evolution can be split up into a kinetic term a batched QFB exponential and an extra phase term which can be interpreted as a fading parame ter regularization term Therefore we see that this evolu tion can be viewed as a regularized QDD algorithm We will refrain from discussing regularization until we treat it in detail in Section IV C 1 We see that as T the coecient of the kinetic term becomes that associ ated with the cost mass while the weight decay term fades and the QFB phase kicking rate dominates Thus starting with a small mass parameter m0 and ending in a much larger mass mc we get an eective kinetic rate k that is decreasing while we get an increasing QFB phase kicking rate Although in practical scenarios where one may want a local minimum rather than a global mini mum one may want to ignore the adiabatic condition The intuition of starting with a large uncertainty state a large kinetic rate and slowly increasing the phase kick ing rate relative to the kinetic rate can be very useful for practitioners manually optimizing hyperparameters Now let us examine the nal cost Hamiltonian close to a local minimum Let us assume that there is a local Taylor series expansion for the potential J around this point in parameter space Because it is a local optimum the rst order term J should vanish giving us J J 1 2 T20 than the gap of the interpolating Hamiltonian To show that the interpolating Hamiltonian has a ground state gap will generally not be possible However if the po tential in the initial Hamiltonian is centered near the op timum of the cost potential ie 0 then we can use the Taylor expansion 84 of the cost potential and approximate the interpolating Hamiltonian as H 2 2m 1 20T K0E0 87 where m h21 of the momentum in order to obtain an estimate for the gradient By studying the statistics of each run in the Schrodinger picture we can assess how many runs are necessary to obtain a particular precision for the gradient estimate In the Schrodinger picture the QFB algorithm acts as a nonlinear phase shift on the initial wavefunc tion 0 7 eiJ 0 95 If the wavefunction is suciently localized in parameter space around the mean 0 then we can Taylorexpand the cost function J J 0 0T J 0 O 02 96 and the QFB algorithm can be approximated as a linear phase shift according to the gradient of the cost function at the point 0 eiJ 0 1 20ei0J 0e 1 4 0T 1 0 0 97 Ignoring global phases Of course this is equivalent to shifting the average of the momentum eiJ 0 20 14 ei0 e0J 0T 00J 0 98 Now that we have shown how the wavefunction gets shifted in momentum space the next step in the algo rithm is to measure the wavefunction in the momen tum basis to get an estimate for the average gradient To perform this measurement one simply applies the Fourier transform on each of the parameter registers F N n1 F n and subsequently measures the state in the standard parameter basis From the Born rule we see that this measurement will draw a sample from the probability distribution p eiJ 0 2 For the case where the wavefunction is highly localized in parameter space this distribution is approximately Gaussian eiJ 0 2 20 12 e20J 0T 00J 0 99 with mean 0 J 0 and covariance 1 41 0 Supposing that we perform r measurement runs our pointestimate of the mean of the momentum can be seen as a sample from the normal distribution N0 J 0 1 41 0 r Thus with M runs and initial pa rameter covariance 0 2 I we can estimate the com ponents of the average gradient each with a precision standard deviation 12r We see that increasing the uncertainty in the initial parameter space wavefunc tion allows for quadratically fewer runs to estimate the gradient within the same precision In the general case without the simplifying assumption of the wavefunction being highly localized in the param eter eigenbasis the outcomes of multiple measurement runs can be combined into a sample mean to generate an estimate of the average momentum Given that the orig inal momentum expectation value 0 is known since it is a parameter of the initial pointer state preparation and the average momentum is updated as 0 0 7 0 J 0 O2 100 where we have written J 0 0 J 0 From an estimate of the average momentum we can ex tract an estimate for the average gradient to rst order in J 0 Once we have estimated this average gradient to the desired precision we use the result to reinitial ize the wavefunction with a new set of parameters 0 0 0 in order to repeat this process at the next epoch The momentum parameter is updated according to the shift of the average momentum induced by the cost potential phase kick ie 0 7 0 J 0 101 Then we use this new momentum to update the param eter value 0 7 0 0 J 0 102 Note the appearance of the hyperparameters and These are in direct analogy to their counterparts in the Quantum Dynamical Descent algorithm from the previ ous subsection We will provide a visual comparison of the phase space behavior of both MoMGrad and QDD in the following subsection An alternative to the above update rule is to discard the momentum between updates and always initialize the momentum of the wavefunction to zero 0 0 In this case we simply update the parameter values accord ing to the rule 0 7 0 J 0 103 Then we recover the classical update rule for gradient descent without momentum Here the learning rate is a product of the phase kicking and kinetic rates Above we discussed the fact that increasing the un certainty in the parameter wavefunction will allow for a more precise estimate of the average gradient using fewer measurement runs However of course increasing the un certainty means that the wavefunction is no longer well22 localized in parameter space Therefore even if we ob tain a precise estimate of the average gradient J 0 Z d02J 104 the information we obtain is only a coarsegrained indi cation of the direction in which the parameters should be shifted in order to achieve the minimum of the cost func tion Ideally as in the case of QDD one would like to use the full operator J to update the wavefunction in each branch of the superposition over the parameters In the case of MoMGrad we only extract an expectation value for this operator and move the entire wavefunction to be centered at this updated parameter value There fore there is a tradeo which is forced upon us by the uncertainty principle namely that obtaining a more pre cise estimate of the gradient requires less uncertainty in momentum space a sharper momentum pointer state yet this estimate will only contain information about the gradient averaged over a larger region of parameter space Figure 8 Two iterations of Momentum Measurement Gra dient Descent We use the same graphical notation as in gure 6 and 0 0 0 represent the descent rate and preparation hyperparameters respectively The measurementcontrolled adder represents equation 101 while the update of the hyperparameters represents equation 102 as well as the change in choice of preparation and rate hyperparameters for the next epoch The register labels are the same as in gure 6 Once again we picture the procedure for classical data learning but the above is directly extendible to quantum data learning scenarios D Phase Space Visualization In Figure 9 we present a visual sidebyside compar ison of the Momentum Measurement Gradient Descent and Quantum Dynamical Descent algorithms For the purpose of simply illustrating the dierences the gure examines the evolution of the wavefunction for a single continuous variable plotted as a Wigner function de ned in Subsection II A under a simple cubic poten tial J 3 2 In the gure we show the ini tial wavefunctions and then three steps of each of the QDD and MoMGrad algorithms a phase kick a ki netic pulsemeasurement and reinitialization and a sec ond phase kick Both of the algorithms are intialized to the same Gaus sian wavefunction with zero mean position and momen tum 0 0 and 0 0 in our notation from the previ ous sections and initial position uncertainty set to 2 0 1 correspondingly the initial momentum uncertainty is 142 0 14 The rst phase kick according to the cubic potential is also the same for both algorithms ie the momentum is updated by 7 32 2 O2 Note that in the above simulation we use the full phase kick eiJ rather than only the approximation to rst order in The point at which QDD and MoMGrad dier is in the next step After the rst phase kick in QDD the next step is to apply a kinetic pulse in order to evolve the quadrature of the wavefunction so that the parameters are updated according to the momentum kick Under this operation all of the branches of the wavefunction move independently of one another according to how they were shifted by the local gradient For example the edges of the wavefunction in the direction which are initialized in an area of higher slope of the potential are kicked with a greater force than pieces of the wavefunction near the origin This causes these pieces of the wavefunction to update more signicantly after the application of the kinetic pulse Note that in the gure we have chosen relatively large kinetic and phase kicking rates in order to exaggerate the evolution so that one can see more clearly the dierences between the two algorithms In MoMGrad one takes the expectation value of the momentum after the phase kick Since we began with 0 0 this average gives us dJ d Then the step after the phase kick is to reinitialize to a new Gaussian shifted according to this measurement which we show in the third step of MoMGrad in the gure Therefore as opposed to QDD the branches of the wavefunction are not shifted independently but they are all updated in the same manner Note also that since MoMGrad parameter updates are achieved using the average of the potential gradient not the gradient of the average potential there fore for example even if the wavefunction happened to be centered at a saddle point and had zero initial mo mentum the location would still be shifted after the parameter update The last step we show is a second phase kick after the kinetic pulse of QDD and the measurement and reini tialization of MoMGrad Because MoMGrad is reini tialized to a Gaussian state in place of applying the ki netic pulse of QDD it does not retain the nonGaussian features of the evolution induced by the generally non quadratic potential J This is particularly apparent in the rightmost plots of the gure where we see that al though both methods roughly track the same evolution in QDD these nonGaussianities accumulate whereas in MoMGrad they are periodically removed23 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base6427acs0PQBXyLt6qAhVh plC2bPAABHicbZDNSgMxFI Xv1L9aq9a1m8EiuCozbnQpuHFZwf 5AW0omc6cNTJDckcow7yALn0Sd KbCDuTX8W1nog8HFOwr05USaFpS D48io7u3v7B9XD2lG9dnxy2qh3bZ objh2eytT0I2ZRCo0dEiSxnxlkKp LYi2b3i7z3jMaKVDRPMORYhMtEs EZOas9bjSDVrCUvw3hGpqw1rjxPY xTnivUxCWzdhAGY0KZkhwiWVtmF vMGJxCQ4caqbQjorlmqV6ZzYT1 LjiZ6f5UTBl7VxF7qZiNLVs4 X5XzbIKbkdFUJnOaHmq0FJLn1Kc WfVgY5CTnDhg3wu3q8ykzjJNrZm NKZNgMqdz0VOlqCvWsg3d61YtM LHAKpwDhdwBSHcwB08QBs6wCGF 3jzXr1372NVZ8Vb93oGIfwCRY JkTlatexit latexit sha1 base64MklCpv6ElzS9637v8Jhx t1KshwgACA3icbZDNSgMxFI XvFtr1erWTbAIrsqMG10KblxWsD QlpJ73RCk8yQZIQyzCuIO30Sd JzCDuzbRd2NYDgY9zEu7NCVPBjf X9b29re2d3b79yUD2sHR2f1E9rHZ NkmGbJSLRvZAaFxh23IrsJdqpD IU2A2n92XefUZteKe7CzFoaQTxS POqC2tQSvmo3rDbpzkU0IltCApV qjs9gnLBMorJMUGP6gZaYU615U xgUR1kBlPKpnSCfYeKSjTDfL5rQS 6dMyZRot1Rlszdvy9yKo2ZydDdlN TGZj0rzfyfmaj2HOVZpZVGwxKM oEsQkpP07GXCOzYuaAMs3droTFVF NmXT0rU0JNp2iLVU8WrqdgvZVN6F w3A78ZPpQgXO4gCsI4Abu4AFa0 AYGMbzAG7x7r96H97lodMtbVnsGK KfgF37Zuklatexit latexit sha1 base64MklCpv6ElzS9637v8Jhx t1KshwgACA3icbZDNSgMxFI XvFtr1erWTbAIrsqMG10KblxWsD QlpJ73RCk8yQZIQyzCuIO30Sd JzCDuzbRd2NYDgY9zEu7NCVPBjf X9b29re2d3b79yUD2sHR2f1E9rHZ NkmGbJSLRvZAaFxh23IrsJdqpD IU2A2n92XefUZteKe7CzFoaQTxS POqC2tQSvmo3rDbpzkU0IltCApV qjs9gnLBMorJMUGP6gZaYU615U xgUR1kBlPKpnSCfYeKSjTDfL5rQS 6dMyZRot1Rlszdvy9yKo2ZydDdlN TGZj0rzfyfmaj2HOVZpZVGwxKM oEsQkpP07GXCOzYuaAMs3droTFVF NmXT0rU0JNp2iLVU8WrqdgvZVN6F w3A78ZPpQgXO4gCsI4Abu4AFa0 AYGMbzAG7x7r96H97lodMtbVnsGK KfgF37Zuklatexit latexit sha1 base64InjBHp21i7WqX37HyhI C1K9auYACDnicbVDLSsNAFJ 3UV62vqks3g0XoqiRudFlw47KCfU AbymR60wydmYSZiVBCfkHc6Ze4E7 fghi3kmbhW09cOFwzr3ce0QcK aN6347la3tnd296n7t4PDoKRet bTcaodGnMYzUIiAbOJHQNMxwGiQ IiAg79YHZXP0nUJrF8tHMEAFmU oWMkpMIY06ERvXG27LXQBvEq8kDV SiM67jCYxTQVIQznReui5ifEzog yjHPLaKNWQEDojUxhaKokA7WeLW3 N8ZUJDmNlSxq8UP9OZERoPReB7R TERHrdK8TvGFqwlsYzJDUi6XB SmHJsYF4jCVNADZ9bQqhi9lZMI6 INTaelS2BIjMwaomcpuTt57KJu ldtzy35T24jXazTKyKLtAlaiIP3 aA2ukcd1EURegZvaI358V5dz6cz 2VrxSlnztEKnK9fMxOcwlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base6427acs0PQBXyLt6qAhVh plC2bPAABHicbZDNSgMxFI Xv1L9aq9a1m8EiuCozbnQpuHFZwf 5AW0omc6cNTJDckcow7yALn0Sd KbCDuTX8W1nog8HFOwr05USaFpS D48io7u3v7B9XD2lG9dnxy2qh3bZ objh2eytT0I2ZRCo0dEiSxnxlkKp LYi2b3i7z3jMaKVDRPMORYhMtEs EZOas9bjSDVrCUvw3hGpqw1rjxPY xTnivUxCWzdhAGY0KZkhwiWVtmF vMGJxCQ4caqbQjorlmqV6ZzYT1 LjiZ6f5UTBl7VxF7qZiNLVs4 X5XzbIKbkdFUJnOaHmq0FJLn1Kc WfVgY5CTnDhg3wu3q8ykzjJNrZm NKZNgMqdz0VOlqCvWsg3d61YtM LHAKpwDhdwBSHcwB08QBs6wCGF 3jzXr1372NVZ8Vb93oGIfwCRY JkTlatexit latexit sha1 base64EjsSiMoTN3E4MQNRjGs zArFVJcACAnicbZDNSgMxFI Xv1L9aq1a3boJFcFVm3OhScOyov 2BdiZNOGJpkhuSOUYR5B3OmTuB PfwdxbqzsK0HAhnJNybE6VSWP T9b60tb2zu1ferxUD4OayfVtk 0yw3iLJTIx3YhaLoXmLRQoeTc1nK pI8k40uZvlnWdurEj0E05THio60i IWjKzHvtNMajVYF9mEYAl1WK o5qP30hwnLFNfIJLW2Fgphjk1KJ jkRaWfWZ5SNqEj3nOoqeI2zOerFu TCOUMSJ8YdjWTun2RU2XtVEXupq I4tuvZzPwv62UY34S50GmGXLPFoD iTBMyzcZCsMZyqkDyoxwuxI2po YydO2sTIkMnXAsVj1VuJ6C9VY2oX 3VCPxG8OBDGc7gHC4hgGu4hXtoQ gsYjOAF3uDdeUvM9FoyVvWe0pr Mj7gWqn5szlatexit latexit sha1 base64EjsSiMoTN3E4MQNRjGs zArFVJcACAnicbZDNSgMxFI Xv1L9aq1a3boJFcFVm3OhScOyov 2BdiZNOGJpkhuSOUYR5B3OmTuB PfwdxbqzsK0HAhnJNybE6VSWP T9b60tb2zu1ferxUD4OayfVtk 0yw3iLJTIx3YhaLoXmLRQoeTc1nK pI8k40uZvlnWdurEj0E05THio60i IWjKzHvtNMajVYF9mEYAl1WK o5qP30hwnLFNfIJLW2Fgphjk1KJ jkRaWfWZ5SNqEj3nOoqeI2zOerFu TCOUMSJ8YdjWTun2RU2XtVEXupq I4tuvZzPwv62UY34S50GmGXLPFoD iTBMyzcZCsMZyqkDyoxwuxI2po YydO2sTIkMnXAsVj1VuJ6C9VY2oX 3VCPxG8OBDGc7gHC4hgGu4hXtoQ gsYjOAF3uDdeUvM9FoyVvWe0pr Mj7gWqn5szlatexit latexit sha1 base64405LRlbIMykPdTHV71 hSoq8YQACDXicbVBNS8NAEJ 34WetX1aOXxSL0VBIveix48VjRfk Abyma7aZfuJmF3IpSQnyDe9Jd4E6 BnId7dtDrb1wcDjvRlm5gWJFA Zd9vZ2Nza3tkt7ZX3Dw6Pjisnp2 0Tp5rxFotlrLsBNVyKiLdQoOTdRH OqAsk7weR25neuDYijh5xmnBf0V EkQsEoWumh3xSDStWtu3OQdeIVpA oFmoPKT38Ys1TxCJmkxvQ8N0Eox oFkzwv91PDE8omdMR7lkZUceNn81 NzcmVIQljbStCMlfTmRUGTNVge 1UFMdm1ZuJ3m9FMbPxNRkiKP2G JRmEqCMZn9TYZCc4ZyaglWthbCR tTRnadJa2BJpObLmsptTt5qKu ukfVX3Lp371YbtSKxEpzDBdTAg 2towB0oQUMRvAMrDmvDjvzofzu WjdcIqZM1iC8ULYQ2cjQlat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 exit3PRWnDymVNYgvP1C2JNnJElat latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 27acs0PQBXyLt 6qAhVhplC2bPA ABHicbZD NSgMxFIXv1L9aq 9a1m8EiuCozbnQ puHFZwf5AW0omc 6cNTJDckcow7y ALn0SdKbCDuT X8W1nog8HFOwr0 5USaFpSD48io7u 3v7B9XD2lG9dnx y2qh3bZobjh2ey tT0I2ZRCo0dEiS xnxlkKpLYi2b3i 7z3jMaKVDRPMO RYhMtEsEZOas9b jSDVrCUvw3hGpq w1rjxPYxTnivUx CWzdhAGY0KZkh wiWVtmFvMGJxC Q4caqbQjorlmqV 6ZzYT1LjiZ6 f5UTBl7VxF7qZ iNLVs4X5XzbIK bkdFUJnOaHmq0F JLn1KcWfVgY5 CTnDhg3wu3q8yk zjJNrZmNKZNgMq dz0VOlqCvWsg3 d61YtMLHAKpwD hdwBSHcwB08QB s6wCGF3jzXr13 72NVZ8Vb93oG IfwCRYJkTla texit latexi t sha1base64 EjsSiMoTN3E4 MQNRjGszArFVJc ACAnicbZD NSgMxFIXv1L9aq 1a3boJFcFVm3Oh ScOyov2BdiZN NOGJpkhuSOUYR5 B3OmTuBPfwdxb qzsK0HAhnJNy bE6VSWPT9b60t b2zu1ferxUD4 OayfVtk0yw3iLJ TIx3YhaLoXmLRQ oeTc1nKpI8k40u ZvlnWdurEj0E05 THio60iIWjKzH vtNMajVYF9m EYAl1WKo5qP30h wnLFNfIJLW2Fg phjk1KJjkRaWfW Z5SNqEj3nOoqeI 2zOerFuTCOUMSJ 8YdjWTun2RU2X tVEXupqI4tuvZz Pwv62UY34S50Gm GXLPFoDiTBMy zcZCsMZyqkDyox wuxI2poYydO2sT IkMnXAsVj1VuJ6 C9VY2oX3VCPxG8 OBDGc7gHC4hgG u4hXtoQgsYjOAF 3uDdeUvM9Foy VvWe0prMj7gWq n5szlatexit latexi t sha1base64 EjsSiMoTN3E4 MQNRjGszArFVJc ACAnicbZD NSgMxFIXv1L9aq 1a3boJFcFVm3Oh ScOyov2BdiZN NOGJpkhuSOUYR5 B3OmTuBPfwdxb qzsK0HAhnJNy bE6VSWPT9b60t b2zu1ferxUD4 OayfVtk0yw3iLJ TIx3YhaLoXmLRQ oeTc1nKpI8k40u ZvlnWdurEj0E05 THio60iIWjKzH vtNMajVYF9m EYAl1WKo5qP30h wnLFNfIJLW2Fg phjk1KJjkRaWfW Z5SNqEj3nOoqeI 2zOerFuTCOUMSJ 8YdjWTun2RU2X tVEXupqI4tuvZz Pwv62UY34S50Gm GXLPFoDiTBMy zcZCsMZyqkDyox wuxI2poYydO2sT IkMnXAsVj1VuJ6 C9VY2oX3VCPxG8 OBDGc7gHC4hgG u4hXtoQgsYjOAF 3uDdeUvM9Foy VvWe0prMj7gWq n5szlatexit latexi t sha1base64 405LRlbIMykP dTHV71hSoq8YQ ACDXicbVB NS8NAEJ34WetX1 aOXxSL0VBIveix 48VjRfkAbyma7a ZfuJmF3IpSQnyD e9Jd4E6BnId 7dtDrb1wcDjvRl m5gWJFAZd9vZ2 Nza3tkt7ZX3Dw6 Pjisnp20Tp5rxF otlrLsBNVyKiLd QoOTdRHOqAsk7w eR25neuDYijh5 xmnBf0VEkQsEoW umh3xSDStWtu3O QdeIVpAoFmoPKT 38Ys1TxCJmkxvQ 8N0EoxoFkzwv9 1PDE8omdMR7lkZ UceNn81NzcmVI QljbStCMlfTmR UGTNVge1UFMdm1 ZuJ3m9FMbPxN RkiKP2GJRmEqCM Zn9TYZCc4Zyagl lWthbCRtTRnad Ja2BJpObLmsp tTt5qKukfVX3 Lp371YbtSKxEp zDBdTAg2towB0 oQUMRvAMrDmvD jvzofzuWjdcIqZ M1iC8ULYQ2cjQ latexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy ElatexitmVNYgvP1C2JNnJ latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base6427acs0 PQBXyLt6qAhVhplC2bPAABHicbZDNSgMxFIXv1L9aq9a1m8Ei uCozbnQpuHFZwf5AW0omc6cNTJDckcow7yALn0SdKbCDuTX8W1nog 8HFOwr05USaFpSD48io7u3v7B9XD2lG9dnxy2qh3bZobjh2eytT0I2ZR Co0dEiSxnxlkKpLYi2b3i7z3jMaKVDRPMORYhMtEsEZOas9bjSDVrCU vw3hGpqw1rjxPYxTnivUxCWzdhAGY0KZkhwiWVtmFvMGJxCQ4caqbQ jorlmqV6ZzYT1LjiZ6f5UTBl7VxF7qZiNLVs4X5XzbIKbkdFUJn OaHmq0FJLn1KcWfVgY5CTnDhg3wu3q8ykzjJNrZmNKZNgMqdz0VOlq CvWsg3d61YtMLHAKpwDhdwBSHcwB08QBs6wCGF3jzXr1372NVZ8V b93oGIfwCRYJkTlatexit latexit sha1base64EjsSiM oTN3E4MQNRjGszArFVJcACAnicbZDNSgMxFIXv1L9aq1a3boJF cFVm3OhScOyov2BdiZNOGJpkhuSOUYR5B3OmTuBPfwdxbqzsK0H AhnJNybE6VSWPT9b60tb2zu1ferxUD4OayfVtk0yw3iLJTIx3Yha LoXmLRQoeTc1nKpI8k40uZvlnWdurEj0E05THio60iIWjKzHvtNMajV YF9mEYAl1WKo5qP30hwnLFNfIJLW2Fgphjk1KJjkRaWfWZ5SNqEj 3nOoqeI2zOerFuTCOUMSJ8YdjWTun2RU2XtVEXupqI4tuvZzPwv62UY 34S50GmGXLPFoDiTBMyzcZCsMZyqkDyoxwuxI2poYydO2sTIkMnXAs Vj1VuJ6C9VY2oX3VCPxG8OBDGc7gHC4hgGu4hXtoQgsYjOAF3uDdeU vM9FoyVvWe0prMj7gWqn5szlatexit latexit sha1base64EjsSiM oTN3E4MQNRjGszArFVJcACAnicbZDNSgMxFIXv1L9aq1a3boJF cFVm3OhScOyov2BdiZNOGJpkhuSOUYR5B3OmTuBPfwdxbqzsK0H AhnJNybE6VSWPT9b60tb2zu1ferxUD4OayfVtk0yw3iLJTIx3Yha LoXmLRQoeTc1nKpI8k40uZvlnWdurEj0E05THio60iIWjKzHvtNMajV YF9mEYAl1WKo5qP30hwnLFNfIJLW2Fgphjk1KJjkRaWfWZ5SNqEj 3nOoqeI2zOerFuTCOUMSJ8YdjWTun2RU2XtVEXupqI4tuvZzPwv62UY 34S50GmGXLPFoDiTBMyzcZCsMZyqkDyoxwuxI2poYydO2sTIkMnXAs Vj1VuJ6C9VY2oX3VCPxG8OBDGc7gHC4hgGu4hXtoQgsYjOAF3uDdeU vM9FoyVvWe0prMj7gWqn5szlatexit latexit sha1base64405LR lbIMykPdTHV71hSoq8YQACDXicbVBNS8NAEJ34WetX1aOXxSL0 VBIveix48VjRfkAbyma7aZfuJmF3IpSQnyDe9Jd4E6BnId7dtDrb1 wcDjvRlm5gWJFAZd9vZ2Nza3tkt7ZX3Dw6Pjisnp20Tp5rxFotlrLsB NVyKiLdQoOTdRHOqAsk7weR25neuDYijh5xmnBf0VEkQsEoWumh3xSD StWtu3OQdeIVpAoFmoPKT38Ys1TxCJmkxvQ8N0EoxoFkzwv91PDE8om dMR7lkZUceNn81NzcmVIQljbStCMlfTmRUGTNVge1UFMdm1ZuJ3m9 FMbPxNRkiKP2GJRmEqCMZn9TYZCc4ZyaglWthbCRtTRnadJa2BJpO ObLmsptTt5qKukfVX3Lp371YbtSKxEpzDBdTAg2towB0oQUMRvA MrDmvDjvzofzuWjdcIqZM1iC8ULYQ2cjQlatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexitMZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base6427acs0 PQBXyLt6qAhVhplC2bPAABHicbZDNSgMxFIXv1L9aq9a1m8Ei uCozbnQpuHFZwf5AW0omc6cNTJDckcow7yALn0SdKbCDuTX8W1nog 8HFOwr05USaFpSD48io7u3v7B9XD2lG9dnxy2qh3bZobjh2eytT0I2ZR Co0dEiSxnxlkKpLYi2b3i7z3jMaKVDRPMORYhMtEsEZOas9bjSDVrCU vw3hGpqw1rjxPYxTnivUxCWzdhAGY0KZkhwiWVtmFvMGJxCQ4caqbQ jorlmqV6ZzYT1LjiZ6f5UTBl7VxF7qZiNLVs4X5XzbIKbkdFUJn OaHmq0FJLn1KcWfVgY5CTnDhg3wu3q8ykzjJNrZmNKZNgMqdz0VOlq CvWsg3d61YtMLHAKpwDhdwBSHcwB08QBs6wCGF3jzXr1372NVZ8V b93oGIfwCRYJkTlatexit latexit sha1base64EjsSiM oTN3E4MQNRjGszArFVJcACAnicbZDNSgMxFIXv1L9aq1a3boJF cFVm3OhScOyov2BdiZNOGJpkhuSOUYR5B3OmTuBPfwdxbqzsK0H AhnJNybE6VSWPT9b60tb2zu1ferxUD4OayfVtk0yw3iLJTIx3Yha LoXmLRQoeTc1nKpI8k40uZvlnWdurEj0E05THio60iIWjKzHvtNMajV YF9mEYAl1WKo5qP30hwnLFNfIJLW2Fgphjk1KJjkRaWfWZ5SNqEj 3nOoqeI2zOerFuTCOUMSJ8YdjWTun2RU2XtVEXupqI4tuvZzPwv62UY 34S50GmGXLPFoDiTBMyzcZCsMZyqkDyoxwuxI2poYydO2sTIkMnXAs Vj1VuJ6C9VY2oX3VCPxG8OBDGc7gHC4hgGu4hXtoQgsYjOAF3uDdeU vM9FoyVvWe0prMj7gWqn5szlatexit latexit sha1base64EjsSiM oTN3E4MQNRjGszArFVJcACAnicbZDNSgMxFIXv1L9aq1a3boJF cFVm3OhScOyov2BdiZNOGJpkhuSOUYR5B3OmTuBPfwdxbqzsK0H AhnJNybE6VSWPT9b60tb2zu1ferxUD4OayfVtk0yw3iLJTIx3Yha LoXmLRQoeTc1nKpI8k40uZvlnWdurEj0E05THio60iIWjKzHvtNMajV YF9mEYAl1WKo5qP30hwnLFNfIJLW2Fgphjk1KJjkRaWfWZ5SNqEj 3nOoqeI2zOerFuTCOUMSJ8YdjWTun2RU2XtVEXupqI4tuvZzPwv62UY 34S50GmGXLPFoDiTBMyzcZCsMZyqkDyoxwuxI2poYydO2sTIkMnXAs Vj1VuJ6C9VY2oX3VCPxG8OBDGc7gHC4hgGu4hXtoQgsYjOAF3uDdeU vM9FoyVvWe0prMj7gWqn5szlatexit latexit sha1base64405LR lbIMykPdTHV71hSoq8YQACDXicbVBNS8NAEJ34WetX1aOXxSL0 VBIveix48VjRfkAbyma7aZfuJmF3IpSQnyDe9Jd4E6BnId7dtDrb1 wcDjvRlm5gWJFAZd9vZ2Nza3tkt7ZX3Dw6Pjisnp20Tp5rxFotlrLsB NVyKiLdQoOTdRHOqAsk7weR25neuDYijh5xmnBf0VEkQsEoWumh3xSD StWtu3OQdeIVpAoFmoPKT38Ys1TxCJmkxvQ8N0EoxoFkzwv91PDE8om dMR7lkZUceNn81NzcmVIQljbStCMlfTmRUGTNVge1UFMdm1ZuJ3m9 FMbPxNRkiKP2GJRmEqCMZn9TYZCc4ZyaglWthbCRtTRnadJa2BJpO ObLmsptTt5qKukfVX3Lp371YbtSKxEpzDBdTAg2towB0oQUMRvA MrDmvDjvzofzuWjdcIqZM1iC8ULYQ2cjQlatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base6427acs0PQBXyLt6qAhVh plC2bPAABHicbZDNSgMxFI Xv1L9aq9a1m8EiuCozbnQpuHFZwf 5AW0omc6cNTJDckcow7yALn0Sd KbCDuTX8W1nog8HFOwr05USaFpS D48io7u3v7B9XD2lG9dnxy2qh3bZ objh2eytT0I2ZRCo0dEiSxnxlkKp LYi2b3i7z3jMaKVDRPMORYhMtEs EZOas9bjSDVrCUvw3hGpqw1rjxPY xTnivUxCWzdhAGY0KZkhwiWVtmF vMGJxCQ4caqbQjorlmqV6ZzYT1 LjiZ6f5UTBl7VxF7qZiNLVs4 X5XzbIKbkdFUJnOaHmq0FJLn1Kc WfVgY5CTnDhg3wu3q8ykzjJNrZm NKZNgMqdz0VOlqCvWsg3d61YtM LHAKpwDhdwBSHcwB08QBs6wCGF 3jzXr1372NVZ8Vb93oGIfwCRY JkTlatexit latexit sha1 base64MklCpv6ElzS9637v8Jhx t1KshwgACA3icbZDNSgMxFI XvFtr1erWTbAIrsqMG10KblxWsD QlpJ73RCk8yQZIQyzCuIO30Sd JzCDuzbRd2NYDgY9zEu7NCVPBjf X9b29re2d3b79yUD2sHR2f1E9rHZ NkmGbJSLRvZAaFxh23IrsJdqpD IU2A2n92XefUZteKe7CzFoaQTxS POqC2tQSvmo3rDbpzkU0IltCApV qjs9gnLBMorJMUGP6gZaYU615U xgUR1kBlPKpnSCfYeKSjTDfL5rQS 6dMyZRot1Rlszdvy9yKo2ZydDdlN TGZj0rzfyfmaj2HOVZpZVGwxKM oEsQkpP07GXCOzYuaAMs3droTFVF NmXT0rU0JNp2iLVU8WrqdgvZVN6F w3A78ZPpQgXO4gCsI4Abu4AFa0 AYGMbzAG7x7r96H97lodMtbVnsGK KfgF37Zuklatexit latexit sha1 base64MklCpv6ElzS9637v8Jhx t1KshwgACA3icbZDNSgMxFI XvFtr1erWTbAIrsqMG10KblxWsD QlpJ73RCk8yQZIQyzCuIO30Sd JzCDuzbRd2NYDgY9zEu7NCVPBjf X9b29re2d3b79yUD2sHR2f1E9rHZ NkmGbJSLRvZAaFxh23IrsJdqpD IU2A2n92XefUZteKe7CzFoaQTxS POqC2tQSvmo3rDbpzkU0IltCApV qjs9gnLBMorJMUGP6gZaYU615U xgUR1kBlPKpnSCfYeKSjTDfL5rQS 6dMyZRot1Rlszdvy9yKo2ZydDdlN TGZj0rzfyfmaj2HOVZpZVGwxKM oEsQkpP07GXCOzYuaAMs3droTFVF NmXT0rU0JNp2iLVU8WrqdgvZVN6F w3A78ZPpQgXO4gCsI4Abu4AFa0 AYGMbzAG7x7r96H97lodMtbVnsGK KfgF37Zuklatexit latexit sha1 base64InjBHp21i7WqX37HyhI C1K9auYACDnicbVDLSsNAFJ 3UV62vqks3g0XoqiRudFlw47KCfU AbymR60wydmYSZiVBCfkHc6Ze4E7 fghi3kmbhW09cOFwzr3ce0QcK aN6347la3tnd296n7t4PDoKRet bTcaodGnMYzUIiAbOJHQNMxwGiQ IiAg79YHZXP0nUJrF8tHMEAFmU oWMkpMIY06ERvXG27LXQBvEq8kDV SiM67jCYxTQVIQznReui5ifEzog yjHPLaKNWQEDojUxhaKokA7WeLW3 N8ZUJDmNlSxq8UP9OZERoPReB7R TERHrdK8TvGFqwlsYzJDUi6XB SmHJsYF4jCVNADZ9bQqhi9lZMI6 INTaelS2BIjMwaomcpuTt57KJu ldtzy35T24jXazTKyKLtAlaiIP3 aA2ukcd1EURegZvaI358V5dz6cz 2VrxSlnztEKnK9fMxOcwlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n exit4vWDaecOUNLcL5ATRTnQMlat latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base6427acs0 PQBXyLt6qAhVhplC2bPAABHicbZDNSgMxFIXv1L9aq9a1m8Ei uCozbnQpuHFZwf5AW0omc6cNTJDckcow7yALn0SdKbCDuTX8W1nog 8HFOwr05USaFpSD48io7u3v7B9XD2lG9dnxy2qh3bZobjh2eytT0I2ZR Co0dEiSxnxlkKpLYi2b3i7z3jMaKVDRPMORYhMtEsEZOas9bjSDVrCU vw3hGpqw1rjxPYxTnivUxCWzdhAGY0KZkhwiWVtmFvMGJxCQ4caqbQ jorlmqV6ZzYT1LjiZ6f5UTBl7VxF7qZiNLVs4X5XzbIKbkdFUJn OaHmq0FJLn1KcWfVgY5CTnDhg3wu3q8ykzjJNrZmNKZNgMqdz0VOlq CvWsg3d61YtMLHAKpwDhdwBSHcwB08QBs6wCGF3jzXr1372NVZ8V b93oGIfwCRYJkTlatexit latexit sha1base64MklCpv 6ElzS9637v8Jhxt1KshwgACA3icbZDNSgMxFIXvFtr1erWTbAI rsqMG10KblxWsDQlpJ73RCk8yQZIQyzCuIO30SdJzCDuzbRd2NYD gY9zEu7NCVPBjfX9b29re2d3b79yUD2sHR2f1E9rHZNkmGbJSLRvZAa Fxh23IrsJdqpDIU2A2n92XefUZteKe7CzFoaQTxSPOqC2tQSvmo3rD bpzkU0IltCApVqjs9gnLBMorJMUGP6gZaYU615UxgUR1kBlPKpnSC fYeKSjTDfL5rQS6dMyZRot1Rlszdvy9yKo2ZydDdlNTGZj0rzfyfmaj 2HOVZpZVGwxKMoEsQkpP07GXCOzYuaAMs3droTFVFNmXT0rU0JNp2iL VU8WrqdgvZVN6Fw3A78ZPpQgXO4gCsI4Abu4AFa0AYGMbzAG7x7r96 H97lodMtbVnsGKKfgF37Zuklatexit latexit sha1base64MklCpv 6ElzS9637v8Jhxt1KshwgACA3icbZDNSgMxFIXvFtr1erWTbAI rsqMG10KblxWsDQlpJ73RCk8yQZIQyzCuIO30SdJzCDuzbRd2NYD gY9zEu7NCVPBjfX9b29re2d3b79yUD2sHR2f1E9rHZNkmGbJSLRvZAa Fxh23IrsJdqpDIU2A2n92XefUZteKe7CzFoaQTxSPOqC2tQSvmo3rD bpzkU0IltCApVqjs9gnLBMorJMUGP6gZaYU615UxgUR1kBlPKpnSC fYeKSjTDfL5rQS6dMyZRot1Rlszdvy9yKo2ZydDdlNTGZj0rzfyfmaj 2HOVZpZVGwxKMoEsQkpP07GXCOzYuaAMs3droTFVFNmXT0rU0JNp2iL VU8WrqdgvZVN6Fw3A78ZPpQgXO4gCsI4Abu4AFa0AYGMbzAG7x7r96 H97lodMtbVnsGKKfgF37Zuklatexit latexit sha1base64InjBH p21i7WqX37HyhIC1K9auYACDnicbVDLSsNAFJ3UV62vqks3g0Xo qiRudFlw47KCfUAbymR60wydmYSZiVBCfkHc6Ze4E7fghi3kmbhW09 cOFwzr3ce0QcKaN6347la3tnd296n7t4PDoKRetbTcaodGnMYzUI iAbOJHQNMxwGiQIiAg79YHZXP0nUJrF8tHMEAFmUoWMkpMIY06ERvX G27LXQBvEq8kDVSiM67jCYxTQVIQznReui5ifEzogyjHPLaKNWQEDoj UxhaKokA7WeLW3N8ZUJDmNlSxq8UP9OZERoPReB7RTERHrdK8TvGFq wlsYzJDUi6XBSmHJsYF4jCVNADZ9bQqhi9lZMI6INTaelS2BIjMw aomcpuTt57KJuldtzy35T24jXazTKyKLtAlaiIP3aA2ukcd1EUReg ZvaI358V5dz6cz2VrxSlnztEKnK9fMxOcwlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexitfFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base6427acs0 PQBXyLt6qAhVhplC2bPAABHicbZDNSgMxFIXv1L9aq9a1m8Ei uCozbnQpuHFZwf5AW0omc6cNTJDckcow7yALn0SdKbCDuTX8W1nog 8HFOwr05USaFpSD48io7u3v7B9XD2lG9dnxy2qh3bZobjh2eytT0I2ZR Co0dEiSxnxlkKpLYi2b3i7z3jMaKVDRPMORYhMtEsEZOas9bjSDVrCU vw3hGpqw1rjxPYxTnivUxCWzdhAGY0KZkhwiWVtmFvMGJxCQ4caqbQ jorlmqV6ZzYT1LjiZ6f5UTBl7VxF7qZiNLVs4X5XzbIKbkdFUJn OaHmq0FJLn1KcWfVgY5CTnDhg3wu3q8ykzjJNrZmNKZNgMqdz0VOlq CvWsg3d61YtMLHAKpwDhdwBSHcwB08QBs6wCGF3jzXr1372NVZ8V b93oGIfwCRYJkTlatexit latexit sha1base64MklCpv 6ElzS9637v8Jhxt1KshwgACA3icbZDNSgMxFIXvFtr1erWTbAI rsqMG10KblxWsDQlpJ73RCk8yQZIQyzCuIO30SdJzCDuzbRd2NYD gY9zEu7NCVPBjfX9b29re2d3b79yUD2sHR2f1E9rHZNkmGbJSLRvZAa Fxh23IrsJdqpDIU2A2n92XefUZteKe7CzFoaQTxSPOqC2tQSvmo3rD bpzkU0IltCApVqjs9gnLBMorJMUGP6gZaYU615UxgUR1kBlPKpnSC fYeKSjTDfL5rQS6dMyZRot1Rlszdvy9yKo2ZydDdlNTGZj0rzfyfmaj 2HOVZpZVGwxKMoEsQkpP07GXCOzYuaAMs3droTFVFNmXT0rU0JNp2iL VU8WrqdgvZVN6Fw3A78ZPpQgXO4gCsI4Abu4AFa0AYGMbzAG7x7r96 H97lodMtbVnsGKKfgF37Zuklatexit latexit sha1base64MklCpv 6ElzS9637v8Jhxt1KshwgACA3icbZDNSgMxFIXvFtr1erWTbAI rsqMG10KblxWsDQlpJ73RCk8yQZIQyzCuIO30SdJzCDuzbRd2NYD gY9zEu7NCVPBjfX9b29re2d3b79yUD2sHR2f1E9rHZNkmGbJSLRvZAa Fxh23IrsJdqpDIU2A2n92XefUZteKe7CzFoaQTxSPOqC2tQSvmo3rD bpzkU0IltCApVqjs9gnLBMorJMUGP6gZaYU615UxgUR1kBlPKpnSC fYeKSjTDfL5rQS6dMyZRot1Rlszdvy9yKo2ZydDdlNTGZj0rzfyfmaj 2HOVZpZVGwxKMoEsQkpP07GXCOzYuaAMs3droTFVFNmXT0rU0JNp2iL VU8WrqdgvZVN6Fw3A78ZPpQgXO4gCsI4Abu4AFa0AYGMbzAG7x7r96 H97lodMtbVnsGKKfgF37Zuklatexit latexit sha1base64InjBH p21i7WqX37HyhIC1K9auYACDnicbVDLSsNAFJ3UV62vqks3g0Xo qiRudFlw47KCfUAbymR60wydmYSZiVBCfkHc6Ze4E7fghi3kmbhW09 cOFwzr3ce0QcKaN6347la3tnd296n7t4PDoKRetbTcaodGnMYzUI iAbOJHQNMxwGiQIiAg79YHZXP0nUJrF8tHMEAFmUoWMkpMIY06ERvX G27LXQBvEq8kDVSiM67jCYxTQVIQznReui5ifEzogyjHPLaKNWQEDoj UxhaKokA7WeLW3N8ZUJDmNlSxq8UP9OZERoPReB7RTERHrdK8TvGFq wlsYzJDUi6XBSmHJsYF4jCVNADZ9bQqhi9lZMI6INTaelS2BIjMw aomcpuTt57KJuldtzy35T24jXazTKyKLtAlaiIP3aA2ukcd1EUReg ZvaI358V5dz6cz2VrxSlnztEKnK9fMxOcwlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit e f g h 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 5 4 3 2 1 0 1 2 3 4 5 latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base6427acs0PQBXyLt6qAhVh plC2bPAABHicbZDNSgMxFI Xv1L9aq9a1m8EiuCozbnQpuHFZwf 5AW0omc6cNTJDckcow7yALn0Sd KbCDuTX8W1nog8HFOwr05USaFpS D48io7u3v7B9XD2lG9dnxy2qh3bZ objh2eytT0I2ZRCo0dEiSxnxlkKp LYi2b3i7z3jMaKVDRPMORYhMtEs EZOas9bjSDVrCUvw3hGpqw1rjxPY xTnivUxCWzdhAGY0KZkhwiWVtmF vMGJxCQ4caqbQjorlmqV6ZzYT1 LjiZ6f5UTBl7VxF7qZiNLVs4 X5XzbIKbkdFUJnOaHmq0FJLn1Kc WfVgY5CTnDhg3wu3q8ykzjJNrZm NKZNgMqdz0VOlqCvWsg3d61YtM LHAKpwDhdwBSHcwB08QBs6wCGF 3jzXr1372NVZ8Vb93oGIfwCRY JkTlatexit latexit sha1 base64MklCpv6ElzS9637v8Jhx t1KshwgACA3icbZDNSgMxFI XvFtr1erWTbAIrsqMG10KblxWsD QlpJ73RCk8yQZIQyzCuIO30Sd JzCDuzbRd2NYDgY9zEu7NCVPBjf X9b29re2d3b79yUD2sHR2f1E9rHZ NkmGbJSLRvZAaFxh23IrsJdqpD IU2A2n92XefUZteKe7CzFoaQTxS POqC2tQSvmo3rDbpzkU0IltCApV qjs9gnLBMorJMUGP6gZaYU615U xgUR1kBlPKpnSCfYeKSjTDfL5rQS 6dMyZRot1Rlszdvy9yKo2ZydDdlN TGZj0rzfyfmaj2HOVZpZVGwxKM oEsQkpP07GXCOzYuaAMs3droTFVF NmXT0rU0JNp2iLVU8WrqdgvZVN6F w3A78ZPpQgXO4gCsI4Abu4AFa0 AYGMbzAG7x7r96H97lodMtbVnsGK KfgF37Zuklatexit latexit sha1 base64MklCpv6ElzS9637v8Jhx t1KshwgACA3icbZDNSgMxFI XvFtr1erWTbAIrsqMG10KblxWsD QlpJ73RCk8yQZIQyzCuIO30Sd JzCDuzbRd2NYDgY9zEu7NCVPBjf X9b29re2d3b79yUD2sHR2f1E9rHZ NkmGbJSLRvZAaFxh23IrsJdqpD IU2A2n92XefUZteKe7CzFoaQTxS POqC2tQSvmo3rDbpzkU0IltCApV qjs9gnLBMorJMUGP6gZaYU615U xgUR1kBlPKpnSCfYeKSjTDfL5rQS 6dMyZRot1Rlszdvy9yKo2ZydDdlN TGZj0rzfyfmaj2HOVZpZVGwxKM oEsQkpP07GXCOzYuaAMs3droTFVF NmXT0rU0JNp2iLVU8WrqdgvZVN6F w3A78ZPpQgXO4gCsI4Abu4AFa0 AYGMbzAG7x7r96H97lodMtbVnsGK KfgF37Zuklatexit latexit sha1 base64InjBHp21i7WqX37HyhI C1K9auYACDnicbVDLSsNAFJ 3UV62vqks3g0XoqiRudFlw47KCfU AbymR60wydmYSZiVBCfkHc6Ze4E7 fghi3kmbhW09cOFwzr3ce0QcK aN6347la3tnd296n7t4PDoKRet bTcaodGnMYzUIiAbOJHQNMxwGiQ IiAg79YHZXP0nUJrF8tHMEAFmU oWMkpMIY06ERvXG27LXQBvEq8kDV SiM67jCYxTQVIQznReui5ifEzog yjHPLaKNWQEDojUxhaKokA7WeLW3 N8ZUJDmNlSxq8UP9OZERoPReB7R TERHrdK8TvGFqwlsYzJDUi6XB SmHJsYF4jCVNADZ9bQqhi9lZMI6 INTaelS2BIjMwaomcpuTt57KJu ldtzy35T24jXazTKyKLtAlaiIP3 aA2ukcd1EURegZvaI358V5dz6cz 2VrxSlnztEKnK9fMxOcwlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base6427acs0PQBXyLt6qAhVh plC2bPAABHicbZDNSgMxFI Xv1L9aq9a1m8EiuCozbnQpuHFZwf 5AW0omc6cNTJDckcow7yALn0Sd KbCDuTX8W1nog8HFOwr05USaFpS D48io7u3v7B9XD2lG9dnxy2qh3bZ objh2eytT0I2ZRCo0dEiSxnxlkKp LYi2b3i7z3jMaKVDRPMORYhMtEs EZOas9bjSDVrCUvw3hGpqw1rjxPY xTnivUxCWzdhAGY0KZkhwiWVtmF vMGJxCQ4caqbQjorlmqV6ZzYT1 LjiZ6f5UTBl7VxF7qZiNLVs4 X5XzbIKbkdFUJnOaHmq0FJLn1Kc WfVgY5CTnDhg3wu3q8ykzjJNrZm NKZNgMqdz0VOlqCvWsg3d61YtM LHAKpwDhdwBSHcwB08QBs6wCGF 3jzXr1372NVZ8Vb93oGIfwCRY JkTlatexit latexit sha1 base64EjsSiMoTN3E4MQNRjGs zArFVJcACAnicbZDNSgMxFI Xv1L9aq1a3boJFcFVm3OhScOyov 2BdiZNOGJpkhuSOUYR5B3OmTuB PfwdxbqzsK0HAhnJNybE6VSWP T9b60tb2zu1ferxUD4OayfVtk 0yw3iLJTIx3YhaLoXmLRQoeTc1nK pI8k40uZvlnWdurEj0E05THio60i IWjKzHvtNMajVYF9mEYAl1WK o5qP30hwnLFNfIJLW2Fgphjk1KJ jkRaWfWZ5SNqEj3nOoqeI2zOerFu TCOUMSJ8YdjWTun2RU2XtVEXupq I4tuvZzPwv62UY34S50GmGXLPFoD iTBMyzcZCsMZyqkDyoxwuxI2po YydO2sTIkMnXAsVj1VuJ6C9VY2oX 3VCPxG8OBDGc7gHC4hgGu4hXtoQ gsYjOAF3uDdeUvM9FoyVvWe0pr Mj7gWqn5szlatexit latexit sha1 base64EjsSiMoTN3E4MQNRjGs zArFVJcACAnicbZDNSgMxFI Xv1L9aq1a3boJFcFVm3OhScOyov 2BdiZNOGJpkhuSOUYR5B3OmTuB PfwdxbqzsK0HAhnJNybE6VSWP T9b60tb2zu1ferxUD4OayfVtk 0yw3iLJTIx3YhaLoXmLRQoeTc1nK pI8k40uZvlnWdurEj0E05THio60i IWjKzHvtNMajVYF9mEYAl1WK o5qP30hwnLFNfIJLW2Fgphjk1KJ jkRaWfWZ5SNqEj3nOoqeI2zOerFu TCOUMSJ8YdjWTun2RU2XtVEXupq I4tuvZzPwv62UY34S50GmGXLPFoD iTBMyzcZCsMZyqkDyoxwuxI2po YydO2sTIkMnXAsVj1VuJ6C9VY2oX 3VCPxG8OBDGc7gHC4hgGu4hXtoQ gsYjOAF3uDdeUvM9FoyVvWe0pr Mj7gWqn5szlatexit latexit sha1 base64405LRlbIMykPdTHV71 hSoq8YQACDXicbVBNS8NAEJ 34WetX1aOXxSL0VBIveix48VjRfk Abyma7aZfuJmF3IpSQnyDe9Jd4E6 BnId7dtDrb1wcDjvRlm5gWJFA Zd9vZ2Nza3tkt7ZX3Dw6Pjisnp2 0Tp5rxFotlrLsBNVyKiLdQoOTdRH OqAsk7weR25neuDYijh5xmnBf0V EkQsEoWumh3xSDStWtu3OQdeIVpA oFmoPKT38Ys1TxCJmkxvQ8N0Eox oFkzwv91PDE8omdMR7lkZUceNn81 NzcmVIQljbStCMlfTmRUGTNVge 1UFMdm1ZuJ3m9FMbPxNRkiKP2G JRmEqCMZn9TYZCc4ZyaglWthbCR tTRnadJa2BJpObLmsptTt5qKu ukfVX3Lp371YbtSKxEpzDBdTAg 2towB0oQUMRvAMrDmvDjvzofzu WjdcIqZM1iC8ULYQ2cjQlat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 3PRWnDymVNYgvP1C2JNnJElat exit latexit sha1 base64UK2cQojz1etnYU5jgUux mHUte9MACDXicbVDLSgNBEO yNrxhfUY9eBoOQU9gVQY8BLx4jmg ckS5id9CZDZnaXmVkhLPkE8aZf4k 28g1iHcnyR5MYkFDUdVNd1eQCK 6N6347hY3Nre2d4m5pbg8Kh8fN LScaoYNlksYtUJqEbBI2wabgR2Eo VUBgLbwfh25refUGkeR49mkqAv6T DiIWfUWOmh1D9csWtuXOQdeLlpA I5Gv3yT28Qs1RiZJigWnc9NzFRp XhTOC01Es1JpSN6RC7lkZUovaza lTcmGVAQljZSsyZK7ncio1HoiA9 spqRnpVW8mud1UxPeBmPktRgxB aLwlQE5PZ32TAFTIjJpZQpri9lb ARVZQZm87SlkDRMZrpsianNidvNZ V10rqseW7Nu7q1Kt5YkU4g3Oog gfXUIc7aEATGAzhGV7hzXlx3p0P5 exit3PRWnDymVNYgvP1C2JNnJElat latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 27acs0PQBXyLt 6qAhVhplC2bPA ABHicbZD NSgMxFIXv1L9aq 9a1m8EiuCozbnQ puHFZwf5AW0omc 6cNTJDckcow7y ALn0SdKbCDuT X8W1nog8HFOwr0 5USaFpSD48io7u 3v7B9XD2lG9dnx y2qh3bZobjh2ey tT0I2ZRCo0dEiS xnxlkKpLYi2b3i 7z3jMaKVDRPMO RYhMtEsEZOas9b jSDVrCUvw3hGpq w1rjxPYxTnivUx CWzdhAGY0KZkh wiWVtmFvMGJxC Q4caqbQjorlmqV 6ZzYT1LjiZ6 f5UTBl7VxF7qZ iNLVs4X5XzbIK bkdFUJnOaHmq0F JLn1KcWfVgY5 CTnDhg3wu3q8yk zjJNrZmNKZNgMq dz0VOlqCvWsg3 d61YtMLHAKpwD hdwBSHcwB08QB s6wCGF3jzXr13 72NVZ8Vb93oG IfwCRYJkTla texit latexi t sha1base64 EjsSiMoTN3E4 MQNRjGszArFVJc ACAnicbZD NSgMxFIXv1L9aq 1a3boJFcFVm3Oh ScOyov2BdiZN NOGJpkhuSOUYR5 B3OmTuBPfwdxb qzsK0HAhnJNy bE6VSWPT9b60t b2zu1ferxUD4 OayfVtk0yw3iLJ TIx3YhaLoXmLRQ oeTc1nKpI8k40u ZvlnWdurEj0E05 THio60iIWjKzH vtNMajVYF9m EYAl1WKo5qP30h wnLFNfIJLW2Fg phjk1KJjkRaWfW Z5SNqEj3nOoqeI 2zOerFuTCOUMSJ 8YdjWTun2RU2X tVEXupqI4tuvZz Pwv62UY34S50Gm GXLPFoDiTBMy zcZCsMZyqkDyox wuxI2poYydO2sT IkMnXAsVj1VuJ6 C9VY2oX3VCPxG8 OBDGc7gHC4hgG u4hXtoQgsYjOAF 3uDdeUvM9Foy VvWe0prMj7gWq n5szlatexit latexi t sha1base64 EjsSiMoTN3E4 MQNRjGszArFVJc ACAnicbZD NSgMxFIXv1L9aq 1a3boJFcFVm3Oh ScOyov2BdiZN NOGJpkhuSOUYR5 B3OmTuBPfwdxb qzsK0HAhnJNy bE6VSWPT9b60t b2zu1ferxUD4 OayfVtk0yw3iLJ TIx3YhaLoXmLRQ oeTc1nKpI8k40u ZvlnWdurEj0E05 THio60iIWjKzH vtNMajVYF9m EYAl1WKo5qP30h wnLFNfIJLW2Fg phjk1KJjkRaWfW Z5SNqEj3nOoqeI 2zOerFuTCOUMSJ 8YdjWTun2RU2X tVEXupqI4tuvZz Pwv62UY34S50Gm GXLPFoDiTBMy zcZCsMZyqkDyox wuxI2poYydO2sT IkMnXAsVj1VuJ6 C9VY2oX3VCPxG8 OBDGc7gHC4hgG u4hXtoQgsYjOAF 3uDdeUvM9Foy VvWe0prMj7gWq n5szlatexit latexi t sha1base64 405LRlbIMykP dTHV71hSoq8YQ ACDXicbVB NS8NAEJ34WetX1 aOXxSL0VBIveix 48VjRfkAbyma7a ZfuJmF3IpSQnyD e9Jd4E6BnId 7dtDrb1wcDjvRl m5gWJFAZd9vZ2 Nza3tkt7ZX3Dw6 Pjisnp20Tp5rxF otlrLsBNVyKiLd QoOTdRHOqAsk7w eR25neuDYijh5 xmnBf0VEkQsEoW umh3xSDStWtu3O QdeIVpAoFmoPKT 38Ys1TxCJmkxvQ 8N0EoxoFkzwv9 1PDE8omdMR7lkZ UceNn81NzcmVI QljbStCMlfTmR UGTNVge1UFMdm1 ZuJ3m9FMbPxN RkiKP2GJRmEqCM Zn9TYZCc4Zyagl lWthbCRtTRnad Ja2BJpObLmsp tTt5qKukfVX3 Lp371YbtSKxEp zDBdTAg2towB0 oQUMRvAMrDmvD jvzofzuWjdcIqZ M1iC8ULYQ2cjQ latexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy mVNYgvP1C2JNnJ Elatexit latexi t sha1base64 UK2cQojz1etnY U5jgUuxmHUte9M ACDXicbVD LSgNBEOyNrxhfU Y9eBoOQU9gVQY8 BLx4jmgckS5id9 CZDZnaXmVkhLPk E8aZf4k28g1i HcnyR5MYkFDUdV Nd1eQCK6N6347h Y3Nre2d4m5pb g8Kh8fNLScaoYN lksYtUJqEbBI2w abgR2EoVUBgLbw fh25refUGkeR49 mkqAv6TDiIWfUW Omh1D9csWtuXO QdeLlpAI5Gv3yT 28Qs1RiZJigWnc 9NzFRpXhTOC01 Es1JpSN6RC7lkZ UovazalTcmGVA QljZSsyZK7nci o1HoiA9spqRnpV W8mud1UxPeBm PktRgxBaLwlQE 5PZ32TAFTIjJpZ Qpri9lbARVZQZm 87SlkDRMZrpsia nNidvNZV10rqse W7Nu7q1Kt5Yk U4g3OogfXUIc7 aEATGAzhGV7hzX lx3p0P53PRWnDy ElatexitmVNYgvP1C2JNnJ latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base6427acs0 PQBXyLt6qAhVhplC2bPAABHicbZDNSgMxFIXv1L9aq9a1m8Ei uCozbnQpuHFZwf5AW0omc6cNTJDckcow7yALn0SdKbCDuTX8W1nog 8HFOwr05USaFpSD48io7u3v7B9XD2lG9dnxy2qh3bZobjh2eytT0I2ZR Co0dEiSxnxlkKpLYi2b3i7z3jMaKVDRPMORYhMtEsEZOas9bjSDVrCU vw3hGpqw1rjxPYxTnivUxCWzdhAGY0KZkhwiWVtmFvMGJxCQ4caqbQ jorlmqV6ZzYT1LjiZ6f5UTBl7VxF7qZiNLVs4X5XzbIKbkdFUJn OaHmq0FJLn1KcWfVgY5CTnDhg3wu3q8ykzjJNrZmNKZNgMqdz0VOlq CvWsg3d61YtMLHAKpwDhdwBSHcwB08QBs6wCGF3jzXr1372NVZ8V b93oGIfwCRYJkTlatexit latexit sha1base64EjsSiM oTN3E4MQNRjGszArFVJcACAnicbZDNSgMxFIXv1L9aq1a3boJF cFVm3OhScOyov2BdiZNOGJpkhuSOUYR5B3OmTuBPfwdxbqzsK0H AhnJNybE6VSWPT9b60tb2zu1ferxUD4OayfVtk0yw3iLJTIx3Yha LoXmLRQoeTc1nKpI8k40uZvlnWdurEj0E05THio60iIWjKzHvtNMajV YF9mEYAl1WKo5qP30hwnLFNfIJLW2Fgphjk1KJjkRaWfWZ5SNqEj 3nOoqeI2zOerFuTCOUMSJ8YdjWTun2RU2XtVEXupqI4tuvZzPwv62UY 34S50GmGXLPFoDiTBMyzcZCsMZyqkDyoxwuxI2poYydO2sTIkMnXAs Vj1VuJ6C9VY2oX3VCPxG8OBDGc7gHC4hgGu4hXtoQgsYjOAF3uDdeU vM9FoyVvWe0prMj7gWqn5szlatexit latexit sha1base64EjsSiM oTN3E4MQNRjGszArFVJcACAnicbZDNSgMxFIXv1L9aq1a3boJF cFVm3OhScOyov2BdiZNOGJpkhuSOUYR5B3OmTuBPfwdxbqzsK0H AhnJNybE6VSWPT9b60tb2zu1ferxUD4OayfVtk0yw3iLJTIx3Yha LoXmLRQoeTc1nKpI8k40uZvlnWdurEj0E05THio60iIWjKzHvtNMajV YF9mEYAl1WKo5qP30hwnLFNfIJLW2Fgphjk1KJjkRaWfWZ5SNqEj 3nOoqeI2zOerFuTCOUMSJ8YdjWTun2RU2XtVEXupqI4tuvZzPwv62UY 34S50GmGXLPFoDiTBMyzcZCsMZyqkDyoxwuxI2poYydO2sTIkMnXAs Vj1VuJ6C9VY2oX3VCPxG8OBDGc7gHC4hgGu4hXtoQgsYjOAF3uDdeU vM9FoyVvWe0prMj7gWqn5szlatexit latexit sha1base64405LR lbIMykPdTHV71hSoq8YQACDXicbVBNS8NAEJ34WetX1aOXxSL0 VBIveix48VjRfkAbyma7aZfuJmF3IpSQnyDe9Jd4E6BnId7dtDrb1 wcDjvRlm5gWJFAZd9vZ2Nza3tkt7ZX3Dw6Pjisnp20Tp5rxFotlrLsB NVyKiLdQoOTdRHOqAsk7weR25neuDYijh5xmnBf0VEkQsEoWumh3xSD StWtu3OQdeIVpAoFmoPKT38Ys1TxCJmkxvQ8N0EoxoFkzwv91PDE8om dMR7lkZUceNn81NzcmVIQljbStCMlfTmRUGTNVge1UFMdm1ZuJ3m9 FMbPxNRkiKP2GJRmEqCMZn9TYZCc4ZyaglWthbCRtTRnadJa2BJpO ObLmsptTt5qKukfVX3Lp371YbtSKxEpzDBdTAg2towB0oQUMRvA MrDmvDjvzofzuWjdcIqZM1iC8ULYQ2cjQlatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexitMZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base6427acs0 PQBXyLt6qAhVhplC2bPAABHicbZDNSgMxFIXv1L9aq9a1m8Ei uCozbnQpuHFZwf5AW0omc6cNTJDckcow7yALn0SdKbCDuTX8W1nog 8HFOwr05USaFpSD48io7u3v7B9XD2lG9dnxy2qh3bZobjh2eytT0I2ZR Co0dEiSxnxlkKpLYi2b3i7z3jMaKVDRPMORYhMtEsEZOas9bjSDVrCU vw3hGpqw1rjxPYxTnivUxCWzdhAGY0KZkhwiWVtmFvMGJxCQ4caqbQ jorlmqV6ZzYT1LjiZ6f5UTBl7VxF7qZiNLVs4X5XzbIKbkdFUJn OaHmq0FJLn1KcWfVgY5CTnDhg3wu3q8ykzjJNrZmNKZNgMqdz0VOlq CvWsg3d61YtMLHAKpwDhdwBSHcwB08QBs6wCGF3jzXr1372NVZ8V b93oGIfwCRYJkTlatexit latexit sha1base64EjsSiM oTN3E4MQNRjGszArFVJcACAnicbZDNSgMxFIXv1L9aq1a3boJF cFVm3OhScOyov2BdiZNOGJpkhuSOUYR5B3OmTuBPfwdxbqzsK0H AhnJNybE6VSWPT9b60tb2zu1ferxUD4OayfVtk0yw3iLJTIx3Yha LoXmLRQoeTc1nKpI8k40uZvlnWdurEj0E05THio60iIWjKzHvtNMajV YF9mEYAl1WKo5qP30hwnLFNfIJLW2Fgphjk1KJjkRaWfWZ5SNqEj 3nOoqeI2zOerFuTCOUMSJ8YdjWTun2RU2XtVEXupqI4tuvZzPwv62UY 34S50GmGXLPFoDiTBMyzcZCsMZyqkDyoxwuxI2poYydO2sTIkMnXAs Vj1VuJ6C9VY2oX3VCPxG8OBDGc7gHC4hgGu4hXtoQgsYjOAF3uDdeU vM9FoyVvWe0prMj7gWqn5szlatexit latexit sha1base64EjsSiM oTN3E4MQNRjGszArFVJcACAnicbZDNSgMxFIXv1L9aq1a3boJF cFVm3OhScOyov2BdiZNOGJpkhuSOUYR5B3OmTuBPfwdxbqzsK0H AhnJNybE6VSWPT9b60tb2zu1ferxUD4OayfVtk0yw3iLJTIx3Yha LoXmLRQoeTc1nKpI8k40uZvlnWdurEj0E05THio60iIWjKzHvtNMajV YF9mEYAl1WKo5qP30hwnLFNfIJLW2Fgphjk1KJjkRaWfWZ5SNqEj 3nOoqeI2zOerFuTCOUMSJ8YdjWTun2RU2XtVEXupqI4tuvZzPwv62UY 34S50GmGXLPFoDiTBMyzcZCsMZyqkDyoxwuxI2poYydO2sTIkMnXAs Vj1VuJ6C9VY2oX3VCPxG8OBDGc7gHC4hgGu4hXtoQgsYjOAF3uDdeU vM9FoyVvWe0prMj7gWqn5szlatexit latexit sha1base64405LR lbIMykPdTHV71hSoq8YQACDXicbVBNS8NAEJ34WetX1aOXxSL0 VBIveix48VjRfkAbyma7aZfuJmF3IpSQnyDe9Jd4E6BnId7dtDrb1 wcDjvRlm5gWJFAZd9vZ2Nza3tkt7ZX3Dw6Pjisnp20Tp5rxFotlrLsB NVyKiLdQoOTdRHOqAsk7weR25neuDYijh5xmnBf0VEkQsEoWumh3xSD StWtu3OQdeIVpAoFmoPKT38Ys1TxCJmkxvQ8N0EoxoFkzwv91PDE8om dMR7lkZUceNn81NzcmVIQljbStCMlfTmRUGTNVge1UFMdm1ZuJ3m9 FMbPxNRkiKP2GJRmEqCMZn9TYZCc4ZyaglWthbCRtTRnadJa2BJpO ObLmsptTt5qKukfVX3Lp371YbtSKxEpzDBdTAg2towB0oQUMRvA MrDmvDjvzofzuWjdcIqZM1iC8ULYQ2cjQlatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1base64UK2cQo jz1etnYU5jgUuxmHUte9MACDXicbVDLSgNBEOyNrxhfUY9eBoOQ U9gVQY8BLx4jmgckS5id9CZDZnaXmVkhLPkE8aZf4k28g1iHcnyR5M YkFDUdVNd1eQCK6N6347hY3Nre2d4m5pbg8Kh8fNLScaoYNlksYtUJ qEbBI2wabgR2EoVUBgLbwfh25refUGkeR49mkqAv6TDiIWfUWOmh1D9 csWtuXOQdeLlpAI5Gv3yT28Qs1RiZJigWnc9NzFRpXhTOC01Es1JpSN 6RC7lkZUovazalTcmGVAQljZSsyZK7ncio1HoiA9spqRnpVW8mud1 UxPeBmPktRgxBaLwlQE5PZ32TAFTIjJpZQpri9lbARVZQZm87SlkDR MZrpsianNidvNZV10rqseW7Nu7q1Kt5YkU4g3OogfXUIc7aEATGAz hGV7hzXlx3p0P53PRWnDymVNYgvP1C2JNnJElatexit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base6427acs0PQBXyLt6qAhVh plC2bPAABHicbZDNSgMxFI Xv1L9aq9a1m8EiuCozbnQpuHFZwf 5AW0omc6cNTJDckcow7yALn0Sd KbCDuTX8W1nog8HFOwr05USaFpS D48io7u3v7B9XD2lG9dnxy2qh3bZ objh2eytT0I2ZRCo0dEiSxnxlkKp LYi2b3i7z3jMaKVDRPMORYhMtEs EZOas9bjSDVrCUvw3hGpqw1rjxPY xTnivUxCWzdhAGY0KZkhwiWVtmF vMGJxCQ4caqbQjorlmqV6ZzYT1 LjiZ6f5UTBl7VxF7qZiNLVs4 X5XzbIKbkdFUJnOaHmq0FJLn1Kc WfVgY5CTnDhg3wu3q8ykzjJNrZm NKZNgMqdz0VOlqCvWsg3d61YtM LHAKpwDhdwBSHcwB08QBs6wCGF 3jzXr1372NVZ8Vb93oGIfwCRY JkTlatexit latexit sha1 base64MklCpv6ElzS9637v8Jhx t1KshwgACA3icbZDNSgMxFI XvFtr1erWTbAIrsqMG10KblxWsD QlpJ73RCk8yQZIQyzCuIO30Sd JzCDuzbRd2NYDgY9zEu7NCVPBjf X9b29re2d3b79yUD2sHR2f1E9rHZ NkmGbJSLRvZAaFxh23IrsJdqpD IU2A2n92XefUZteKe7CzFoaQTxS POqC2tQSvmo3rDbpzkU0IltCApV qjs9gnLBMorJMUGP6gZaYU615U xgUR1kBlPKpnSCfYeKSjTDfL5rQS 6dMyZRot1Rlszdvy9yKo2ZydDdlN TGZj0rzfyfmaj2HOVZpZVGwxKM oEsQkpP07GXCOzYuaAMs3droTFVF NmXT0rU0JNp2iLVU8WrqdgvZVN6F w3A78ZPpQgXO4gCsI4Abu4AFa0 AYGMbzAG7x7r96H97lodMtbVnsGK KfgF37Zuklatexit latexit sha1 base64MklCpv6ElzS9637v8Jhx t1KshwgACA3icbZDNSgMxFI XvFtr1erWTbAIrsqMG10KblxWsD QlpJ73RCk8yQZIQyzCuIO30Sd JzCDuzbRd2NYDgY9zEu7NCVPBjf X9b29re2d3b79yUD2sHR2f1E9rHZ NkmGbJSLRvZAaFxh23IrsJdqpD IU2A2n92XefUZteKe7CzFoaQTxS POqC2tQSvmo3rDbpzkU0IltCApV qjs9gnLBMorJMUGP6gZaYU615U xgUR1kBlPKpnSCfYeKSjTDfL5rQS 6dMyZRot1Rlszdvy9yKo2ZydDdlN TGZj0rzfyfmaj2HOVZpZVGwxKM oEsQkpP07GXCOzYuaAMs3droTFVF NmXT0rU0JNp2iLVU8WrqdgvZVN6F w3A78ZPpQgXO4gCsI4Abu4AFa0 AYGMbzAG7x7r96H97lodMtbVnsGK KfgF37Zuklatexit latexit sha1 base64InjBHp21i7WqX37HyhI C1K9auYACDnicbVDLSsNAFJ 3UV62vqks3g0XoqiRudFlw47KCfU AbymR60wydmYSZiVBCfkHc6Ze4E7 fghi3kmbhW09cOFwzr3ce0QcK aN6347la3tnd296n7t4PDoKRet bTcaodGnMYzUIiAbOJHQNMxwGiQ IiAg79YHZXP0nUJrF8tHMEAFmU oWMkpMIY06ERvXG27LXQBvEq8kDV SiM67jCYxTQVIQznReui5ifEzog yjHPLaKNWQEDojUxhaKokA7WeLW3 N8ZUJDmNlSxq8UP9OZERoPReB7R TERHrdK8TvGFqwlsYzJDUi6XB SmHJsYF4jCVNADZ9bQqhi9lZMI6 INTaelS2BIjMwaomcpuTt57KJu ldtzy35T24jXazTKyKLtAlaiIP3 aA2ukcd1EURegZvaI358V5dz6cz 2VrxSlnztEKnK9fMxOcwlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n 4vWDaecOUNLcL5ATRTnQMlat exit latexit sha1 base64NWSHSjiU0wgQ5LE8PASb TFkF1SkACDnicbVDLSsNAFJ 34rPVdelmsAhdlUQEXRbcuKxgH9 CGMplOmqEzkzBzI5SQXxB3iXuxK 24Ie4d9JmYVsPXDicy3hMkgh tw3W9nY3Nre2e3slfdPzg8Oq6dnH ZNnGrKOjQWse4HxDBFesAB8H6iW ZEBoL1guld4femDY8Vo8wS5gvyU TxkFMChTRsR3xUq7tNdw68TryS1F GJ9qj2MxzHNJVMARXEmIHnJuBnRA OnguXVYWpYQuiUTNjAUkUkM342vz XHl1YZ4zDWthTgufp3IiPSmJkMbK ckEJlVrxD8wYphLdxlWSAlN0sS hMBYF4jMdeMgphZQqjm9lZMI6 IJBRvP0pZAkymDfFmTuc3JW01lnX Svmp7b9B6u61GmVgFnaML1EAeu kEtdIaqIMoitAzekVvzovz7nw4n exit4vWDaecOUNLcL5ATRTnQMlat latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base6427acs0 PQBXyLt6qAhVhplC2bPAABHicbZDNSgMxFIXv1L9aq9a1m8Ei uCozbnQpuHFZwf5AW0omc6cNTJDckcow7yALn0SdKbCDuTX8W1nog 8HFOwr05USaFpSD48io7u3v7B9XD2lG9dnxy2qh3bZobjh2eytT0I2ZR Co0dEiSxnxlkKpLYi2b3i7z3jMaKVDRPMORYhMtEsEZOas9bjSDVrCU vw3hGpqw1rjxPYxTnivUxCWzdhAGY0KZkhwiWVtmFvMGJxCQ4caqbQ jorlmqV6ZzYT1LjiZ6f5UTBl7VxF7qZiNLVs4X5XzbIKbkdFUJn OaHmq0FJLn1KcWfVgY5CTnDhg3wu3q8ykzjJNrZmNKZNgMqdz0VOlq CvWsg3d61YtMLHAKpwDhdwBSHcwB08QBs6wCGF3jzXr1372NVZ8V b93oGIfwCRYJkTlatexit latexit sha1base64MklCpv 6ElzS9637v8Jhxt1KshwgACA3icbZDNSgMxFIXvFtr1erWTbAI rsqMG10KblxWsDQlpJ73RCk8yQZIQyzCuIO30SdJzCDuzbRd2NYD gY9zEu7NCVPBjfX9b29re2d3b79yUD2sHR2f1E9rHZNkmGbJSLRvZAa Fxh23IrsJdqpDIU2A2n92XefUZteKe7CzFoaQTxSPOqC2tQSvmo3rD bpzkU0IltCApVqjs9gnLBMorJMUGP6gZaYU615UxgUR1kBlPKpnSC fYeKSjTDfL5rQS6dMyZRot1Rlszdvy9yKo2ZydDdlNTGZj0rzfyfmaj 2HOVZpZVGwxKMoEsQkpP07GXCOzYuaAMs3droTFVFNmXT0rU0JNp2iL VU8WrqdgvZVN6Fw3A78ZPpQgXO4gCsI4Abu4AFa0AYGMbzAG7x7r96 H97lodMtbVnsGKKfgF37Zuklatexit latexit sha1base64MklCpv 6ElzS9637v8Jhxt1KshwgACA3icbZDNSgMxFIXvFtr1erWTbAI rsqMG10KblxWsDQlpJ73RCk8yQZIQyzCuIO30SdJzCDuzbRd2NYD gY9zEu7NCVPBjfX9b29re2d3b79yUD2sHR2f1E9rHZNkmGbJSLRvZAa Fxh23IrsJdqpDIU2A2n92XefUZteKe7CzFoaQTxSPOqC2tQSvmo3rD bpzkU0IltCApVqjs9gnLBMorJMUGP6gZaYU615UxgUR1kBlPKpnSC fYeKSjTDfL5rQS6dMyZRot1Rlszdvy9yKo2ZydDdlNTGZj0rzfyfmaj 2HOVZpZVGwxKMoEsQkpP07GXCOzYuaAMs3droTFVFNmXT0rU0JNp2iL VU8WrqdgvZVN6Fw3A78ZPpQgXO4gCsI4Abu4AFa0AYGMbzAG7x7r96 H97lodMtbVnsGKKfgF37Zuklatexit latexit sha1base64InjBH p21i7WqX37HyhIC1K9auYACDnicbVDLSsNAFJ3UV62vqks3g0Xo qiRudFlw47KCfUAbymR60wydmYSZiVBCfkHc6Ze4E7fghi3kmbhW09 cOFwzr3ce0QcKaN6347la3tnd296n7t4PDoKRetbTcaodGnMYzUI iAbOJHQNMxwGiQIiAg79YHZXP0nUJrF8tHMEAFmUoWMkpMIY06ERvX G27LXQBvEq8kDVSiM67jCYxTQVIQznReui5ifEzogyjHPLaKNWQEDoj UxhaKokA7WeLW3N8ZUJDmNlSxq8UP9OZERoPReB7RTERHrdK8TvGFq wlsYzJDUi6XBSmHJsYF4jCVNADZ9bQqhi9lZMI6INTaelS2BIjMw aomcpuTt57KJuldtzy35T24jXazTKyKLtAlaiIP3aA2ukcd1EUReg ZvaI358V5dz6cz2VrxSlnztEKnK9fMxOcwlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexitfFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base6427acs0 PQBXyLt6qAhVhplC2bPAABHicbZDNSgMxFIXv1L9aq9a1m8Ei uCozbnQpuHFZwf5AW0omc6cNTJDckcow7yALn0SdKbCDuTX8W1nog 8HFOwr05USaFpSD48io7u3v7B9XD2lG9dnxy2qh3bZobjh2eytT0I2ZR Co0dEiSxnxlkKpLYi2b3i7z3jMaKVDRPMORYhMtEsEZOas9bjSDVrCU vw3hGpqw1rjxPYxTnivUxCWzdhAGY0KZkhwiWVtmFvMGJxCQ4caqbQ jorlmqV6ZzYT1LjiZ6f5UTBl7VxF7qZiNLVs4X5XzbIKbkdFUJn OaHmq0FJLn1KcWfVgY5CTnDhg3wu3q8ykzjJNrZmNKZNgMqdz0VOlq CvWsg3d61YtMLHAKpwDhdwBSHcwB08QBs6wCGF3jzXr1372NVZ8V b93oGIfwCRYJkTlatexit latexit sha1base64MklCpv 6ElzS9637v8Jhxt1KshwgACA3icbZDNSgMxFIXvFtr1erWTbAI rsqMG10KblxWsDQlpJ73RCk8yQZIQyzCuIO30SdJzCDuzbRd2NYD gY9zEu7NCVPBjfX9b29re2d3b79yUD2sHR2f1E9rHZNkmGbJSLRvZAa Fxh23IrsJdqpDIU2A2n92XefUZteKe7CzFoaQTxSPOqC2tQSvmo3rD bpzkU0IltCApVqjs9gnLBMorJMUGP6gZaYU615UxgUR1kBlPKpnSC fYeKSjTDfL5rQS6dMyZRot1Rlszdvy9yKo2ZydDdlNTGZj0rzfyfmaj 2HOVZpZVGwxKMoEsQkpP07GXCOzYuaAMs3droTFVFNmXT0rU0JNp2iL VU8WrqdgvZVN6Fw3A78ZPpQgXO4gCsI4Abu4AFa0AYGMbzAG7x7r96 H97lodMtbVnsGKKfgF37Zuklatexit latexit sha1base64MklCpv 6ElzS9637v8Jhxt1KshwgACA3icbZDNSgMxFIXvFtr1erWTbAI rsqMG10KblxWsDQlpJ73RCk8yQZIQyzCuIO30SdJzCDuzbRd2NYD gY9zEu7NCVPBjfX9b29re2d3b79yUD2sHR2f1E9rHZNkmGbJSLRvZAa Fxh23IrsJdqpDIU2A2n92XefUZteKe7CzFoaQTxSPOqC2tQSvmo3rD bpzkU0IltCApVqjs9gnLBMorJMUGP6gZaYU615UxgUR1kBlPKpnSC fYeKSjTDfL5rQS6dMyZRot1Rlszdvy9yKo2ZydDdlNTGZj0rzfyfmaj 2HOVZpZVGwxKMoEsQkpP07GXCOzYuaAMs3droTFVFNmXT0rU0JNp2iL VU8WrqdgvZVN6Fw3A78ZPpQgXO4gCsI4Abu4AFa0AYGMbzAG7x7r96 H97lodMtbVnsGKKfgF37Zuklatexit latexit sha1base64InjBH p21i7WqX37HyhIC1K9auYACDnicbVDLSsNAFJ3UV62vqks3g0Xo qiRudFlw47KCfUAbymR60wydmYSZiVBCfkHc6Ze4E7fghi3kmbhW09 cOFwzr3ce0QcKaN6347la3tnd296n7t4PDoKRetbTcaodGnMYzUI iAbOJHQNMxwGiQIiAg79YHZXP0nUJrF8tHMEAFmUoWMkpMIY06ERvX G27LXQBvEq8kDVSiM67jCYxTQVIQznReui5ifEzogyjHPLaKNWQEDoj UxhaKokA7WeLW3N8ZUJDmNlSxq8UP9OZERoPReB7RTERHrdK8TvGFq wlsYzJDUi6XBSmHJsYF4jCVNADZ9bQqhi9lZMI6INTaelS2BIjMw aomcpuTt57KJuldtzy35T24jXazTKyKLtAlaiIP3aA2ukcd1EUReg ZvaI358V5dz6cz2VrxSlnztEKnK9fMxOcwlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit latexit sha1base64NWSHSj iU0wgQ5LE8PASbTFkF1SkACDnicbVDLSsNAFJ34rPVdelmsAhd lUQEXRbcuKxgH9CGMplOmqEzkzBzI5SQXxB3iXuxK24Ie4d9JmYVsP XDicy3hMkghtw3W9nY3Nre2e3slfdPzg8Oq6dnHZNnGrKOjQWse4H xDBFesAB8H6iWZEBoL1guld4femDY8Vo8wS5gvyUTxkFMChTRsR3xU q7tNdw68TryS1FGJ9qj2MxzHNJVMARXEmIHnJuBnRAOnguXVYWpYQuiU TNjAUkUkM342vzXHl1YZ4zDWthTgufp3IiPSmJkMbKckEJlVrxD8wYp hLdxlWSAlN0sShMBYF4jMdeMgphZQqjm9lZMI6IJBRvP0pZAkymD fFmTuc3JW01lnXSvmp7b9B6u61GmVgFnaML1EAeukEtdIaqIMoitA zekVvzovz7nw4n4vWDaecOUNLcL5ATRTnQMlatexit a b c d Figure 9 Comparison of the Momentum Measurement Gradient Descent MoMGrad algorithm ad and the Quantum Dynamical Descent QDD algorithm eh Both algorithms are initialized to the same Gaussian state in a and e and undergo the same phase kick in b and f The momentum measurement and reinitialization of MoMGrad is shown in c whereas the result of the kinetic pulse of QDD is shown in g Plots d and h show the nal phase kick for MoMGrad and QDD respectively IV FURTHER QUANTUM DESCENT METHODS In this section we discuss a collection of variations upon elements of the algorithms presented above as well as possible augmentations one can use in conjunction with some of these procedures A Batching Parallelization In Subsections III B and III C we outlined basic cases of the Quantum Dynamical Descent QDD and Momen tum Measurement Gradient Descent MoMGrad algo rithms During this previous analysis we assumed full batching of the data at every parameter update That is we assumed the cost function was the averaged loss over every data point in the dataset Here we will make the distinction between an iteration corresponding to an update of the parameters and an epoch which is a sweep through the entire dataset In the above discussion each iteration was a full epoch now we will examine alterna tives As QDD and MoMGrad have numerous connec tions with classical deep learning we can draw inspira tion from these techniques to engineer such alternatives 1 Quantum Stochastic Descent One of the core techniques in classical deep learning is stochastic gradient descent SGD 1 In this approach one uses the loss function of a single data point as the cost function for each parameter update iteration We will rst describe how to apply this to Quantum Dynamical Descent Let us denote a dataset classical or quantum by D For example this could be a collection of inputoutput pairs of classical vectors or quantum states The data points in this set will be indexed by j B For each data point classical or quantum the corresponding eective loss function arising from the Quantum Feedfor ward and Baqprop algorithm will be written as Lj Stochastic Quantum Dynamical Descent then consists of applying the unitary Usqdd Y jB eij 2eijLj 105 That is for each data point we have the exponentiated ef fective loss function from QFB which holds to rst order in j as well as a kinetic pulse At the jth iteration the parameters get shifted by the gradient of the loss func tion for the jth data point to rst order in the kicking rate eijLjeij 22 eij 22eijLj j jjLj O2 j 2 j 10624 We get the gradient update rule similar to classical stochastic gradient descent Note the unitary 105 is for a single epoch sweep over the entire dataset it can be repeated for multiple epochs For the SGD variant of Momentum Measurement Gra dient Descent we similarly update the parameters af ter kicking with the exponential of the loss of each data point Before the jth parameter update the parameter space wavefunction can be reinitialized to a Gaussian state j 1 2j14 eij e 1 4 jT j1j 107 where j is the expectation value of the momentum from the previous measurements j is the expectation value of the parameter vector and j is the covariance matrix for the jth update round the latter being consid ered as a classical hyperparameter Now by applying the QFB circuit using the loss function of the jth data point j 7 eijLj j 108 and then applying the Fourier transform on each parame ter register and measuring the output ie measuring the momentum we obtain the updated average momentum as the expectation value j1 jeijLj eijLjj j jLjj O2 109 We then classically update the parameter expectation value for the next round as j1 j jj1 110 This is the parameter iteration for the the jth data point We then sweep over the data set for a full epoch updating both the momentum and parameter expectation values at each step This can be repeated for multiple sweeps over the dataset epochs as necessary In both stochastic QDD and stochastic MoMGrad we have the hyperparameters j jjB which are the phase kicking and kinetic rates for each update To op timize these hyperparameters there are various classical heuristics from which we can draw inspiration 61 62 Note that in our case the learning rate is a product of both the phase kicking rate and kinetic rates As in the classical case stochastic descent has some perks it tends to regularize the landscape and avoid over tting 63 However this comes with a tradeo of being noisy and hence unstable for high learning rates 2 Sequential MiniBatching In classical machine learning a common practice is to partition the training data into minibatches of data That is we can partition our dataset as D kSMk where S is an index set over the minibatches In turn each minibatch Mk consists of a number of data points indexed by j Bk For the purposes of generating a cost function sequen tial minibatching will consist of consecutive applications of the phase kicks for each data point in a minibatch before either acting the kinetic term or classically shift ing the mean parameter vector in the cases of QDD and MoMGrad respectively By sequentially applying the phase kicks for every data point in the minibatch there is a summation of the contributions to the shifts in the momentum of the parameters Therefore the average momentum shift over the minibatch can be used in the parameter update In the next subsubsection we will ex plore an alternative where the average momentum shift over a minibatch is produced through an accumulation of phase kicks applied in parallel rather than sequentially For the present case we can write the explicit unitary corresponding to a sequentially minibatched version of Quantum Dynamical Descent Usmqdd Y kS eik 2 Y jBk eikLj 111 where we have the loss function Lj for each data point j Bk We have also denoted a modied kick ing rate k kMk which is normalized by the size of the minibatch If we consider the the cost function for minibatch k to be the average loss over the minibatch Jk 1 Mk P jBk Lj 112 then we see that sequentially applying the QFB losses for each data point in the minibatch is the same as applying the minibatch cost exponential Umqdd Y kB eik 2eik P jBk Lj Y kB eik 2eikJk 113 Note that for each application of eikLj one executes the QFB circuit for the jth data point and in the above each data points exponential loss is applied in sequence over the minibatch index before a kinetic pulse is applied Using the above expression we can derive the update rule for the parameters for each minibatch eikJkeik 22 eik 22eikJk k kkJk O2 k 2 k 114 We see that we have the same update rule as in the case of SQDD 106 but now for the averaged gradient over the minibatch For the minibatched Momentum Measurement Gradi ent descent there are two options accumulate momen tum kicks sequentially in a quantum coherent fashion be fore measuring the momenta or measuring the momen tum shift for each data point and classically summing up25 the contributions for the parameter iteration Although the latter approach requires less coherence more runs are necessary to get an accurate estimate of the momen tum expectation value as compared to the former Let us consider the coherent momentum accumulation rst we start with a pointer state k for the kth iteration which we assume has a parameter space representation k 1 2k14 eik e 1 4 kT k1k 115 The coherently sequentially minibatched momentum measurement gradient descent approach then consists of applying all the QFB circuits for the loss function expo nential of each data point in sequence k 7 Y jBk eikLj k eikJk k 116 Note that same notation as in SMQDD above was used If we then measure in momentum space by applying the Fourier transform and measuring all the parameter reg isters we can update the momentum expectation value k1 k k k k kJkk O2 k 117 If the covariance matrix of the parameters is chosen to be diagonal with entries k n 2 where n indexes the pa rameters then the nth component of the gradient can be estimated to a precision standard deviation 12k n r with r runs of preparation and measurement In contrast the classically accumulated minibatched momentum measurement gradient descent proceeeds by preparing a copy of the pointer state for each data point ie k N jBk kj Here we use the tensor product notation but one could also consider measuring and resetting sequentially Assuming the pointer states kj are all identical copies of the form 115 by ap plying sequentially the exponential loss of each data point in the minibatch on the dierent copies k 7 O jBk eikLj k O jBk eikLj kj 118 measuring each copys momenta and classically summing up the results yields the expectation value 1 Mk P jBk k jk k k kJkk O2 k 119 Note that if the covariance matrix for all data points is diagonal with entries k n 2 then with r runs for each data point in the minibatch Mk the expectation value of the nth component of the gradient can be estimated to a precision p Mk2k n r Hence an option is to increase the variance of the pointer states in position or to perform more runs Given a minimal variance of pointer states eg limited number of qubits per param eter in the DV case or limited squeezing levels in the CV case then the coherent accumulation of momenta yields an advantage in terms of runs needed for a cer tain precision In any case one then updates the average momentum parameter vector by the expectation value above k1 kk kJkkO2 k 120 We get the same result as with the coherently accumu lated version but possibly with a dierent precision as noted above In both cases once the momentum was updated we can classically update the parameters expectation for the next round as k1 k kk1 121 Thus concludes an iteration of sequentially minibatched gradient descent with either coherent or classical mo mentum accumulation We can now consider how to par allelize the gradient accumulation over a minibatch 3 Coherently Accumulating Momentum Parallelization An important development in the deployment of large neural networks is the possibility to parallelize the train ing over minibatch elements Classically this would be achieved by feeding forward the information of each data point and computing the gradient contribution on dif ferent replicas of the network each replica running on dierent registers in spatially parallel fashion either on dierent cores or dierent processors Once the gradi ent contributions are computed the replicas must com municate the gradient values to update their parameters synchronously The same parallelization will be possi ble for Baqprop but instead of adding gradients we will accumulate momenta coherently The noncoherent clas sical version of parallel accumulation of momenta adapts trivially from the above sequential version hence we will focus on the coherent accumulation of momenta We call this approach Coherently Accumulating Momentum Par allelization CAMP The parallelism of CAMP relies on leveraging GHZ like entanglement 64 of the weights of the replicas A central quantum parameter server keeps in quantum co herent memory the weights at the beginning of an epoch then by coherently adding the parameter values onto dif ferent replicas acting the QFB circuit for each data point replica and uncomputing the coherent addition the cen tral parameter server will have accumulated all the mo menta contributions from the various replicas This can26 be seen as extending the phase backpropagation through the computation that is the distribution and recollection of the parameter values via adder gates Consider the parameter eigenstates of the various repli cas to be labelled as c where c is the index of the replica with c 0 being the index of the central pa rameter server For minibatch parallelization we have a number of replicas equal to the minibatch size as such for the kth minibatch c Bk we begin with the state of the central parameter server at iteration k with the replicas parameters set to null squeezed state for CV or null position state for qudit ie k0 O cBk 0c X k 0 O cBk 0c 122 Now using adder gates either CV or DV depending on the architecture we transform this state to k0 O cBk 0c 7 X k 0 O cBk c 123 Eectively we are applying a parameter replication uni tary which we will call the Tree Entangler TENT which adds the parameter values to the replicas as in Utent Y cBk ei0 c 124 Rather than applying this addition sequentially as above this addition can be achieved in logarithmic depth in the size of the minibatch by using adders in a sequence shaped like a nary tree This is a depthoptimal way to add standard basis values onto multiple target registers by using adders recursively as to form a perfect or com plete nary tree of adders For a complete nary tree structure we can create a GHZ state on r1 registers in a depth On lognr Practically the depth will be de termined by the interconnectivity of the dierent sets of registers as it is the case classically where the bottleneck of data parallelization is highly dependent on the inter connect between chips 65 Figure 10 provides a circuit diagram illustrating the Tree Entangler Coherently Accumulating Momentum Parallelization CAMP consists of applying the TENT unitary then ap plying the QFB circuit for each corresponding data point in the minibatch on the dierent replicas and nally un computing the TENT This is illustrated in Figure 11 For the minibatch of index k the CAMP unitary would consist of Ucampk U tent O jBk eikLjj Utent 125 We can compute the eect of this conjugation as Ucampk Y jBk eikLj0j 126 Figure 10 Example of Tree Entangler TENT unitary with a binary tree structure Left is a compact graphical represen tation for the TENT and on the right is its expanded form Each CV adder is as in equation 2 For a complete nary tree structure of adders we can create a GHZ state on r 1 registers in a depth On lognr Figure 11 Example of a rst iteration of a Coherent Ac cumulation of Momentum Parallelization protocol CAMP applied to a Quantum Dynamical Descent QDD for classi cal data The replica 0 is the quantum parameter server the initial pointer state of the parameters is prepared in this register via the unitary Up the TENT unitary see Fig 10 is then applied to entangle the parameter server with the replicas j Following this the QFB circuit is applied for a certain data point in each replica in a parallel fashion then the TENT unitary is uncomputed inverse TENT represented with boxes Finally the kinetic term exponential is applied on the parameter server We see that we get a simultaneous exponential loss func tion on both the parameter server and the replica but since the replicas are initialized in nullparameter value pointer states the eective unitary on the parameter27 server is the minibatch loss function Ucampk O cBk 0c Y jBk eikLj0 O cBk 0c eikJk O cBk 0c 127 Hence we can consider CAMP as simply an ancilla assisted way to enact the exponential loss function over the minibatch in a parallelized fashion eikJk 128 One main draw of this method is that there is a speedup to estimate the minibatch momentum update when done coherently as compared to a classically accumulated mo mentum Assuming perfect null eigenstates 0 0 initially in the replicas parameter registers then induc ing the GHZlike entanglement acting the phase kicks on each replica and undoing the GHZ entanglement as described above we can resolve the nth component of momentum within a standard deviation 12k n r with r runs while classically accumulating momenta in dier ent replicas one would get a p Mk2k n r standard deviation in the same number of runs Using our results from sequential minibatching from above it is then straightforward to see how one ap plies CAMP to minibatched Quantum Dynamical De scent and minibatched Momentum Measurement Gradi ent Descent For example to perform paralellized Quan tum Dynamical Descent one applies the CAMP for each minibatch using the initially null parameter replicas in terlaced with the kinetic pulses in the parameter server to update the parameter values Upqdd Y kB eik 2 0U tent O jBk eikLjj Utent Y kB eik 2 0 Ucampk 129 For Momentum Measurement Gradient Descent one can apply CAMP on the kth iteration pointer state Ucampk k0 O cBk 0c 130 and follow the same steps and classical updates of the co herently sequentially minibatched MoMGrad as in 117 and 121 The technique of employing GHZentanglement to get a sensitivity speedup for phase sensing is well known and widely used in the quantum sensing literature 66 In this literature this speedup is usually achieved in the context of discrete qubitqudit pointers 4 Quantum Random Access Memory Minibatching One may consider attempting to train the algorithm using multiple data points of a minibatch in superpo sition A possible means for doing minibatching with a superposition of data is to use Quantum Random Ac cess Memory QRAM to generate a sum of input states along with an address register for every data point in the minibatch The general form of the entangled addressinput state is ac 1 Bk X jBk ja jc 131 where j Bk is an address for a data point in the mini batch and jc is an input state on the computational Hilbert space Hc associated with the corresponding data point In this approach the parametric unitary U acting on H Hc remains unchanged The loss exponential becomes a controlledloss exponential with the address register as the control X jBk jjA ei Lj 132 where Lj is the loss function associated with data point j The full QFB circuit in this case is U X jBk jjA ei Lj U AC 1 Bk X jBk jA ei Lj jC 133 where Lj U Lj U To examine the eect this has on training the parameters we can compute the eective phase operator eiJk again which holds on average to rst order in by tracing out both the address and compute registers a and c Jk 1 Bk X jBk j Lj jc O2 134 Thus we see that in the end we obtain the same ef fective phase as in either sequential minibatching or CAMP Let us briey compare the CAMP and QRAM approaches to minibatching with a rough analysis of the errors in the eective cost function For CAMP the er ror is OBk 2Bk2 O2Bk since we have Bk copies each with error O2Bk2 For QRAM the error is O2 since we have a single copy of QFB apply ing a phase kick with kicking rate The sum in QRAM is obtained from averaging over multiple runs Thus in QRAM we are applying larger kicks and the cost function is obtained stochastically over the data points whereas in CAMP we add logarithmic depth to the cir cuit in order to run a separate instance of QFB for each data point in parallel and the phase kicks are accumu lated coherently Even though in CAMP one has to add this logarithmic depth to the circuit it seems that the error in the eective cost function is suppressed by the size of the minibatch and one also would not have the diculty of building a QRAM 2428 Furthermore for classical data the CAMP procedure for each data point is singleshot because the computa tional register is left in a computational eigenstate at the end of the QFB procedure For quantum data one would need to run the QFB procedure multiple times in order to obtain the average over the computational registers in the eective cost function Thus it would seem that CAMP may be advantageous to QRAM in certain contexts We leave a more careful analysis of the overhead needed in both approaches for future work B Discrete Parametric Optimization In some cases instead of optimizing over a continuous space of parameters one may want to restrict the opti mization to a discrete subspace of parameters In this subsection we consider how to perform both QDD and MoMGrad in order to optimize discretelyparametrized quantum circuits Furthermore we propose a way to embed a given discrete parametric optimization into the continuousvariable versions of the QDD and MoMGrad protocols via a specic choice of regularizing potential Finally we show how to approximate continuum gradi ents as a nitedierence using single qubits for each of the parameters Let us rst formally dene what we consider to be dis crete parametric optimization If P is the index set of parameters then in previous considerations in this pa per the space of parameters was RP Using nqubit precision simulated continuous registers whose parame ter values form a lattice isomorphic to ZP 2n we considered approximating the set of possible parameters on some interval of RP as described in sec II We consider discrete optimization to be the case where a subset of parameters jjS for some index subset S are dis crete The optimization is then over RC ZS d where C P S In this subsection we will mainly focus on having a hybrid set of parameters where some parameters are binary and some are continuous ie hybrid discrete continuous parametric optimization over RC ZS 2 1 Kicking Hybrid DiscreteContinuous Parameters Consider the case where we would like a subset of the parameters to be binary In a nutshell the strategy will be to replace the quadratures of the continuous parame ters j jjS for Pauli operators Zj XjjS in the various instances where the quadratures play a role in the optimization procedures For this discrete continuousbinary hybrid parametric optimization we start by going from a a continuous para metric feedforward unitary U to a hybrid continuous discreteparametric feedforward unitary U Z of the form U Z X RC X bZS 2 bb U b 135 where bj are the eigenstates of Zj of eigenvalues 1bj and b N jS bj For above hybridparametric unitary given a batch of loss operators LkkB the quantum feedforward and Baqprop circuit for the datum of index k B would have an induced eective phase kick of the form eiLk Z U Zei Lk U Zc 136 where Z is a vector of Pauli operators Z ZjjS and the expectation value is taken over the initial state of the compute register as in 53 Here the rate B is the phase kicking rate divided by the size of the batch By concatenating multiple of these eective phase kicks in sequence or in parallel see sec IV A for techniques to do so we can get an eective phase kick for the full batch cost function eiJ Z Y kB eiLk Z 137 We now have an eective phase kick on both the discrete and continuous parameters for the full cost function Before we derive an update rule induced by this eec tive phase kick let us dene some notation and formalism to treat functions of Paulis Starting with a single qubit any function f of the operator Z can be written as f Z f1 0 0 f1 1 1 1 2f1 f1I 1 2f1 f1 Z 138 Then we dene the derivative of f with respect to the PauliZ operator as the coecient of Z in the above ex pression ie Zf Z 1 2f1 f1 139 Note that since this derivative is simply a coecient as an operator it is taken to be proportional to the identity Hence we see that the commutator of a function of Z with X will give f Z X 2i Zf Z Y 140 In general one can have a function of Pauli operators of multiple registers Z ZjM j1 which in general has a decomposition as a polynomial of Paulis f Z X bZM 2 b Zb Zb NM j1 Zbj j 141 where k R bj Z2 for all j and k For such multi operator function we dene the partial derivative as Zkf Z X bZM 2 b 1bk N jk Zbj j 14229 which is the sum over terms which had a nonnull power of Zk and for such terms the power of Zk is removed Finally notice that if we look at the commutator of f Z with Xk we get f Z Xk 2i Zkf Z Yk 143 Now using this formalism we can derive an update rule due to an eective phase kick of the form featured in equation 136 eiL Z XkeiL Z ei adL Z Xk cos2 ZkL Z Xk sin2 ZkL Z Yk Xk 2 ZkL Z Yk O2 144 Now that we have derived an update rule from the phase kick induced by the hybridparametric QFB we can leverage this to perform hybridparametric variants of both Quantum Dynamical Descent QDD and Momen tum Measurement Gradient Descent MoMGrad 2 ContinuousDiscrete Hybrid QDD We begin with the hybridparametric variant of QDD As established in Section III B QDD with continuous quantum parameters is a form of Quantum Alternating Operator Ansatz QAOA Thus naturally a continuous discrete variable hybrid QDD should be analogous to both the continuousvariable QDD and a discrete vari able QAOA 39 Our task is to optimize the cost function J Z via alternating exponentials of operators In order to construct such an optimization scheme we draw inspira tion from discrete QAOA In typical discrete QAOA for qubits the cost Hamiltonian is a function of the standard basis operators Z Zjj ie Hc f Z 145 For such a cost Hamiltonian the traditional choice of mixer Hamiltonian is one of the form Hm X j Xj 146 ie the sum of Pauli X operators To some extent the Pauli Z and X operators are to a qubit what and are to a harmonic oscillator con tinuous quantum register The Hadamard gate is the qubits analogue of a discrete Fourier transform and con jugation of Z by Hadamards gives H Z H X Recalling the formalism from section II the quadrature for a qudit is the Fourier conjugate to the position operator As such in the context of this analogy the mapping 7 Z X of continuous to discrete operators is sensible Although there is some analogy between and X this analogy has its limits as there are a few dierences to keep in mind Looking at the update rule in equation 143 we can contrast this with an analogous formula for functions f of the continuous parameter variables f k ikf 147 We again get a derivative of the function but in contrast to 143 there is no remaining operator with support on the kth register which tensored with the derivative we have the identity on this register instead Furthermore for continuous QDD see sec III B the mixer Hamilto nian is 2 but Pauli operators are involutory X2 I hence we will have to use Xj as the mixer Hamiltonian for each discrete parameter Now that we have built some further intuition we can proceed to building the continuousdiscrete hybrid QDD procedure For this hybrid QDD the analogue to the cost Hamiltonian is the fullbatch the eective phase operator from 137 Hc J Z 148 Since this eective phase function is dependent on both the continuous parameter operators and on the Pauli operators Z we need to add mixer terms of both the continuous and discrete type Thus we chose the mixer Hamiltonian Hm X kS Xk X jC 2 j 149 where is a hyperparameter which can serve as a modi er of the ratio between kinetic rates of the discrete and continuous parameters Now that we have dened can dened our cost and mixer Hamiltonians we can write out the whole hybridparametric QDD unitary for mul tiple epochs as Uhqdd Y j eij 2 eij XeijJ Z 150 where we use the following notation for the discrete mixer exponential eij X eij P kS Xk O kS eij Xk 151 In general one could vary for each epoch ie 7 j in 150 This would allow for an independently variable kinetic rate specic to the discrete registers as it could then be chosen as dierent from the continuous param eters kinetic rate at each epoch In general one could go as far as having specic kinetic and kicking rates for each parameter at the cost of having to optimize more hyperparameters30 Now we see in equation 150 that the QDD unitary becomes a hybrid continuousdiscrete QAOAtype uni tary For a discrete QAOA 39 the hyperparameters must usually be optimized in order to minimize the cost function In this case our hybrid QAOAs cost function is the QFBinduced phase J Z which is a function of both the continuous and discrete parameters By opti mizing the the hyperparameters j j j of the above Hybrid QDD we can expect to minimize the cost func tion We have established above that the hybrid QDD is like a QAOA problem with J Z as the QAOA cost function Hamiltonian From previous literature on dis crete QAOA 39 we know we can nd an approximate minimum of the cost function by optimizing the hyper parameters such as to minimize the cost function These hyperparameters are themselves continuous parameters and thus could be optimized via MetaQDD or Meta MoMGrad approach covered in the Quantum Meta Learning Section IV D or via quantumclassical meth ods After the QDD as discussed in section III B the continuous parameters should concentrate near a local minimum in the continuous landscape and from discrete QAOA 39 we expect a superposition of bitstrings in the discrete parameters which statistically favors bitstrings of low cost function value Thus by jointly measuring both the continuous parameters with the discrete param eters after applying the optimized hybrid QDD unitary one should then obtain a set of discrete and continu ous parameters for which the cost function is relatively low value with high probability Once these measure ments yield a set of classical parameters deemed su ciently optimal one can then perform inference with a classicalparametric variant of the feedforward circuit as described in section III A 1 3 ContinuousDiscrete Hybrid Momentum Measurement Gradient Descent Now that we have derived a hybrid continuousdiscrete variant of QDD using the update rule derived in 144 we can derive a hybrid variant of Momentum Measure ment Gradient Descent MoMGrad To add to the intuition provided by the operator up date rule in 144 we can understand the fullbatch ef fective phase kick as a conditional rotation of each qubit conditioned on other parameters To see this let us rst dene the notation zb 1b as the vector of eigenval ues for the bitstring b We then can rewrite costphase gate induced by the QFB circuit as a controlledrotation of the qubit k about its Z axis for any k S eiJ Z O jSk bjbj ei ZkkJ zb 152 We see that conditioned on all parameters other than that of qubit k the gate is eectively a rotation of the form eik Zk where the the angle k of rotation is k kJ zb ie proportional to the negative gradient This controlledrotation interpretation further provides intuition for the update rule derived in 144 Now using the update rule 144 we can see that given an initial state of qubit k in the zy plane of the Bloch sphere we will be able to read o the gradient given small rotations That is given an initial state of the form S0 O jS cosj 0j i sinj 1j 153 which all have Xk 0 and where the j are hyper parameters which we consider as classical parameters for the time being By applying the QFB phase kick each qubit is eectively rotated about the z axis hence the state travels laterally at a certain lattitude in the Bloch sphere as depicted in Figure 12 This rotation then converts initially null x component of the state to one depending on the initial latitude on the Bloch sphere and on the gradient of the cost function as depicted in Figure 12 By measuring the expectation value Xk after the QFB phase kicks from 137 for pointer states of the form of 153 we can get the gradient as follows S0eiJ Z XkeiJ Z S0 2 sin2k ZkJ Z O2 154 We can then use this measurement of the gradient to up date the parameters k 7 k ZkJ Z for all k where is a hyperparameter while simultaneously performing the MoMGrad for the continuous parame ters using update rules discussed in III C Thus one can perform a continuousdiscrete hybrid MoMGrad Once a sucient number of iterations has been performed since the parameters are classical parameters which are known one can round each of the binary parameters to the most probable value b k sin2k One can then perform further MoMGrad on the continuous pa rameters for the hybrid cost function with the classi cal binary parameters b in the feedforward ie perform continuous MoMGrad with the cost function J zb Finally one obtains suciently optimal continuous pa rameters which can then allow for inference with the hybrid discretecontinuous classically parametrized oper ation U zb 4 ContinuumEmbedded Discrete Optimization In this section we discuss the possibility of perform ing optimization over a discrete subset of hypotheses by embedding the problem into the continuum and adding a regularizing potential which forces the weights to con verge onto the closest discrete value As was discussed above one can use discrete variable quantum registers such as a qubits in order to perform31 Figure 12 Bloch sphere diagram of singlequbit gradient esti mation 1 eiJ Z 0 the longitudinal angle of rota tion relative to the zy plane is given by 2 ZkJ Z hence allowing for estimation of the gradient of the cost function via multiple runs and measurements of the expectation value of the Pauli X such a discrete optimization These discrete optimiza tion approaches came with caveats namely the hybrid MoMGrad necessitates multiple QFB runs and measure ments to get an accurate estimate of the gradient while the hybrid QDD requires hyperparameter optimization to yields approximately optimal bitstrings By embed ding the discrete parametric optimization into a continu ous parametric optimization we can directly leverage the continuous parameter variants of MoMGrad and QDD from section III rather than have to modify the lat ter to accommodate discrete parameters The approach which will be outlined in this subsection will be consist of adding a regularization potential to the continuous pa rameters which eectively turns these into a simulated qubit or qudit If we have some subset of parameters jjS where S is the subset of indices which we would like to be either 0 or j then for either quantum dynamical descent or momentum measurement gradient descent we can add a regularizing potential to the cost J 7 J VS where the regularizing potential is of the kind VS X jS 2 j 2 j j 2 2 j 2 2 2 155 which is a sum of doublewell potentials each with wells centered at 0 and j To simulate QDD with this poten tial one would need to apply the unitary Uqdd Y j eij 2eijJ eijVS 156 with the quartic potential exponential being applicable through the use of multiple qubic phase gates with CV techniques 67 68 or in the DV case using Olog d4 4local Pauli terms where d is the range of the qudit parameter register see section II similarly to how ex ponentials of 2 are implemented see section IV C 1 for details Although an extensive analysis of the dynamics of quartic oscillators is beyond the scope of this paper there has been extensive physics literature on the subject 69 hence we will stick to a qualitative understanding of its dynamics for our discussion Notice that Taylor expanding the above potentials at points j 0 we see that each well is locally like a harmonic oscilla tor of harmonic frequency jj The ground states of each parameter are then approximately equal to a sym metric superposition of the ground states of each of the wells Thus the added potential induced by a cost func tion J can bias the eective potential on the param eters and break this energetic degeneracy a specic well will then be favoured as having lower energy Thus nat urally a metaheuristic like Quantum Dynamical Descent or MoMGrad would nudge the parameter wavefunction into the well which minimizes VS J Once the param eter wavefunction is concentrated into one of the wells assuming it approximates the ground state of the well its expectation value can be estimated with few mea surements the determinant of the covariance should be around Q jSjj 1 2 due to the locally harmonic dy namics of the well One can then round to the closest value of each parameter on the lattice A nearterm implementation of this approach might favour using an analog ux qubit for simplicity but in this case the poor readout capabilities will limit precision and thus speed of execution of the optimization Simu lating a similar system at a highlevel of precision at the logical level will allow for smoother state transitions the ability to resolve a continuum of momenta and parameter values and generally will allow for nergrained dynam ics by having the parameter query a continuum of possi bilities This is thus the longterm favourable approach 5 Estimating Continuum Gradients with Single Qubits Before we move on from this subsection now that we have treated how we can embed a discrete optimization into continuous parameters let us very briey mention how we can use a lowdimensional discrete system such as a qubit to perform apprximate MoMGrad III C This is a straightforward application of the fairly general for malism developed in Section II A reason this is worth mentioning is because this singlequbit readout might be relevant in the shortterm for Noisy Intermediate Scale Quantum devices 49 where the number of quantum de grees of freedom and level of control is limited Large qudits for the parameters should be more relevant in the longterm post Quantum Error Correction and Fault Tolerance era of Quantum Computation We can use a single qubit for each parameter in MoM Grad at the cost of having to perform more runs in order to get an accurate gradient estimate That is we can use32 a set of parameters for parametric controlledunitaries whose parameters are a mixture of classical oset and a qubits standard basis operator 7 0 I Z where 0 is relatively small ie 0 This is akin to equation 11 from the background for a qubit instead of a qudit Note a small enough is necessary for the dis crete gradient to estimate the continuous gradient That is if we have some unitary parametrized with 0 I Z then by denition of the discrete gradient 138 we have Z U0 I Z 1 2 U0 U0 157 In the case of 0 the expression 1 Z U0 I Z will approach a notion of a continuous derivative As the goal of the quantum parameters is to sense the phase kickback induced by Baqprop one can use a de tector which has a very lowresolution readout Using a single qubit as the qudit from that equation 11 then we get a very rough estimate of the kickback on the parame ters this is equivalent to a singlequbit phase estimation from the nature of phase space we only get a singlebit readout of the gradient This then takes multiple runs so that the continuous gradient can be estimated C Regularization Variants Regularization methods are useful tools for training deep neural networks in the classical literature 17 18 The role of regularization is to ensure a smooth train ing process and avoid overtting to a particular dataset Such techniques are thus indispensable when training very large networks whose training dynamics can be somewhat unwieldy and ensures that the network is able to generalize beyond the given dataset In this section we will mainly focus on techniques which restrain the dynamics of the weights in a certain way to either avoid weight blowup or avoid overreliance on certain connections in the network hyperparameter op timization which is another important tool to avoid un derovertting will be treated in Subsection IV D 1 ParameterWeight Decay A technique from classical machine learning which allows one to dynamically bound the norm of the weightsparameters is weight decay or more generally parameter decay The trick to weight decay is to add a regularization term to the cost function the canonical choice being a simple quadratic penalty For our quan tum weights we can also add a quadratic potential J 7 J 2 158 where we use the notation T 2 To see how this inuences the execution of Quantum Dynamical descent we can write Uqdd Y j eij 2eijJ eij2 159 Note that the eijJ and the eij2 terms can be executed simultaneously partly due to the fact that these operations on the parameters commute but also because the QFB eective phase kick involves many steps not involving every parameter If some parameter were used in every gate of the QFB algorithm then the regularizing potential could not be applied simultaneously However since the QFB algorithm involves an exponentiated loss operator which acts as the identity on the parameters then the regularizing potential on the parameters could be applied at this stage That is we could enact the above unitary as eijLeij2 U eij2 eij L U 160 Note that in the above U is the feedforward oper ation and the Hilbert space factorization is H HC where H and Hc are the parameter and computational Hilbert spaces respectively Alternatively one could ap ply the regularizing potential phase shift for a given pa rameter at any other point in the QFB circuit where it is not being used to implement a controlled operation As we have seen previously both Quantum Dynami cal Descent and Momentum Measurement Gradient De scent have the parameters experiencing a force which is induced by the cost function acting as a potential In the above case adding a quadratic regularizing poten tial to each parameter register is eectively like adding a harmonic oscillator potential For illustration consider performing quantum dynamical descent for a null cost function ie J 0 then QDD becomes Uqdd J 0 Y j eij 2eij2 161 For a certain set of hyperparameters can be viewed as a simulation of Trotterized evolution under a free Hamilto nian of the form H P n2 n n 2 n which represents a set of free uncoupled Harmonic oscillators The cost function J is responsible for the coupling between the parameters during QDD This allows the parameters to entangle and dynamically inuence one another 2 Metanetworked Interacting Swarm Optimization An option for simultaneous parallelization and regular ization is to have multiple replicas similar to the paral lelized minibatch method from IV A 3 above which have their parameters coupled to each other with an attractive potential Such a potential can be used to correlate the dynamics of the replicas equilibrium point while still al lowing for some degree of independent dynamics We call33 this approach Metanetworked Interacting Swarm Opti mization MISO To precisely describe how to couple replicas rst con sider a metanetwork of replicas which is a graph G V E where each vertex is a replica with parameters c for c V The edges E of the graph G represent cou plings between replicas Each edges weight will represent the coupling strength The way this coupling will be in troduced into the parameter dynamics is via an added potential Let ccV be the operatorvalued metavector of all parameter vectors The global added potential is the sum of the coupling potentials V X jkE Vjkj k 162 For example we could choose each of these coupling po tentials of the form Vjkj k jk j kT j k jk34 which only happens during the feedforward and uncom putation portions of QFB This can be done in any or der since the phase shifts commute As long as the ki netic term is not executed there is freedom to choose exactly how to compile the operation A simple way is sequentially adding the potential pulse before or after the replicaparallelized QFB e ik JkV O jV eikJ j k jeikj 2 j Y mlE ei2klm T l m 168 where we denoted the coupling strength averaged over all edges incident to a metanetwork vertex as j P kV jk Again since all the above exponentials are commuting there is opportunity to combine the execu tion of all these terms in the potential in a more ecient manner than serially Note that to execute a MoMGrad optimization with MISO one simply prepare a pointer state of choice in all parameters of all replicas as in 92 Then one applies the above MISO phase unitary from 168 measures the momentum of all parameters and updates them according to the regular MoMGrad update rule from 117 An option for the swarm approach is that one can have multiple networks with the same architecture hence the name replica but with dierent hyperparameters ie dierent initializations andor kicking and kinetic rates at dierent iterations This would mean a modication of the above formulas to having replicaspecic rates ie k k 7 j k j k as well as replicaspecic initial izations mean and variance for the weights This can allow a sort of eective majority voting of where to go in the parameter landscape which may possibly kick repli cas with poor initializations out of a local well but also might perturb a replica performing well in terms of cost optimization to get kicked o of its trajectory to a low cost function value As this is a strategy which will in crease the training set error to possibly improve the test set error we consider it as a regularization technique As Quantum Dynamical Descent is eectively a QAOA approach to nding low energy states of a Hamiltonian we can see that MISO is eectively like trying to nd the ground state of a swarm of interacting particles As suming uniform descent hyperparameters and consider ing full batch cost function J P k Jk we can write down this eective Hamiltonian to be of the form H 1 2 2 2 2 J 2 2 V 169 which resembles a lattice of oscillators with an added nonlinear potential proportional to J Theoretically one could expand J about its minimum value to second order and obtain a quadratic potential The approximate ground state would then be given by a Gaussian ground state of the form 8 The important takeaway is that the joint system of parameters is like a coupled network of oscillators with the intrareplica coupling induced by the cost function and the interreplica couplings due to the metanetworks topology There are many way to modify the approach described above in terms of how to manage data how to modify hyperparameters etc In the next subsection IV D we discuss how to leverage the quantum phase backpropagation of errors and quantum dynamical descent to optimize all these possible hyperparameters via quantum dynamical descent 3 Dropout The method of dropout in classical machine learning encompasses a set of techniques which add noise to the training process in order to regularize the learning The addition of noise to the neural information processing ef fectively forces the network to learn to process informa tion in a redundant robust manner In a sense adding errors forces the neurons to not overrely on a specic neural pathway and thus to split signals into multiple pathways thereby spreading the computation over neu ral elements in order to add noise resistance Traditional dropout consists of adding classical erasure noise to the neural information processing this consists of eectively blocking the path of the information owing forward by stochastically dropping out certain neural elements Modern techniques for dropout also include Gaussian multiplicative noise or Gaussian additive noise 70 for neural networks In this section we focus on techniques to use quantum registers as stochastic classical variables which control whether certain subsets of parametric op erations are applied Note that we will reuse much of the machinery developed in this subsection in our subsection on network architecture optimization via Quantum Meta Learning section IV D 3 where instead of simply using the quantum registers as a source of stochastic noise we can optimize over superpositions of network architectures via a quantum metalearning optimization loop As our parameters naturally have Gaussian noise in both the gradient and parameter value due to our opti mization approach outlined in Section III using Gaussian pointer state the Gaussian multiplicative noise dropout comes for free for our schemes In a sense the Quantum uncertainty of the wavefunction serves as natural regu larizing noise For Gaussian additive noise dropout re fer to Section V where we describe quantum parametric circuits for neural networks In this section the com putational registers are initialized in nullposition qu dit or qumode eigenstates 0 It would be straightfor ward to use computational registers which have some added Gaussian noise to their position value ie are in a simulated squeezed state rather than a perfect posi tion eigenstate initially Because these types of dropout are straightforward to implement with our schemes we focus on operation dropout stochastically removing cer tain subsets of parametric operations35 The goal of operation dropout is to probabilistically create a blockage of information ow in the feedforward computational graph Furthermore another important aspect of dropout is the ability to backpropagate errors with knowledge of this erasure error As our backprop agation approach relies on the ability to backpropagate error signals through the quantum computational graph via uncomputation after the feedforward operation and phase kick we will need to keep in memory the register which controls the erasure We use a quantum states computational basis statistics as the source of classical stochasticity in this section for notational convenience but note that could equivalently replace these qubits with classical random Bernoulli variables of equivalent statis tics Now let us develop some formalism to characterize how to leverage ancillary quantum registers in order to stochastically control which architecture is used in the quantum feedforward and Baqprop Whether it is a quantum parametric circuit as those discussed in Sec tion VI or a neural network embedded into a set of quan tum parametric circuits as discussed in Section V we can assume the parametric circuit ansatz can be written as a layered circuit of unitaries ie U L Y 1 U U O jI Ujj 170 where U is the multiparameter unitary corre sponding to the th layer which can itself be composed of multiple parametric unitaries Ujjj and where I L 1I is the partition of parameter indices for the parameters of each layer Now if we would like to parametrize whether a number N of certain subsets of parametric unitaries are applied or not we need to rst index which unitaries are con trolled by the same variable For this index consider a partition of the indices I N j0Aj where Aj I j For notational convenience let us reserve the subset A0 as the set of unitaries over which we would not like to be stochastically controlled ie we want to implement these with absolute certainty the reason for this nota tion will be apparent below To quantumly control the application of these subsets of unitaries we will need a set of N ancillary qubits which index the architecture say we label these as Aj where the 1Aj indicates we are applying the unitaries in subset Aj For notational convenience consider the following operatorvalued func tion which takes indices of operations from I and maps them to operators on the architecture ancillas Hilbert space HA NN j1 HAj C I BHA Cj N O k1 11 1Ak j Ak 171 where we denote 1Akj as the indicator function for the set Ak I and 110 I This operator can serve a the control operator for a given index essentially given an index of an operation it is a projector onto 1Ak for the ancilla whose index corresponds to that of the partition in which j belongs Also note that the above operator is a function of Pauli Zs of the architecture ancillas hence it is only dependent on the vector of Paulis ZA ZAkk We can consider the architecture index to be a stochasticallydetermined hyperparameter We can then modify our parametric unitary to become a hyper parametric unitary which acts on both the Hilbert space of architecture indices used as controls and the joint computational and parameters Hilbert spaces U ZA L Y 1 Y jI CAj Ujj 172 note this is essentially the same unitary as previously 170 except now each unitary in I A0 is a controlled unitary and the control qubit for each index j is that which corresponds to the partition of inidices Ak such that j Ak Although the above operation may seem complex the circuit to execute the above may be compiled eciently simply by adding a control to each operation For exam ple each parametric unitary see VI A is of the form Ujj X j jj Ujj 173 now assuming each unitary is generated by a certain Hamiltonian ie Ujj eij hj then the above becomes Ujj eijhj 174 which we see is an exponential with a generator j hj In order to convert a certain parametric unitary of index k Aj to have an added qubit control register one simply has to exponentiate the modied generator 11Aj k hk ie X bjZ2 bjbjAj U bj k e i11Aj khk 175 which can be synthesized into a product of vk 2local exponentials of Paulis where vk is the locality of hk Now that we have covered how to modify the parame teric circuit ansatz to include quantum controls we can now describe how to modify the Quantum Feedforward and Baqprop QFB to include dropout Suppose we would like to perform the QFB for a certain iteration where we have the loss operator Lj the usual QFB op eration would consist of applying U ei Lj U 17636 onto a computational register state jC the eective phase on the parameters see sec III A 1 would then be Lj j U Lj U jC 177 Now to modify QFB to include dropout we simply mod ify the regular feedforward parameteric unitary to be the controlled unitary from 172 U ZAei Lj U ZA 178 and we act this hyperparametric unitary on the same computational registers state jC and an initial state 0A of our architecture qubits 0A N O k1 sink 0Ak cosk 1Ak 179 where the kN k1 are hyperparameters which will control the probability of dropout the probability of ap plying a set of operations of index Ak will be given by cos2k Tracing out the computational and architecture registers the resulting eective phase on the parameters will be Lj j0 U ZALj U ZA jC0A 180 Thus we get the average cost function phase kick aver aged over the possible architectures To see this more explicitly we can expand the above expression to do so it will be convenient to dene some more notation Let us begin with a N Y k1 sink1akcoskak 181 hence the state from 179 could be written as 0 P aZN 2 a aA Additionally let Ua a U ZA aA be the unitary corresponding to the parametric circuit of the following architecture Ua L Y 1 Y jIZa U aj j j 182 where Za Ak 0 k N ak 1 This corresponds to acting all unitaries of index j for which j Ak for some k such that ak is nonzero Now we can use the above to expand equation 180 Lj X aZN 2 a j U aLj Ua jC 183 and we see that we get on average the expectation over architectures of the eective phase kick We can then use tools from Section III to leverage this eective phase signal for optimization of the parameters via MoMGrad or QDD Note that just like the regular eective phase kicking for quantum data the registers other than the parame ters must be reinitialized refreshed after each QFB run in order to get the averaged behaviour If we were to keep the same quantum ancillas indexing the architecture for multiple runs of QDD then the parameters would en tangle with the superposition of architectures such as to optimize the cost function for each architecture in each individual branch of the superposition rather than opti mizing for the mixture of architectures We harness this very property of training dierent network architectures in superposition for metalearning and architecture opti mization in section IV D 3 As dropout is integral to training classical neural net works the above technique is useful to have for training classical neural networks on a quantum computer This operation dropout may also be useful for robust para metric circuit learning in certain settings Since dropout will emulate faulty execution of gates this would force the parametric circuit to not rely too much on a single gate for a large change to the state each parametric op eration would stay not too far from the identity each continuously parametrized gate would then have a small angle hence keeping 2 small This becomes eec tively similar to the parameter decay described in sec tion IV C 1 In general one would expect dropout will have other eects on the parameters that simple weight decay alone cannot emulate For general quantum parametric circuits one could consider adding additional parametric unitaries which stochastically drop in potentially to emulate various forms of noise For example we could consider adding controlledX and controlledZ as additional operations in a given parametric circuit ansatz Using two qubits for controls one could then apply stochastically apply X andor Z at each site using techniques from above One could thus emulate a depolarizing channel for example Optionally one could stochastically swap out or swap in computational registers again controlled by architec ture binary hyperparameters This would simulate a form of erasure noise Generally one could use this tech nique to add a great variety of types of noise There are many ways to add noise to a system but dropout is used to regularize the training networks It is not yet clear whether quantum parametric circuits need dropout for better training nor what kind of noise map would be best at this stage in the development of the eld D Quantum MetaLearning 1 Overview In practical machine learning scenarios it is often better to rapidly nd a local minimum rather than a global optimum which has a cost of longer runtime This is where the lowdepth limit becomes interesting Rather than having many pulses in order to minimize37 the SuzukiTrotter error approximating the adiabatic path it will often be better to have a higher phase kicking and kinetic rate and to variationally optimize these hyperparameters This variational optimization is done by training the model with a certain set of hyper parameters and by checking the value of the cost func tion with respect to a subset of data called the test set Oftentimes this is done via trial and error and careful handtuning but there exists ways to automate this pro cess Automation of this hyperparameter optimization is called metalearning 71 Instead of using a classical optimizer which would in volve nitedierence optimization we can use the Quan tum Dynamical Descent method at the hyperparameter level Hyperparameter optimization methods commonly used in classical deep learning namely grid search ran dom search manual search or even Bayesian optimiza tion come with multiple training cycles for optimization often scaling exponentially in overhead with the number of hyperparameters 72 This problem of metatraining training the hyperparameters is what these techniques address Metalearning has been used to boost the learn ing speed decrease training set error in less iterations 73 74 has allowed for better test set error and general ization error and has been used to learn how to rapidly adapt a network trained for a given task to perform a new one an approach known as transfer learning 75 A recent approach to metalearning has been to use gradient descent on the hyperparameters often with an additional neural network relating the choices of hyper parameters between dierent iterations 74 The opti mization of this hyperparameter network is done via a backpropagation of errors up the computational graph which traces back the inuence of the hyperparameters on the output loss function The following techniques we will describe below are analogous in a sense to this hyperparameter gradient descent Each hyperparameter inuences either the ini tialization the descent rates or even the architecture of the network In the rest of this section we will explore how to move from what we hitherto have considered to be classical xed hyperparameters to quantum con tinuous or discrete parameters By considering how to perform the feedforward and backpropagation with quan tum hyperparameters we will then be able to perform metaBaqprop once again using the quantum backpropa gation of phases principle We will then be able to apply either quantum dynamical descent or momentum mea surement gradient descent on the hyperparameters and do so in an ecient manner as Baqprop does not re quire knowledge of analytic derivatives of each part of the computation Finally note that the Quantum MetaLearning ap proach relies heavily on the possibility of entanglement between the quantum hyperparameters and the pa rameterscompute registers Given a superposition of hyperparameters one can consider each branch of the wavefunction of these hyperparameters As the hyper parameters inuence the training of the network via Quantum Dynamical Descent each value of the joint set of hyperparameters will lead to a dierent trained network Since the whole training process is kept quan tum coherent the result is an entangled superposition of hyperparameters and their corresponding fully trained networks At this point applying a cost function ex ponential of choice for the network tags the dierent branches of the wavefunction with relative phases and unitarily uncomputing the training allows for a backprop agation of errors all the way up to the hyperparameters Thereby allowing for their optimization via a MetaQDD or MetaMoMGrad approach 2 Quantum hyperparameter Descent In previous discussions of Quantum Dynamical De scent and Momentum Measurement Gradient Descent given a xed network architecture ansatz there were sets of classical hyperparameters for the preparation of the parameters pointer states denoted 0 0 0 and some for the choice of kicking and kinetic rates for each iteration which were denoted We can then consider a parameter pointer state prepara tion unitary as a classically parametrized unitary Up and similarly the entire Quantum Dynamical Descent unitary as featured in equation 66 can be seen as a unitary parametrized by classical hyperparameters which acts both on computational registers and the pa rameter registers Uqdd The key to our metalearning problem will be to view the combination of the prepara tion and quantum dynamical descent unitaries as hyper parametric circuits to be optimized The task of metalearning usually involves optimizing the initialization and execution of the training process in order to minimize some cost function which assesses either generalization or optimization performance This loss function can be the same as the training losscost function using the training data or it can be some dier ent cost function than that of the training either through the use of the same loss applied to dierent data or some dierent loss function altogether In cases where the learning comes from data either classical or quan tum the subset of data reserved for the hyperparameter training is called the test set while the subset of data reserved for the training of the parameters is called the training set or development set dev set In any case there is a cost function which we want to optimize whose eective phase we will call Jm subject to variations in the hyperparameter vectors Using the same approach as our parameter optimiza tion for regular learning we can quantize the hyper parameters 7 and using either Quantum Dynamical Descent or Momentum Measurement Gradi ent Descent for quantumenhanced optimization of these hyperparameters We will refer to these approaches as MetaQDD and MetaMoMGrad respectively We re38 gard the metafeedforward hyperparametric unitary to be Umeta Uqdd Up 184 ie the parameter state preparation unitary followed by the Quantum Dynamical Descent unitary Before we proceed with how to leverage such a uni tary let us examine how exactly this upgraded quantum hyperparametric unitary can be synthesized into elemen tary gates Note the Quantum Dynamical descent uni tary is now of the form Uqdd Y j eij 2eijJ 185 where the exponentials are now quantumcontrolled The synthesis of the kinetic exponential is straightforward taking Olog3 d 3local exponentials of qubit Paulis to enact where d is the qudit dimension of our parameter and hyperparameter registers For the hyperparametric eective phase one can apply the regular feedforward unitary but the exponential of the loss function now be ing quantumparametric ie apply eijLj U eijLj U 186 and the expectation of the above for an input com putational state will give the quantumparametric ef fective phase How to synthesize this exponential of the loss function will vary In general for a compila tion of eijLj down to Cliord gates and Zrotations gates of the form ei Z for some constants we can then modify the classically parametric rotations to be quantumhyperparametric ei Z 7 ei Z which them selves can each be broken down into Olog d exponen tials For more details on parametric circuit synthe sis see section VI A 1 If the loss function is based on parametric exponentialswap as we will treat in VI we provide compilation of these into Fredkin and Zrotation hence can be quantumhyperparametrized straightforwardly Finally for the preparation uni tary upgrading the hyperparameters to quantum is straightforward since for Gaussian state preparation we can have quantumparametrized simulated continuous variable displacement and squeezing operators to quan tum parametrize the rst and second moments of the Gaussian wavefunctions Now that we have covered how to synthesize the hyper parametric unitary we can now proceed to leveraging this unitary to perform the Quantum Feedforward and Phase Kick Backpropagation procedure at the metalevel The cost function we are trying to optimize can be the loss over some minibatch which corresponds to the test data Let U be the parametric unitary acting on the compute and parameter registers the exponential of the loss function for the metalearning is the exponential loss eiJm Y jBt eiL 187 where Bt is the test set batch index and is the phase kicking rate Bt is the same rate divided by the test batch size Recall that to enact each of the loss function eective phase shifts exponentials this entails applying the QFB procedure for the parameter circuit U ie eiLj U eiLj U 188 and the expectation value of the above when the compu tational register is traced out is the eective phase as in equation 53 hence to enact 187 multiple applications of 188 must be applied using multiple ancillas that are swapped in and out of the compute register in the gen eral case of quantum data training In the case of train ing classical neural networks on a quantum computer as described in section V since the compute registers are in an eigenstate of the QFB circuit we can simply con catenate the phase kicks without the need for swapout simply need to ip the input registers to the right input after each round which is done unitarily For quantum data training if the phase kicking rates are kept small during training even if the compute register ancillas are tossed away as was shown in section III the dynamics of the weights are eectively unitary to rst order in Now we have dened the hyperparametric unitary and the exponential loss function to be applied we can consider applying the MetaQFB Quantum Feedforward and Phase Kick Backpropagation procedure for an it eration of such a phase kick one must apply U meta eiJm Umeta 189 we can consider the eective phase function induced by the kickback from this metaQFB Let the compute and parameters initial state be labelled as 0CP then the eective phase of the metaQFB on the hyperparameters can be labelled as eiK 0 U meta eiJm Umeta 0CP 190 which is true to rst order in Now we have reduced the problem of hyperparameter optimization to that of optimizing an eective exponential phase as was the case before for the base case of QDD and MoMGrad It is then straightforward to extend previous techniques to hyperparameter optimization First for Quantum Dynamical descent suppose we have a set of prepara tion hyperhyperparameters ie classical parameters which control how the initial quantum pointer states of the quantum hyperparameter are initialized in a sense the hyperparameter analogue of Let Uhp be the hyperparameter state preparation unitary Let act as the classical hyperhyperparameters representing the metaQDD or metaMoMGrad kicking and kinetic rates ie the hyperhyperparameters are39 analogues of the hyperparameters for the metaoptimization The MetaQDD algorithm pictured in gure 14 can be summarized as applying the hyper parameter preparation unitary followed by the sequence Umqdd Y jBt F heij22 FheijK 191 where the Fh is the componentwise Quantum Fourier transform for all hyperparameter registers For metaMoMGrad similarly we can begin by prepar ing the quantum pointer states of the hyperparameters using a parametric unitary which itself is dependent on preparation hyperhyperparameters ie Uhp fol lowing this we can apply the metaQFB circuit from equation 190 in order to apply the eective phase kick eiK To complete MetaMoMGrad we can then apply the componentwise Fourier transform Fh on the hyperparameter registers and then measure these regis ters in their computational bases From this phase kick the shift in expectation value will be proportional to the negative gradient of the eective phase we can see this by looking at the Heisenberg picture AdeiK F F F F K O2 AdeiK F F F F K O2 192 the computational basis hyperparameter observables af ter an Fourier transform is shifted by the negative gra dient multiplied by the hyperparameter phase kicking rate Similar to MoMGrad for the regular parameters see 101 102 one can then update the prepara tion hyperparameters for the next iteration ie update the initial expectation value of position and momentum which are hyperhyperparameters in the vector akin to the 0 and 0 but for the hyperparameters the rst and second moments of the pointer states The rate at which the mean hyperparameter value is updated can be multiplied by some constant akin to 102 but with replacing This hyperhyperparameter can be considered the eective kinetic rate for the Meta MoMGrad We represent an iteration of MetaMoMGrad in Figure 14 Finally note that the hyperhyperparameters remain to be optimized Theoretically just as we have shown above that one can optimize the hyperparameters via MoMGradQDD if the parameters are being opti mized by QDD we could consider performing a quan tum parameter descent on the hyperhyperparameters To do so one could consider quantizing the hyperhyper parameters 7 and applying a meta metaoptimization on these using MoMGrad or Quantum Dynamical Descent with MetaQDD taking the role of QDD In a sense QDD is selfconcatenable to as many Figure 14 Examples of the rst iteration of both optimization strategies for quantumenhanced hyperparameter optimiza tion via Quantum MetaLearning QMetaL Represented in a is the MetaQDD protocol while in b is MetaMoMGrad protocol Note that in the above H2P HP and P are de note the hyperhyperparameters the hyperparameters and the regular parameters respectively C is the compute reg ister while DD and TD denote the development training data and the test data respectively The process begins with the preparation of the hyperparameter pointer state using a unitary Uhp the metafeedforward is then applied eq 184 the phase kick according to the test set error is then applied see eq 187 and the metafeedforward is then uncompute Finally in the case of QDD a kinetic pulse is applied on the hyperparameters whereas for MoMGrad the gradient of the hyperparameters is measured and the hyper hyperparameters are updated for the next metaiteration metalevels of optimization as is desired Practically each additional level adds a nested QFB loop of opti mization which grows the overhead of execution expo nentially Additionally the number of hyperparameters increases with the number of metaoptimizations since the parameter preparation hyperparametric unitary has multiple hyperparameters per parameter For meta levels of concatenation to be useful in the sense of achiev ing a lower expectation value of the cost function of choice one would need to consider a choice of hyper parameters which reduces the ratio of hyperparameters per parameter for each level of metaoptimization Per haps one could take inspiration from classical machine learning techniques 74 where a recurrent neural net work is used to relate the dierent rates of descent at subsequent iterations thus providing an educated ansatz for how these should relate to each other and thus re ducing the number of unique degrees of freedom in the hyperparameters To incorporate such a technique into40 QDD would require adding a hyperhyperparametric cir cuitneural network to relate the hyperparameters be tween iterations which would require a modication of the QDD approach we leave this for future work 3 Network Architecture Optimization Another application of the Quantum MetaLearning principle is for Quantum Network Architecture Opti mization In some instances one may want to optimize over various network architectures in order to improve performance eg one may optimize whether a certain set of parametric circuit elements should be applied or optimize over a space of possible neural network connec tomes topology of connections This problem can be seen as a metalearning problem as the goal is to pick the network architecture which performs best once each network is trained As such the optimization must be done over a space of trained networks and this space of architectures is generally discretely parametrized To enact this optimization we can adapt techniques of quan tum discrete parametric optimization from section IV B and combine it with some of the machinery from our treatment of dropout IV C 3 along with the principles of MetaQDD or MetaMoMGrad which were just discussed above The key to network architecture optimization will be to have ancillary quantum registers to index the architec ture Luckily we have already developed a formalism for this in section IV C 3 as such we will use the same no tation in this section Recall our general decomposition of the parametric unitary from 170 U L Y 1 U U O jI Ujj 193 We can add a set of N control qubits ancillas of Hilbert space HA which will each control whether a certain sub set of parametric unitaries is applied These qubtits can be seen as hyperparameters and as such we can con struct the hyperparametric unitary same as eq 172 of the form U ZA L Y 1 Y jI Cj Ujj 194 where I N j0Aj is the partition of indices which groups operations for which we wish to share the same control parameter eg multiple parametric operations of a neuron The subset of indices A0 corresponds to the set of indices of unitaries over which we would not like to optimize The operator Cj is dened in equa tion 171 it is simply a way to index which control qubit each unitaries is assigned For further details on how to compile this unitary refer to section IV C 3 Now we have outlined how to convert a given para metric circuit to a hyperparametric circuit with architec ture index qubits we can simply apply techniques from the MetaMoMGradMetaQDD from subsection IV D 2 combined with the adaptations of MoMGradQDD for discrete optimization from section IV B The key is to re place the parametric unitary U from 193 with the architecturehyperparametric unitary U ZA from 194 For architecture MetaLearning MoMGrad we can start in a pointer state of the architecture hyper parameter registers as in 144 0A N O k1 cosk 0Ak i sink 1Ak 195 and a pointer state of choice for the regular parame ters 0 see sec III C Onto this joint pointer state of choice we can then apply the modied feedforward hyperparametric unitary from U ZA 172 then the usual loss function exponential and then uncompute the feedforward in order to complete the eective hyper parametric phase kick unitary eg ei Lj ZA U ZAei Lj U ZA 196 and by batching multiple kicks like the above into a mini batch Bk we can enact an eective phase kick on the joint system of hyperparameters and parameters eikJ ZA Y jBk eikLj ZA 197 This exponential eective cost function can then be used for a minibatched architecturedependent Quantum Dy namical Descent by interlacing some kinetic pulses on the parameters just as in regular QDD Uaqdd ZA Y kB eik 2eikJ ZA 198 Note that as opposed to our method in dropout sec IV C 3 in this metalearning approach the state for the architecture qubits 0 is kept in quantum memory be tween QFB runs and for multiple QDD iterations rather than being reinitialized every run Now similar to 190 and 189 one can perform a metaQFB using UAQDD as the metafeedforward U aqdd ZAeiJm ZA Uaqdd ZA 199 where eiJm ZA is a cost function hyperparametric phase kick for the test set of the data We can let eiK ZA be the the eective phase induced on the ar chitecture hyperparameters by the operation in 199 To optimize the architecture hyperparameters we can then either apply a discrete MetaQDD Y j eij XAeijK ZA 20041 where the hyperhyperparameters will need to be op timized One option being combining this architecture metaoptimization and the regular metalearning from the previous subsection into one metaoptimization loop We leave this as exercise to the reader Finally another option is to perform a discrete MetaMoMGrad by measuring XA after phase kicks eijK ZA and updating the angles j according to the estimated gradient as prescribed in IV B Another option which might be benecial in this case would be to use continuumembedding for the discrete parameters since the gradient estimation can be much more negrained V QUANTUM NEURAL NETWORK LEARNING In this section we will elaborate upon the use of the ideas presented in the previous section for the purpose of quantumly training deep neural networks on a quan tum computer to solve machine learning problems in volving classical data Here we will present a quantum neural network architecture which encodes classical neu ral networks as quantum parametric circuits along with an indepth analysis of the phase kick backpropagation procedure from the previous section and how the error signals backpropagate through the quantum neural net work A QuantumCoherent Neural Networks In this subsection we show how to encode a classical feedforward neural network into a quantum computa tion and how to leverage the Quantum Feedforward and Baqprop as well as optimization techniques introduced in Sections III and IV for the training of such a network 1 ClassicaltoQuantum Computational Embedding A central principle employed in this section is the ability to encode a classical computation into a quan tum computation 76 In general for an nbit input x Zn 2 and a computable function from n bits to m bits f Zn 2 Zm 2 we can embed the classical compu tation as a unitary quantum computation Uf Ha Hb acting on n m qubits 55 This unitary takes compu tational basis state equivalent of the input x an maps it as follows Uf x 0 x fx 201 Thus this quantumconverted classical function maps computational basis states to computational basis states Note that trivially by the linearity of quantum operators superpositions of computational basis states get mapped to entangled superpositons between the possible inputs and their corresponding standard basis outputs UfP j j xj 0 P j j xj fxj 202 Of course since the evaluation is unitary the above com putation is fully reversible U f P j j xj fxj P j j xj 0 203 Notice that for such functions the probability ampli tudes are unaected during the evaluation That is each branch of the wavefunction labelled by the xjs evolves independently of the others This property will be harnessed during the computation and uncomputa tion stages of the Quantum Feedforward and Backwards Quantum Propagation of Phase errors QFB That is we use the ability to query classical functions in super position in order to tag the output with relative phase shifts and follow this with by uncomputation The com bination of all three of these steps causes appropriate momentum kicks for the parameters which can be lever aged for optimization using techniques from Sections III and IV Simply by the nature of the embedding of a classi cal computation into a quantum computation by the re quirement of reversibility we are forced to store the com putational graph of the classical computation in quan tum memory For a Directed Acyclic Graph represent ing the ow of classical variables being transformed by a composing multivariate functions such as is the case for neural networks this socalled computational graph 77 then has to be embedded into an entangled set of quantum registers which hold the history of the compu tation The encoding of computation into multiple quan tum registers can be seen in a sense as embedding the classical computational graph in quantum memory it is then natural that one can backpropagate a phase error signal through the computational graph via uncomputa tion which we know carries gradient information This generalized backpropagation through a general compu tational graph is called Automatic Dierentiation AD the specialization of AD to Neural Networks is what is considered to be the error backpropagation algorithm In subsection V B we analyse indepth how the phase signal is carried through during the uncomputation and how one can rederive the classical neural network backprop agation principle from it Although we do not explic itly do so this analysis could then easily be extendable to a general computational graph thereby providing a demonstration of emergence of automatic dierentiation through quantum phase backpropagation in a general set ting Although recent progress has been made to perform common classical operations eciently on a quantum computer 76 in general synthesizing quantum circuits for quantumembedded classical computations may not always be ecient On the other hand our focus is on training neural networks which only require certain42 types of operations namely multiplication addition and the evaluation of activation functions for continuous val ues In this section we will thus cover how to addition and multiplication using machinery introduced in the background section II A Later in this section see V C we cover possible implementations of activation functions commonly used in classical machine learning 2 Classical Data Phase Kicking First let us begin by detailing how exactly to enact the phase kicking according to a classical loss function For purposes of demonstrating the key concepts we will consider the employment of the QFB algorithm for an classical supervised learning although it could also be used in other contexts In classical supervised learning the goal is to build a model for a function f x 7 y based on training data xj yjj The model con sists of a parametrized ansatz f x with parameters Every set of parameters gives a prediction denoted y f x As above the Hilbert space for the pa rameters will be denoted H The Hilbert space for the computation requires registers for the inputs and the pre diction HC HxHy Note that in the case of training with a superposition of data one would also requires a set of registers for the output Hy For the moment we will only consider using a single data point at a time so each yi will only enter the loss functions as a classical parameter although the extension to encoding the out puts in a quantum register is straightforward depending on the loss function For now consider a single inputoutput pair x y The parametrized algorithm for classical training is a uni tary Uf that computes f x ie Uf x 0 7 x f x 204 Later in this section we will be constructing explicit circuits quantumcoherent neural networks that imple ment Uf For now we will write this unitary some what abstractly as Uf X x xx eifx p y eif x p y 205 where py is the generator of shifts in the prediction reg ister Our notation will be suggestive of continuous reg isters although this can be achieved for discrete registers as well Now we apply a loss function which compares the out put to the prediction eiLyy 206 For example the loss function could be Ly y y y2 2 207 note that this is still an operator which acts on Hy The loss exponential of such a meansquared error loss is e ciently compilable into a tensor product of secondorder phase exponentials of each register In principle this loss function could be any classical computable function which maps the output to the set of reals After uncomputing with Uf the entire QFB cir cuit is UQFB eif x p yeiLyyeif x p y eiLyf xy 208 Applied to the momenta of the parameters we have that Eq 209 gives U QFB k UQFB k k Ly f x y 209 In particular we see that all of the higher order terms vanish since all the operators in L commute and hence L commutes with its partial derivatives For classical data all of the terms in the above update rules for the momentum truncate at rst order in Thus for training a classical machine the momentum of each parameter gets shifted by an amount equal to the partial derivative of the loss function We also have the important fact that the parameter and computational registers are not entangled at the end of the QFB circuit hence the parameters will experience no decoherence due to these operations Of course this is assuming perfect position eigenstates for the compu tational registers Hx and Hy the parameters will expe rience some decoherence if this is not the case eg if one is using nitely squeezed continuous variable pointer states Notice that UQFB applied to the initial state yields UQFB X x 0 7 X eiLfxy x 0 210 Since UQFB leaves x 0 HC invariant then the QFB circuit simply tags dierent values of with a phase de pending on the corresponding output of the circuit In this case we get a true phase kickback Because UQFB acting on this initial state does not generate entangle ment between the parameter and the computational reg isters then for further data points it is simple to coher ently reinitialize x 0 to input a new data point x 0 For multiple data points in a minibatch xj yjjB we begin in a state X 0 0 H Hx Hy 211 For each data point we can rst shift the input register to the appropriate xj apply the QFB circuit with the43 appropriate output yj in the loss function and then shift the input register back to zero and repeat for all of the data points in the minibatch Explicitly the algorithm is Y jB eixj pxeiLyf xyjeixj px Y jB eiLyf xxjyj 212 This maps the parameter momenta and the state to re spectively k 7 k X jB k Ly f x xj yj 213 and X 0 0 7 X ei P jB Lfxjyj 0 0 214 We see that the momenta of the parameters and the phase induced in the nal state is according to the cost function J 1 B X jB Lf xj yj 215 and accumulated kicking rate B Note that this discussion also applies if f is comprised of multiple layers f x fNN f22 f11 x 216 with parameters are divided up among these layers as N 2 1 The update rule for the mo menta of the parameters that we derived above also holds in this special case so abstractly we can conclude that the uncomputation step of the QFB algorithm indeed propagates gradient information back through the com putational graph In Section V B we will explore indepth how this mechanism behaves in the context of backprop agation of error in neuralnetwork type computations as a special case of the above analysis A nal note about implementation of the loss function for classical data problems Occasionally it may be more practical to use an auxiliary register to store the compu tation of the loss function rather than to exponentiate the loss function as a phase kick That is suppose we added another register HL and appended a computation ULy to the feedforward operation ULy Uf x 0 0 7 x f x Lf x y 217 which we can denote abstractly as ULy eiLyypL Then instead of exponentiating the loss function as a phase kick we simply apply a linear phase shift eixL to this new register before uncomputing the modied feedforward circuit In all this modied QFB algorithm is Uqfbl eifxp yeiLyypLeixLeiLyypLeifxp y eifxp yeixLLyyeifxp y eixLLyfxy 218 We see that if we initialize the new register HL to xL 0 then this is equivalent to 208 Now we will proceed to discuss the construction of the circuit Uf which computes the output to a neural net work with quantum parameters We will also discuss in detail the feedforward and backpropagation mechanisms in this setting in order to make some of the previous dis cussions more concrete 3 Abstract Quantum Neuron Classical neurons usually act by taking as input a col lection of signals adding up these contributions in a weighted fashion and applying a nonlinearity to this sum to nally generate an output A simple example given a vector of inputs x weights w and bias b the mapping corresponding to the neuron is given by x 7 w xb where R R is a nonlinear activation function To embed this mapping into a quantum computation we need to make the whole process reversible as quantum computation ultimately has to be enacted by unitary hence invertible operations To do this we can assume that the weights the inputs and the outputs activa tion are all quantum number registers either continu ous variable or a discrete variable binary approximation thereof The quantum neuron should ideally map xi w bw 0a 7 xi w bw w x ba 219 which could be implemented via an idealized unitary which enacts the above map ei wxbpa 220 where a is the position quadrature of the activation reg ister and pa is its canonical conjugate a pa i In Figure 15 we picture such an abstract neuron and a cor responding abstract quantum circuit As the above form is quite abstract let us briey de scribe how we could unpack the execution of the above feedforward operation while remaining fairly abstract In section V C we outline various circuits and physical scenarios which would practically enact such a mapping either using nonlinear optics or using ancilla registers44 Figure 15 Diagram of abstract quantum neuron model Left is a representation of the neuron itself right is an abstract quantum circuit representing its feedforward operation and phase estimation In both cases the weighted con tributions of the input are rst accumulated in collector register c as such xi w bw 0c 7 xi w bw zc where z w x b The remaining operation is to take the stored in the collector register and synthesis the computation of the activation function R R as a quantum circuit zc 0a 7 zc za where a is the label for the continuous activation register For a general classically computable activation function for which we know a classical circuit one could directly con vert it to a quantum circuit using Tooli gates 55 or more ecient coherent implementations of basic classical functions 76 although this may end up being somewhat inecient For more details see section V C for various examples of lowoverhead implementations of certain ac tivation functions In gure 16 we represent a neuron with a collector degree of freedom and the corresponding twostage abstract circuit Until we reach V C for sake of compactness and gen erality of our analysis we will use the ideal form of the feedforward unitary from equation 220 4 Quantum Neural Network Feedforward Baqprop A typical feedforward neural network is comprised of a collection of neurons organized into layers Each layer can be thought of as a unit which takes the activations of the neurons of the previous layer and produces a col lection of output activations based on some simple non linear function of the input activations The composition of many layers produces an output activation for the en tire network which overall can be seen as a nonlinear function of the input decomposed into the simpler func tions of each layer The output of the network produces a prediction for the supervised learning problem The pa rameters of the function consist of the weights and biases of the collection of neurons in the network In our above notation the feedforward step of comput ing the prediction of the neural network on a quantum computer is the computation of the unitary Uf x 0 7 x f x 221 Figure 16 Diagram of a neuron with collector and its corre sponding feedforward quantum circuit The controlledadders are of the form 2 and the controlcontrol adder are of the form 3 The operator involving U computes the nonlinear activation function on the collector register and stores the result in the activation Recall for each data point the quantum phase kick back propagation consists of three steps the feedforward costfunction phase kick and backpropagation The quantum neural network described here will con sist of a set of quantum number registers for the input to the network as well as the weights biases and output activations for each neuron in the network Let the neu rons in a single layer be indexed by n 1 N and let the layer index run from 1 L Recall that ideally the neuron n enacts a1A1 wn bnWn 0An 7 a1A1 wn bnWn wn a1 bnAn 222 using a unitary ei wna1bnpan 223 which acts on the Hilbert space of the activations of the layer 1 as well as those of the weights and activation of neuron n ie HA1 HWn HAn Of course our notation implies HA NN n1 HAn so for example a ann ie a is a vector of operators whose nth component is an Combining the action of all the neurons in layer we45 have a1A1 N O n1 wn bnWn 0An 7 a1A1 N O n1 wn bnWn wn a1 bnAn 224 To compress the notation a little let us write W wT nn b bnn Wa1 b wn a1 bnn note that W is a matrix with rows wT n and acts componentwise on elements of the vector Wa1 b and W bW NN n1 wn bnWn aA NN n1 anAn The previous equation for the action of layer in this notation is then a1A1 W bW 0A 7 a1A1 W bW Wa1 bA 225 under the unitary ei Wa1b pa 226 The feedforward for the entire network consists of a concatenation of these unitaries Uff ei WL aL 1bL paL ei W2a1b2 pa2 ei W1 xb1 pa1 L Y 1 ei Wa1b pa 227 where a0 x ie the input to the network The feedforward unitary maps xI L O 1 W bW 0A 7 xI L O 1 W bW aA 228 where the als satisfy the recursion relation a Wa1 b a0 x 229 Of course the output of the network is aL satisfying the above recursion which corresponds to the predic tion of the network upon input x Notice that this is indeed of the form of 221 with the input register xI parameter registers L 1 W bW and prediction reg ister aL AL along with auxiliary registers for the in termediate activations L 1 1 aA At the end of the feedforward the phase kick for data point x y is generated by eiLaL y aL AL 7 eiLaL y aL AL 230 The state after the phase kick is eiLaL y xI L O l1 Wl blWl alAl 231 After the phase kick U ff is employed for backpropaga tion The phase is not aected by the backpropagation U ff simply uncomputes the activations of each layer in the reverse order of the feedforward For example the rst uncompute is ei WL aL 1bL pAL eiLaL y xI WL bL WL aL AL L 1 O 1 W bW aA 7 eiLaL y xI WL bL WL 0AL L 1 O 1 W bW aA 23246 This continues until we arrive back at the initial state along with a phase eiLaL y xI L O 1 W bW 0A 233 where in the phase we still have a Wa1 b a0 x 234 This demonstrates more concretely how the quantum phase kick backpropagation algorithm can be performed for a quantum neural network In the next section we examine in further detail the quantum mechanism be hind the backpropagation of the error signal during the uncomputation step and explicitly show how the Quan tum Phase Error Backpropagation relates to the classical Backpropagation of Errors B Quantum Phase Error Backpropagation Layerwise Analysis Although we have already examined the trajectory of the state of the entire network under the QFB circuit and have a general form for the parameter momentum updates here we want to examine the internal mecha nisms of the feedforward and backpropagation more con cretely For instance we showed before that under the full QFB circuit Uqfb U ffeiLaL y Uff the mo menta of the parameters are shifted by U qfb Uqfb L U ffy Uff y In this case we have y aL and abstractly represents the collection of weights and biases W bL 1 However here the purpose is to ex amine the propagation of impulses in the network layer bylayer to better understand the behavior of the net work during training algorithm A key observation of this section will be that during the training the activations of a layer a are always inu enced only by the activations of previous layers whereas the momenta of the activations weights and biases pa pW and pb respectively are directly aected by ac tivations of previous layers along with momenta of later layers Ultimately this is what allows the feedforward operation to propagate signals forward in the network and the uncomputation to propagate momentum updates backward through the network For convenience we will write the feedforward unitary for layer as U ei Wa1b pa 235 and the feedforward from layer to with as U ff U U1 U1 U 236 Of course the feedforward for the entire network is Uff U L 1 ff Notice that each of the operators a pa pW and pb are only directly aected by one of these unitaries Of course they can depend indirectly on the others For example the activation for layer a is only aected by U on the forward pass and U on the backward pass since these are the only operators in the QFB circuit containing the conjugate operator pa The feedforward unitary for layer changes the acti vation by U a U a Wa1 b 237 The operation in the backpropagation U just changes the sign of the shift U a U a Wa1 b 238 We see that the activation in one layer depends on the activation of the previous layer as well as the weights in the current layer Of course in the full feedforward cir cuit the activations in the previous layer will also depend on the preceeding layers so we get a recursion U 1 ff a U 1 ff a W U 11 ff a1 U 11 ff b 239 which ends with U 1 a1 U1 a1 W1 x b1 240 Of course these expressions are unaected by the remain ing feedforward operations U L 1 ff so we could also write U ffa Uff a W U ffa1 Uff b 241 In the backpropagation steps we see that the activa tion is still only aected by the activations of the previous layers Uffa U ff U L ff a U L ff a Wa1 b 242 Therefore we see clearly that the domain of inuence of the activations consists only of activations and weights of the preceeding layers Furthermore since the activa tions are not directly aected by the phase kick at the output eiLaL y the entire QFB circuit simply com putes and then uncomputes the activations U qfba Uqfb U ffeiLaL y Uffa U ffeiLaL y Uff U ffeiLaL y a Wa1 b eiLaL y Uff U ff a Wa1 b Uff a W U ffa1 Uff b W U ffa1 Uff b a 24347 This fact could have been deduced more easily by writ ing Uqfb U ffeiLaL y Uff eiL U ffaL Uffy and noticing that in the above recursion relation for U ffaL Uff that no pas appear However the purpose of this exercise was to demonstrate that the activations only depend on the values of the activations and weights in the previous layers Hence insofar as the activations are concerned there is only a forward propagation of in formation in the network Now we will discuss the momenta of the activations and the weightsbiases We will see that these will be aected by both earlier and later layers in the network In partic ular these respond to the activations in previous layers and momentum kicks in succeeding layers Therefore to propagate information forward in the network we have to act on the activations and to propagate backwards we have to act on the momenta of the activations The momenta of the activations in layer pa are only aected by the unitary U1 The single exception is paL which is aected only by the phase kick eiLaL y For the nal layer we get eiLaL y paL eiLaL y paL LaL yaL 244 and for L we have U 1 pa U1 pa W T 1 W1a b1 pa1 245 where is the derivative of the nonlinear activation function acting on components of the vectorial argument and denotes componentwise multiplication ie x1 y1 x2 y2 x1x2 y1y2 246 Also note that U1 pa U 1 is the same expression with the opposite sign for the shift We see explicitly that the shift in the momentum of the activation for a layer depends on the activation of that layer as well as the momentum of the activation and the values of the weightsbiases of the following layer For the full feed forward circuit we get U ff pa Uff U 11 ff pa U 11 ff pa W T 1 W1 U 1 ff a U 1 ff b1 pa1 247 Note that the momenta get kicked on the forward pass not just in the backpropagation since the shift depends on the current activation which in turn implicitly de pends on activations and weightsbiases earlier in the network due to the feedforward Uff Now if we look at the backpropagation in isolation without the preceding feedforward and phase kick we get Uff pa U ff U L 1 ff pa U L 1 ff pa W T 1 W1a b1 U L 2 ff pa1 U L 2 ff 248 Which as before is shifted according to the activation in the current layer as well as momenta and weightsbiases in the following layer However the full backpropagation also carries inuences from later in the network through U L 2 ff pa1 U L 2 ff In summary we see that roughly the activations carry information forward through the network via the feed forward operations and the momenta of the activations carry information backward through the network via the uncomputation operations Therefore for the entire QFB circuit we feedforward the activations to make the prediction kick the momentum of the output activation and then this momentum kick propagates back to the re maining activation momenta and returns the activations to their original state Explicitly for L U qfb pa Uqfb U ffeiLaL y Uff pa U ffeiLaL y Uff U ffeiLaL y pa W T 1 W1a b1 U L 2 ff pa1 U L 2 ff eiLaL y Uff U ff pa W T 1 W1a b1 eiLaL y U L 2 ff pa1 U L 2 ff eiLaL y Uff pa W T 1 W1 U 1 ff a U 1 ff b1 pa1 W T 1 W1 U ffa Uff b1 U ffeiLaL y U L 2 ff pa1 U L 2 ff eiLaL y Uff pa W T 1 W1 U ffa Uff b1 pa1 W T 1 W1 U ffa Uff b1 U qfb pa1 Uqfb 24948 We have a shift which is a sum of the shift on the forward pass and the backward pass A more illustrative way to look at this is in terms of dierences U qfb pa Uqfb pa W T 1 h W1 U ffa Uff b1 U qfb pa1 Uqfb pa1 i 250 This shows that the dierences of the activation momenta before and after the QFB circuit propagate back recur sively ie here from pa1 to pa Of course the recursion ends with the output of the network where we apply the loss function to kick the output activation momentum U qfb paL Uqfb paL L U ffaL Uff y 251 where the derivative on the loss function is understood to be with respect to the rst argument This propagates back via the above recursion to kick the momenta of the activations throughout the network These activation momentum updates in turn aect the momenta of the weights and biases which are the shifts that we are actually interested in for the training The calculation is similar to that for pa It is simple to show that for a single feedforward step U pW U pW Wa1 b paaT 1252 U pb U pb Wa1 b pa 253 where we note that here we have respectively a matrix and a vector of operators For the full feedforward and uncomputation we get respectively U ff pW Uff pW W U ffa1 Uff b pa U ffaT 1 Uff 254 U ff pb Uff pb W U ffa1 Uff b pa 255 and Uff pW U ff pW Wa1 b Uff pa U ffaT 1 256 Uff pb U ff pb Wa1 b Uff pa U ff 257 Using these one obtains for the full algorithm that U qfb pW Uqfb pW h W U ffa1 Uff b U qfb pa Uqfb pa i U ffaT 1 Uff 258 U qfb pb Uqfb pb W U ffa1 Uff b U qfb pa Uqfb pa 259 Therefore the update for the momentum of the weights is directly related to the update of the momentum of the activation of the same layer With the formula we derived before for the update of the activation momentum the kick in the activation momentum of this layer depends on the updates of the following layers back to the kick at the output of the network Together the equations 250 258 and 259 provide the key insight into the physics of the backpropagation of errors in the quantum neural network 1 Operator Chain Rule There is yet another way of viewing backpropagation in the Heisenberg picture by directly applying the chain rule to the loss function This perspective of the back propagation of errors is not as vivid as in the previous section but is more closely related to classical backprop agation which would be written schematically as L W L aL aL aL 1 a W 260 Recall that from above we have the QFB circuit for the neural network as Uqfb eiL U ffaL Uffy Therefore we can write U qfbpai Uqfb pai iT L U ffaL Uff y U ffaL Uff pai 261 where pai denotes the ith component of the vector pa It is straightforward to write a similar expression with pai replaced by pWij or pbi Note that the term on the righthand side is analogous to writing L a L aL aL a 262 which is akin to forward mode accumulation of automatic dierentiation One typically continues with backpropa gation by iterating this procedure of using the chain rule In our operator picture this proceeds by successively us ing the following identity U ffa Uff pa i W U ffa1 Uff b W U ffa1 Uff pa i 263 which holds for One can check that this com mutator vanishes for the cases where Hence this backpropagation procedure terminates at where one can show that49 h U ffak Uff paii iki 264 h U ffak Uff pWiji ikiek W U ffa1 Uff b U ffa1j Uff 265 h U ffak Uff pbii ikiek W U ffa1 Uff b 266 where ek denotes the kth standard basis vector It is simple to show that these expressions can be used to derive the equations 250 258 and 259 from the previous section C Implementations of Quantum Coherent Neurons Recall that the idealized neuron takes a vector of in puts combines it with a vector of weights and scalar bias and outputs a scalar activation as a nonlinear function of this combination ei wxbpa xI w bW 0A 7 xI w bW w x bA 267 The linear operations can be implemented in a straight forward manner using digital or continuous adders gen eralized CNOTs and the multiplications from two regis ters into a third can be done using generalized CCNOTs all of which were outlined in Section II The step which is less palpable is the application of the nonlinear activation function Two activation functions which are commonly used in classical neural networks are the sigmoid function z 1 1 ez where R 268 and the rectied linear unit ReLU z max0 z 269 The parameter in the sigmoid function controls the sharpness of the transition In the limit it be comes a step function In this section we will discuss means of approximating the sigmoid and ReLU activation functions with quantum circuits for the purpose of implementing the quantum neural network described above We will rst examine an implementation using a hybrid discretecontinuous vari able or simulated continuous variable system which is based on the phase estimation algorithm Although using phase estimation requires some overhead of gates projec tion onto the positive subspace of the input can be done easily The second method is a fully continuous variable or simulated continuous variable implementation In this case projection onto the positive subspace of the in put requires some overhead to enact a nonlinear phase gate and also requires squeezed states for precision Of course this second issue can be overcome if using simu lated continuous variables on a suciently large quantum computer In both cases of hybrid CVDV neurons and CVonly we will separate the procedure of applying the nonlinear activation into stages The rst will be simply to assume the combination of inputs weights and bias are stored in a continuous variable collector register c That is we will assume that we have prepared xi w bw 0c 7 xi w bw zc where z w x b The aim is to take the value stored in the collector register and approximate the computation zc 0a 7 zc za where a is the label for the continuous activation register 1 Hybrid CVDV Neurons For the current case of hybrid discretecontinuous vari able neurons we will also make use of an intermediate discrete variable lter register f which will be taken to be a collection of N qubits The purpose of this inter mediate lter is to determine the sign of the value of the collector This will allow for an implementation of a step function as an approximation to the sigmoid function as well as ReLU The rst stage of this version of the neuron will be to perform phase estimation on the collector using the lter subsystems as the pointer system We will use the nota tion of Section II and write a simulated position operator df for the lter system where d 2N The spectrum of df should be taken to encompass the range of expected values of the collector variable z For convenience we will assume that the range is symmetric about the ori gin and will denote the maximum value by R Thus the discrete variable system aims to provide a simulation of the collector variable z on the interval a b R R The phase estimation step can then be written as zc df d zc 0f zc X kZd z d 1 2R k kf zc 1 X x1xN0 2N 1 z 2R N X n1 xn2n1 x1 xNf 27050 Note that although we only wish to determine the sign of z thus only the value of the most signicant qubit xN the use of additional qubits aids in suppressing the probability of error as discussed in Section II Now we can proceed to implement the nonlinear ac tivation by conditioning on the value of the most signi cant qubit after the phase estimation step For example the sigmoid function can be approximated with a step function by acting the unitary e i 2 1 ZN 2f a 11N f eia 00N f Ia 271 where 1 21 ZN 2f 11N f is the projector onto the value 1 of the most signicant qubit in the lter register Therefore if the value of this register is 1 which corre sponds to z 0 the activation register is shifted to a value of 1 otherwise it retains its original value of 0 The case of ReLU can be approximated similarly with the unitary e i 2 zc1 ZN 2f a 11N f eizc a 00N f Ica 272 Here we see that if z 0 the unitary which is im plemented is eectively an adder zc 0a 7 zc za otherwise it is just the identity 2 CVonly The case of a fully continuous variable implementation of the quantum neurons does not involve a lter register but a single unitary applied to the collector and activa tion registers eiP zca zc 0a 7 zc Pza 273 where P is some polynomial67 The idea here is to choose a polynomial to approximate the desired activa tion function on a particular interval a b Suppose we wish to approximate the activation func tion with a polynomial of xed degree N so that Pz PN n0 cnzn One possibility is to truncate a Taylor se ries of the activation function if it exists to order N Another possibility would be to choose the coecients cnN n0 to minimize the distance between this polyno mial and the desired activation function in some norm For example one could choose a weighted L2 norm on the interval a b and minimize the meansquared error MSE 1 2 Z b a wzdz z Pz2 274 The weight function wz can be used to demand more accuracy on particular regions of the interval which are of interest The minimum is achieved by choosing coe cients which solve the matrix equation N X m0 Z b a wzdz znm cm Z b a wzdz znz 275 Solving for these coecients amounts to inverting the Hankel matrix with elements Tnm R b a wzdz znm where n m 0 N and applying the inverse to the righthand side of the equation For a sigmoid func tion z 11 ez and uniform weight func tion wz 1 the righthand side involves calculating a collection of incomplete FermiDirac integrals How ever if we approximate the sigmoid function with a step function or in the case where we are using ReLU as the activation function then evaluation of the elements of the righthand side is trivial assuming a simple weight func tion One can also straightforwardly use this technique to build a polynomial approximation to other nonlinear activation functions provided one can calculate or ap proximate the integrals on the righthand side of the above equation VI QUANTUM PARAMETRIC CIRCUIT LEARNING Quantum Deep Learning of quantum data will gener ally consist of having to learn a certain quantum map As all quantum operations can be seen as unitaries in a dilated Hilbert space possibly along with a standard ba sis measurement learning a certain quantum map will often reduce to optimizing over a space of candidate uni taries in order to minimize some loss function of the out put In general a certain unitary transformation over a large Hilbert space of multiple registers can be decom posed into a composition of unitaries which are unitary on smaller sets of registers Each unitary can be seen as a form of generalized rotation in the Hilbert space of its registers It is then natural to consider parametrized ansatze constructed by composition of such generalized rotations each with a given direction Hamiltonian generator and a certain angle We call parametric quan tum circuits such hypothesis classes of unitaries com posed of multiple unitaries with are each parametrized by real numbers The key to learning is then to leverage ecient optimization strategies to search over the space of possible parameters in order to minimize some loss function The traditional approach to the optimization of these parameters has been a classicalquantum hybrid ap proach In this case the circuit for a certain set of param eter values would be executed and the expectation value of the loss function for a given set of parameters would be estimated Then by querying the expectation value for multiple values of the parameters for multiple runs one could use a classical optimizer to nd a suitable set of pa rameters which minimize the loss to a satisfying degree For example a nitedierence gradient method 26 27 is often used but this approach necessitates ON where N is the number of parameters runs to obtain enough expectation values of the loss for various values of the parameters in order to estimate the gradient Instead of using a hybrid quantumclassical method51 based on estimation of multiple expectation values of the loss function for the optimization of quantum parametric circuits we can harness the Backwards Quantum Prop agation of Phase Errors Baqprop principle to descend the optimization landscape more eciently Given a way to quantum coherently evaluate the exponential of the loss function of a potential candidate solution one will be able to use either Momentum Measurement Gradient Descent or Quantum Dynamical Descent to optimize over the set of possible circuits In this section we will explore various use cases of parametric quantum circuits explain in greater detail how to query exponential loss functions for various cases and explore how the update rule derived in previous sec tions specializes in these various cases A Parametric Ansatze Error Backpropagation Before we talk about applications of parametric cir cuits to various problems and how to adapt the Quantum Feedforward and Baqprop procedure to each application let us briey review parametric circuits in a formal man ner and provide an overview of how error signals back propagate through the circuit during uncomputation 1 From Classically to QuantumlyParametrized Ansatze Let us rst consider a generic classically parametrized circuit ansatz Consider a set of indices for the param eteric operations partitioned into the indices for each layer I L 1I we can write the parametric unitary as U L Y 1 U 276 where U is the multiparameter unitary corre sponding to the th layer which can itself be composed of multiple parametric unitaries Ujjj as follows U O jI Ujj 277 Now suppose we wish to optimize some loss operator Lj for the above parametric unitary applied onto an ini tial state j the typical approach to optimizing this is to compute the expectation value of the loss operator for this feedforwarded state Lj j U Lj U j 278 A classical optimizer is then tasked to nd the set of parameters which minimize the cost function which in general can be the su of multiple loss operators ie argminP jBk Lj In general for a quantum classical optimization procedure multiple expectation values of the loss operators will need to be estimated For example one may perform nitedierence gradient descent by estimating derivatives of each loss at a time k Lj 1 Ljk Lj 279 where kj jk 1 For an Nparameter ansatz and M terms in the loss function in order to estimate the gradient this requires OM N expectation value esti mations which in some cases must each taken in separate feedforward runs Instead of classically parametrizing the circuits as we have covered extensively in this paper we can use quan tum parameters in order to leverage either MoMGrad or QDD As we know from section III gradients can then be estimated via MoMGrad in OM feedforward and Baqprop queries which we then gave techniques to fully parallelize this gradient acquisition over the mini batch sec IV with only Olog M added depth over the singlereplica QFB operation To convert a classically parameterized a circuit of the form 276 to a quantumlyparametrized circuit we con vert U 7 U where U L Y 1 U U O jI Ujj 280 and each unitary is converted to a continuously controlled unitary with a quantum parameter register Ujj X j jj Ujj 281 Now assuming each unitary is generated by a certain Hamiltonian ie Ujj eij hj then the above becomes Ujj eijhj 282 which we see is an exponential with a generator j hj Let us examine how to synthesize such an exponen tial into basic gates For a given index j I suppose we know a way to synthesize the classically parametrized gate eijhj with j as the classical parameter into a product of nonparametric unitaries and of one or multi ple parametric unitaries of the form eikj j Zkj where all kj R We can then convert this synthesis of the classically parametric gate into a synthesis for its respective quantumparametric analogue by converting all the parametric exponentials of j Z into quantum parametric exponentials of j Z ie eikj j Zkj 7 eikj j Zkj 283 Each quantumparametric exponential eikj j Z is es sentially like a singlequbit observable phase estimation52 unitary as discussed in section II B 1 it can be can be broken up into logd exponentials of Z Z where d is the eective qudit dimension of the parameter register Often the generators of these exponentials are chosen to be simple eg nlocal Paulis Pn 50 hence as an explicit example we can consider a case where hj is a Pauli operator on n qubits For any h Pn there exists a V Cn where Cn is the nqubit Cliord group 50 such that h V Zr V where Zr is the Pauli Z on a register of choice which we label as having an index r To decompose Cliord group operator V into basic Clif ford gates there are multiple known algorithms for this synthesis 78 and Cliord gates are very eciently im plementable on errorcorrected quantum computers 79 For such an operator h a parametric exponential of the form eih ei V Zr V V ei Zr V 284 thus to convert this parametric exponential into a quantumparametric exponential we need to apply eih V ei Zr V 285 the quantumphaseestimationlike exponential in the middle can then be broken down into logd exponen tials of Paulis between the qubits of the parameter regis ter and that of the r register As a side note for analog quantum computers for parameter registers which are physical qumodes the quantumparametric exponential ei Z can be implemented using an interaction Hamil tonian of the form Hint Z 286 where is a quadrature of a qumode and some coupling strength Such an interaction should be feasible to im plement in various quantum computing implementations of today 80 Now that we have seen how to execute quantum parametric unitaries let us recall that in order to op timize the parameters such as to minimize some loss operator on the output Lj we can execute the quantum feedforward and quantum phase error backwards prop agation QFB procedure with quantumparametric cir cuits and leverage techniques from section III for opti mization Recall that the QFB consists of applying the quantumparametric feedforward operation an exponen tial of the loss function followed by the uncomputation of the feedforward ei L U ei Lj U 287 Recall 54 that for an input state j to rst order in the momenta of the parameters get kicked by the gradient 7 eiL eiL O2 L O2 288 where the eective loss function is given by L L 289 We can then leverage this momentum shift to optimize the parameters via MoMGrad or QDD see sec III No tice that all components of the momentum get kicked but each component of the parameters comes into con tact with the compute at a dierent time during both the feedforward and Baqprop phases In order to under stand how exactly the error signal backpropagates and inuences the various parameters momenta during the uncomputation we can further examine how the param eters get kicked in a layerwise fashion which we do now below 2 Quantum Parametric Circuit Error Backpropagation In Section V B we elaborated upon the mechanism through which the QFB circuit propagates the errors layerbylayer for quantumcoherent neural networks through the recursive formulas 250 258 and 259 Here we will briey discuss a layerwise analysis of the quantum phase error backpropagation for layered quan tum parametric circuits Of course since we are using a very general ansatz for the quantum parametric circuits we cannot repeat the analysis in the same level of detail as for the quantumcoherent neural networks Consider once again a parametrized circuit decom posed into layers U L Y 1 U U O jI Ujj 290 where L 1 is the operator vector of all the parameters and jjI is that of the param eters for a single layer For convenience let us write the circuits of operations before and after the layer k as U kk k Y 1 U U kk L Y k U 291 Recall the QFB circuit for the entire circuit is U ei L U 292 Now suppose we would like to focus on a certain layer in the QFB circuit above we could group the feedforward phase kick and uncomputation operations for layers be yond layer as U ei L U ei L 29353 we see that this is just a loss exponential with respect to a dierent loss operator L U L U 294 which is eectively a backpropagated loss operator in the Heisenberg picture Similarly we can group the oper ations for layers below the layer combined with the backpropagated loss exponential from above the whole QFB circuit can then be seen as U U ei L U U 295 which is eectively like a singlelayer QFB with the back propagated loss and a modied input state being the fedforward input state In a sense we can view the above picture as a quan tum form of automatic dierentiation 77 In classical automatic dierentiation for a composite of functions composed in layers in order to compute the gradient of the output with respect to a certain parameter the gradient of layers beyond that of the parameter are com puted layerwise starting from the output Using both this backpropagated gradient and the value of the fedforward input up to the layer of the given parameter of interest one can compute the gradient for the said parameter We can try to examine how automatic dierentiation is nat urally executed in each branch of the multiparameter wavefunction by the Quantum Phase Error Backpropa gation Let us label eigenstates of all parameters other than those of layer as N j j where j are the correponding parameter operators Furthermore consider the very initial input state to the whole QFB circuit to be and let us dene the condi tional feedforwarded state up to layer as U 296 Suppose we consider each branch of the wavefunction of parameters of layers other than eg each term in P then conditioning each branch of this wavefunction we get an eective phase kick on the parameters of layer Equation 295 becomes the fol lowing conditional eective phase on the parameters of layer X L 297 where L U ei L U 298 We see above that for each case branch of the parameter values wavefunction for value there is an incoming state to layer and there is a backpropagated phase kick operator ei L This is similar in vein to classical automatic dierentiation but this happens in every branch of the wavefunction in parallel In a sense it is automated automatic dierentiation Because each parameter can take gradients of the loss conditioned on previous and later layers quantum states all parame ters momenta can thus get nudged simultaneously by the gradient of the conditional loss in each branch of the wavefunction This allows for singlesweep gradient esti mation of all parameters in contrast to some other tech niques for parametric circuits which require each deriva tive to be computed one at a time 81 Now recall from section V in the case of the quantum neural networks we rewrote analysed how the backprop agating error signal is carried between parameter regis ters by the compute registers In the case of coherent neural networks the phase kick corresponding to the er ror signal would kick the activations momenta which in turn kicks the momenta of the weights and biases Here to see which operator is getting kicked one would need to examine more concretely the conjugate of the generator of the unitaries in each layer of the circuit Performing such an analysis could shed some light as to what makes a good choice parametric circuit ansatz such as to avoid the vanishing gradient problem of most currently known ansatze 51 We leave this remaining analysis for future work B Quantum State Exponentiation In this subsection we will delve into greater detail into the ways to enact a certain set of loss function exponen tials for quantum data In previous section III we showed how we could harness the MoMGrad and QDD optimiza tion procedures given access to a phase kick complex exponential of a loss function operator for which we would like to minimize the error For many applications of quantum parametric circuit learning it will be useful to create the phase kick for a loss function which will pro vide a notion of distance between the output state and the target state and in our case this notion of metric will be induced by some form of inner product between states 1 Single state exponentiation Wellknown in quantum information is the notion of delity between quantum states For pure quantum states the delity F between states and is simply the magnitude of the inner product F Note that clearly delity itself is not a metric but one can cre ate a proper metric on the space of states by considering the sine distance S 55 which is related to the delity by the equality S 1 F In order to perform gradient ascent on the delity in the case of pure state learning54 which we will treat indepth in the next subsection we will need to be able to exponentiate states ie perform ei given multiple copies of in memory That is given a set of n copies of pure states n held in memory we would like to execute the unitary ei to a certain precision by consuming some of these copies More generally for a set of mixed states n held in memory we would like to be able to enact the unitary ei on our target state As we will see in VI C 1 the exponential of mixed states will induce a gradient ascent on the HilbertSchmidt inner product rather than the delity This task is referred to as quantum state exponentia tion QSE 82 The original protocol to perform quan tum state exponentiation was rst formulated by Lloyd Mohseni and Rebentrost 20 This approach was re cently proven to be optimal for the Quantum State Ex ponentiation task 83 For the target state and a copies of mixed states in data the original QSE protocol applies the map n 7 eiei Adei 299 up to an error accuracy in the diamond norm by using n O2 steps each consuming a copy of More explictly this quantum state exponentiation ap proach consists of approximating within diamond norm error the nal target state Adei i 1 2 2 300 with n steps each consisting of partialswapping of a copy of onto the target using an exponential swap oper ation ei S where for a total of n O2 stepscopies In Figure 17 we provide further detail as to the implementation of exponential swaps via more stan dard gates If we look at the eective operation acted upon the on the target register we have tr255 M1 P jM j we can then batch the state expo nential as ei ei P jM j N Y n1 Y jM ei N j O M2 N 305 Since we will be considering both mixtures of mixed states and pure state as input the above techniques are an important option Note this batching is used for the data loading which is dierent from batching phase kicks on the parameters Quantum Feedforward and Baqprop iterations as discussed in IV 3 QRAM Batching Another option to create the mixed state is to use a Quantum Random Access Memory QRAM Although using a QRAM is not essential the QRAM will create a mixture of various states thus eectively preparing a mixed state Given a set of states jjM using a QRAM with a uniform superposition over addresses in the index set M we can prepare a state 1 M X jM ja 7 1 M X jM ja jd 306 where A is the quantum address index and D is the data register using a treelike network of Fredkin gates of depth Olog M 24 The reduced state of the data register with the address traced out is the desired mixed state d 1 M X jM jjd 307 Through multiple queries of the QRAM multiple copies of the mixed state d can be obtained then using the same singlestate exponentiation techniques as described above in VI B 1 for a number n copies we can enact the exponential eid within an error Note that apart from requiring a lower depth there is no clear advantage of using a QRAM batching over sequential batching Once again this batching of state exponentiation is only for data lodaing one can also use a QRAM for minibatching of the descent of the wavefunc tion in the parameter landscape as discussed in subsec tion IV A 4 Then again there does not seem to be a necessity for QRAM in that scenario either C Quantum State Learning Quantum state learning can be seen as the quantum analogue of unsupervised learning In classical ML given samples from a certain distribution using neural network anstaze such as Restricted Boltzmann machines autoen coders or Generative Adversarial Networks one learns a way to sample for the underlying distribution of the data The statistics of the classical probability distri bution are replicated by learning a map which can take as input simple often taken to be uncorrelated random variables called the latent variables and transforms their joint distribution into an approximation of the datas un derlying distribution In quantum mechanics instead of strictly classical probability distributions there are wavefunctions and classical distributions of wavefunctions These are known as pure states and mixed states respectively Similarly to the classical case we can learn a way to map simple distributions such as tensor products of pure states or tensor products of mixed states to the quantum distri bution which underlies the data We begin by learning how to generate pure states given many copies of the same state from data Following this we will cover a way to recover mixed states given copies of the mixed state or access to pure state samples from the distribution 1 Quantum Pure State Learning The pure state learning task is the following given n copies of an unknown state we would like to learn a circuit decomposition which prepares the state with a high delity to the desired state One can achieve this by employing the framework of this paper of optimizing over a family of parametrized circuits U which are applied to an initial resource state 0 This resource state for example could be the computational null state N j 0j of a collection of qubits Depending on the complexity of the pure state to be learned it may be advantageous to exploit any available prior knowledge to begin in a state which is closer to the target state Now we will explain how this task can be solved using the Quantum Feedforward and PhaseKick Backpropa gation QFB algorithm in conjunction with either Mo mentum Measurement Gradient Descent MoMGrad or Quantum Dynamical Descent QDD Recall that a sin gle run of QFB entails an application of the parametrized unitary U on the input state 0 followed by the exponentiated loss function ei L and the uncompute U In the present task of pure state learning the loss function will be L Exponentiation of this loss function can be achieved using multiple copies of through the methods described in Section VI B This circuit is illustrated in Figure 19 For this loss function the eective phase which gener ates the kick in the momenta of the parameters is L 0 L 0 U 0 2 308 Recall that above we dened L U L U ie56 Figure 19 The QFB circuit for quantum pure state learning This consists of a feedforward unitary U controlled by quantum parameters a phase kick achieved through state exponentiation of n copies of in the lower registers and classicallycontrolled by the hyperparameter followed by the uncomputation U The initial state on the computa tional registers is an input resource pure state 0 the evolution of the cost function under the parametrized algorithm U Notice that the eective phase for each value of is minus the squared delity between the output of the parametrized circuit on the input re source state with the desired state Since the momenta are kicked according to 7 L O2 we see that the use of QFB along with MoMGrad or QDD performs gradient ascent on the squared delity of the output of the parametrized circuit with the state we wish to learn 2 Quantum Mixed State Learning The task of mixed state learning is similar to the case of pure states given n copies of an unknown state BH one would like to learn a parametrized cir cuit which prepares a state close to The methods presented here will use the notion of proximity induced by the HilbertSchmidt inner product on BH The parametrized circuit U will act on a pure ini tial resource state 0 on a larger Hilbert space H of sucient size to be capable of containing the purica tion of the state to be learned We will then identify a subsystem of H as the Hilbert space H so that we can decompose H HHc The goal is for the reduced state on H after applying the parametrized circuit to the in put state to approximate Let us denote this reduced state as trHc U 00 U 309 For example if the state is a mixed state on N qubits then one can take the extended Hilbert space to be a space containing 2N qubits Then the goal is to create a mixed state on a subset of N qubits which approximates The loss function will be L IHc acting on H H Hc As before exponentiation of this loss function can be achieved using the methods of Section VI B given multiple copies of the state It is straightforward to show that the eective phase will be minus the HilbertSchmidt inner product between the desired state and the reduced state on H after applying the parametrized circuit on the input L 0 U IHc U 0 trH trHc U 00 U trH 310 Therefore the training algorithm will perform gradient ascent on this inner product The circuit for this proce dure is illustrated in Figure 20 Figure 20 The QFB circuit for quantum mixed state learn ing Again the phase kick is achieved through quantum state exponentiation using n copies of the mixed state lower reg isters The phase kick gate is also classicallycontrolled by the hyperparameter The feedforward and uncomputation unitaries act on a dilated Hilbert space with initial pure re source state 0 The task of mixed state learning is for the feedforward unitary to prepare the desired mixed state on a subset of these registers those upon which the phase kick acts D Quantum Unitary Channel Learning 1 Supervised Unitary Learning One means of learning a unitary operator V is via samples of inputoutput pairs i j o j j Ideally these pairs are such that o j V i j for all j How ever it is possible that the source of these samples is noisy in which case one may need to assume some of the data states are not pure and hence be represented as mixed states related through a channel Such a situation will be subsumed by the following subsection where we describe the process for supervised channel learning In that context one can use a unitary ansatz for the chan nel mapping between mixed states For this section we will focus on the more particular case where the data states are pure and we want to learn a unitary which approximates the ideal unitary V For each inputoutput data pair indexed by j the input to the parametrized algorithm U is i j The loss function will be Lj o j o j which as opposed to state learning will be dierent for every data pair j Again this loss function can be implemented as a phase using state exponentiation given multiple copies of the57 state Using these the eective phase on the parameters for the data pair j will be Lj o j U i j 2 311 ie the negative squared delity between the output of the parametrized algorithm upon input i j and the desired output o j This is quite similar to the phase obtained for pure state learning but here the input and loss functions are dierent for every kick of the momenta This setup is illustrated in Figure 21 Figure 21 QFB circuit for supervised unitary learning The data points i j o j are ideally generated by some unitary operator V which we wish to approximate with U This is achieved by using i j as an input to the QFB circuit and the corresponding projector onto o j is used as a loss function via state exponentiation 2 Supervised Channel Learning Supervised learning of a quantum channel requires inputoutput pairs i j o j j which will generally be mixed states acting on a Hilbert space H Ideally these pairs satisfy o j i j but of course there may be noise in the dataset In a similar fashion to mixed state learning we will employ a parametrized unitary U acting on an ex tended Hilbert space H H Hc We will then train this algorithm so that when restricted to H the algo rithm approximates the channel Explicitly for each data pair j we will have the parametrized unitary U act on i j and an initial resource state 0 Hc Tracing out Hc after the unitary gives a quantumparametrized channel i j 7 trHc Ui j 00 U 312 We will also denote the output of this channel for input as The goal is to parametrize the channel so that for each j this output is close to o j To this end we will take the loss operator to be Lj o j IHc similar to the case of mixed state learning The eective phase we obtain is Lj tr HL Ui j 00 U trHo j i j 313 which is negative the HilbertSchmidt inner product be tween the output of the parametrized channel upon in put i j and the desired output state o j The QFB circuit for this task is illustrated in Figure 22 Figure 22 QFB circuit for supervised channel learning Sim ilar to unitary learning the data points i j o j ideally cor respond to the input and output of a quantum channel The goal of supervised channel learning is to nd a unitary on a dilated Hilbert space such that the desired channel is approximated when this unitary is restricted to a subset of the input and output registers 3 Unsupervised Unitary Learning Another situation in which the methods presented herein can be used to learn a unitary V is when one is given an oracle for V which can be queried rather than a set of inputoutput data pairs The basic idea is to turn the problem into that of state learning on the Choi state of the unitary The oracle for V will be used to create the desired Choi state in order to use it as a loss function This technique can also be used to learn the Choi state of a channel next section but rst we will describe the special case of learning a unitary To generate the appropriate loss function we employ the unitary oracle mapping V H H First let us denote 1 dim H P j jj as a maximally entangled state on H2 equivalent to the identity map in the Choi Jamiolkowski picture The loss function will be the Choi state obtained by acting the oracle on one of the two subsystems of this maximally entangled state L IH V IH V V 314 Exponentiation of this state to obtain a phase operator will require multiple queries to the oracle The parametrized algorithm U will similarly be applied to one of the two subsystems of H2 as an input state ie IH U Then the above loss function will be applied as a phase yielding an eective58 phase on the parameters L IH V IH U 2 trH2 V U trH V U2 315 Notice we have dened analogous to V U IH U IH U 316 This eective phase can be seen in terms of either the HilbertSchmidt inner product on the Choi states of the two unitaries or the square of the HilbertSchmidt in ner product of the parametrized unitary and the desired unitary The setup for this task is illustrated in Figure 23 Figure 23 QFB circuit for unsupervised unitary learning Using an oracle for V one can prepare multiple copies of the Choi state of the unitary for the purposes of state exponenti ation The QFB circuit rst involves creating the Choi state of the parametrized ansatz U as the feedforward then using the desired Choi state as a loss function before the un computation 4 Unsupervised Channel Learning Unsupervised channel learning will be very similar to unsupervised unitary learning with the addition of us ing a parametrized unitary U acting on an extended space H H Hc as in mixed state learning and su pervised channel learning Here we assume access to an oracle for a quantum channel and the task is to use this to learn a set of parameters for the unitary U acting on H so that the parametrized channel BH BH given by 7 trHc U 00Hc U 317 approximates Note that 0 Hc is some resource state as described in the mixed state learning section As in the previous section we will use the oracle and a maximally entangled state to generate a loss function which will be the Choi state of L I 318 The procedure then is to apply IH U to the input state 0 H2 Hc apply ei L followed by the uncompute After tracing over everything except the parameter registers we obtain an eective phase L trH2Hc LIH UHH 00HcIH U trHH 319 where the Choi state of the parametrized channel is I 320 Hence the eective phase is the HilbertSchmidt inner product between the Choi state of the parametrized chan nel with that of the desired channel The setup for unsupervised channel learning is illus trated in Figure 24 Figure 24 QFB circuit for unsupervised channel learning Given an oracle for the channel one can create multiple copies of the Choi state of the channel for state expo nentiation in the phase kick step of QFB The ansatz for the channel consists of a parametrized unitary on a dilated Hilbert space The goal is for this unitary to approximate the channel on a subset of the input and output registers The feed forward step of QFB involves creating the Choi state of the parametrized channel on this subset of registers The phase kick applies the Choi state of the desired channel as a loss function on the output of the feedforward Of course this is followed by uncomputation The input to the parametrized unitary on the dilation of the input space of the channel is some initial resource state 059 E Quantum ClassicationRegressionMeasurement Learning 1 Overview Classication is the task of associating collections of objects with some set of discrete labels Regression is essentially a similar task but where the labels are con tinuous The present discussion will apply to both cases of discrete and continuous labels hence we will not re strict the discussion to either case and simply denote the labels by a parameter Here we will describe how one can train a quantum algorithm to assign labels to quantum states using a set of training examples Let us denote the set of labelled ex ample quantum states by j j j BH with j denot ing the label for example of index j The set of labels will be denoted A The goal of the classicationregression task is to build a measurement scheme so that upon in put of a state the measurement outcome corresponds to the appropriate label Therefore this task could also be called measurement learning Ideally the labels for the example states are exactly characterized by a POVM with eects E so that tr Ej j j Thus we wish to design a set of quantumparametrized eects E to approximate this assignment of labels Note that if the example states are joint eigenstates of some collection of observables then the problem is essen tially classical since we would simply be assigning labels to elements of the conguration space Also for the cases where all of the example states are pure one may imag ine attempting to build a measurement scheme by using a unitary to map to a xed basis of label states which upon measurement in this basis would provide a label This problem would correspond to learning a PVM However it is clear that this task is simply providing ex act labels to some basis of the Hilbert space which again is essentially a classical labelling task Of course the classical task of learning a PVM will be included here as a special case but here we will focus on the more general case of learning a POVM Naimarks dilation theorem reduces the problem of learning a POVM to learning a unitary and a projective measurement on an extended space The projective mea surements can be the projectors onto the label states and the unitary will be a parametrized algorithm U acting on an extended Hilbert space H H Hc with an initial resource state 0 in Hc For the input j j on H to the parametrized algo rithm a possible choice of loss operator is Lj IH jjHc In the next subsection we discuss various loss function options which share the same optimum The corresponding eective phase on the parameters is Lj trH Ejj j 321 where the parametrized eects E H H are E 0 U U 0 322 Note that this is similar to the result obtained for super vised channel learning The dierence is that here the loss function penalizes the incorrect label states on Hc rather than the incorrect output states on H The setup for this task is illustrated in Figure 25 Figure 25 QFB circuit for quantum measurement learn ing The parametrized ansatz for a POVM consists of a parametrized unitary acting on a dilated Hilbert space fol lowed by a standard basis measurement The loss function is a projector onto the corresponding basis state assigned to a particular label Naimarks theorem ensures that this mea surement can be performed solely on the registers extending the original Hilbert space of the states to be classied 2 Output Encodings Implementation Options There exists multiple ways of encoding the output es pecially when considering both continuousvariable and discrete labels As such there exists multiple options for our choice of loss function which all reward having the correct label but may penalize incorrect labels dierently It is worth discussing these options as some have varying implementation and compilation overheads In all cases if we denote the output label registers as A the negative projector onto the correct label state Lj jjA gives us a valid loss function which clearly has minimal value when the correct label is assigned in each case Let us denote the label state nqubit projector jA n O k1 k j Ak 323 in which j is an nbit string of bit values k j for the kth bit of the jth label and in which we denote the quan tum registers of each qubit in this label as Ak To im plement an exponential of this loss function since the label states jA are computational basis states for an nqubit projector we can implement the exponen tial of this projector by applying Ujei11 U j where Uj N k X k k j is the product of bit ips correspond ing to the bitwisenegated label bit string To implement the exponential of the multi1 state we can use an ad ditional ancilla work register W onto which we apply60 a CnNOT ie and nqubit control generalized Tooli gate which itself can be broken down into a linear num ber of Toolis 55 then apply an exponential of Z on the work register and undo the multicontrolTooli that is CnNOTawei ZwCnNOTaw 0w ei11a 0w Hence we have described how to enact the exponen tial of any given bitstring represented label Now the cost function is a quite sparse in the Hilbert space of the possible bit string states for the label It might then be advantageous in some cases to have a cost function whose representation in the computational basis has a greater support larger rank than a singlestate projector such as to nudge the optimization of parameters even when the output of the network for a given set of parameters is wrong with high probability To do so we can con struct a Hamiltonian whose ground state coincides with that of the correct label For example for the onehot en coded label j N kA jkk we can use the following Hamiltonian for which it is the ground state Lj X kA 1jk Zk 324 One of the draws of this approach is that this loss func tion exponential is easy to synthesize using a product of individual qubit exponentials ei Lj O kA ei1jk Zk 325 which is much easier than synthesizing a singlestate pro jector Additionally the rank of this Hamiltonian is the same as that of dimension of the label space ie of A this can in turn provide a better kickback on the pa rameters being optimized especially for the sectors of the wavefunction which have minimal overlap with the correct label In the case of continuouslabel classication ie re gression we can imagine having each class label be a ten sor product of multiple quditcomputational basis states That is j N kA k j Ak where each component k j is a dary number and the states of each label sub register are qudit states k j ak N jA Xk j 0ak where Xk j are qudit shifts In terms of cost function one option is to use once again the negative projector on the joint label eigen state ie Lj jjA To apply an exponential of this projector we can apply Ujei00 U j where Uj N k Xk j To apply an exponential of the joint null state one could consider using the same trick as outlined above using multicontrolled Toolis but now multiquditcontrolled generalize Toolis If each qudit is made of qubits one can shift the state of each qu dit from 0 to whichever state has all qubits be in their 1 state then use a CNNOT gate with an ancilla work qubit as before where N nlog2 d is the total number of qubits This can be achieved in ON gates Another possible choice of loss function is the mean squared loss where we consider the loss as Lj X kA Ak k2 326 where k is the simulated position operator of the qudit register similar to the j operators of the parameters Note that the state jA is the ground state of this loss Hamiltonian hence optimizing the above will also result in the correct label being output and there is a less sparse error signal since the rank of this loss function spans the whole space of possible labels rather than being rank 1 in the case of the projector In terms of implementation of the exponential loss Ak k2 Xk j 2 Ak Xk j hence a simple way to enact the exponential loss is by applying ei Lj Ujei 2 Ak U j 327 which is similar to the weight decay exponentials de scribed in section III and can be synthesized into a cir cuit of depth Olog2 d2 F Quantum Code Learning In this section we consider how to automate the learn ing of quantum codes for compression and error correc tion In both cases there exists a skew subspace HG of the input Hilbert space H which we would like to isolate into a subset of registers This nonlocal subspace could be the subspace where most of the input space has its support or the logical subspace of a quantum error cor recting code Finding the code transformation which concentrates this nonlocally encoded subspace onto a subset of registers will be the task we will automate with quantum learning To nd a good transformation of the input space we can optimize over a family of paramet ric quantum circuits this can be achieved by imposing cost functions which either maximize the delity of re construction after encoding and decoding or minimize the information leakage to the environment or maximize the delity of the state in the logical subspace We will briey introduce the information theoretic task in each case and outline how to evaluate the cost function and execute the Quantum Feedforward and Phase Backprop agation QFB procedure in each case This leaves all options discussed in section III for optimization over the space of parameters open 1 Quantum Autoencoders Compression Code Learning We rst consider regular quantum autoencoders and later consider the more specialized case of denoising quantum autoencoders The information theoretic task automated by autoen coders is that of compression of a sources signal also61 known as the quantum source coding task 84 in quan tum Shannon theory Consider a quantum source to be akin to a sender of a quantum messages where the sender picks from a set of possible quantum states to send through and the variable representing the decision to send a specic state is modelled by a classical random variable More specically we can consider having a classi cal random variable X with a probability distribution pX x px This classical random variable is an in dex for a certain alphabet of states which we can consider to be either a set of pure states jjX or mixed states jjX Each incoming message can be represented as a classical mixture of states in the alphabet with the clas sical probability distribution being that of the alphabet index P jX pj jj or P jX pj j In general this message will be send using an alphabet made of states of multiple registers whether these be qubits qudits or qumodes The goal of compression is to map these states to a space of fewer qubitsqudits while retaining sucient information so that they can be recovered with high probability The theoretical optimum rate ie number of qubits per message at which we can encode our messages with out loss considering the asymptotic limit of sending many messages is given by the Von Neumann entropy of the mixed state S tr log X spec log 328 and the scheme which achieves this optimal rate is called Schumachers quantum data compression protocol In the following we will outline how one can train a parametrized unitary as an encoder to perform this com pression task Note that for the case of the regular au toencoder we consider the source to be noiseless ie the messages are brought to the network as is When we consider the denoising autoencoder later in this sec tion we will consider adding noise to the input Thus for the regular autoencoder the information theoretic task is akin to noiseless Shannon compression except that in general there is no guarantee to reach the theoretical entropy limit nor to be completely lossless The inputs to the quantum autoencoder will run through the collection of states in the alphabet on the Hilbert space H For simplicity we will denote a general input state to the autoencoder as The autoencoder will consist of a parametrized unitary U acting on H We will factorize the Hilbert space at the output of the unitary into H Hg Ha where Hg is the sector containing the compressed representation of the input state and Ha corresponds to trash registers Before we can discuss appropriate loss functions rst we must determine a means of characterizing the success of an encoder One means of characterizing the success of the encoder is by measuring the delity between the state at the input of the encoder with that of a decoding of the compressed state A decoding scheme would be to input the compressed state along with a reference state into U Explicitly let us introduce a new register Ha to denote the source of the reference state used during the decoding Then an encoding followed by a decoding involves applying U to ga where the subscripts g a have been introduced to specify the appropriate sub systems then applying a swap Saa between Ha and a reference state 0 Ha followed by U acting on Hga Then we can write the decompressed state as ga tra U Saa Uga 00a U Saa U 329 Then the success of the compression is quantied by the delity between ga and ga Fga ga An alternative and for our purposes more convenient means to quantify the quality of the encoding begins with the observation that any information lost during the com pression will manifest itself as entropy in the trash regis ter after the encoding Therefore we can train the algo rithm to minimize the entropy in the trash register or Regular autoencoder Figure 26 QFB circuit for the regular autoencoder In the feedforward step the parametrized unitary acts on the input state to be compressed The loss function is a projector onto a pure resource state on the trash registers at the output of the encoding or Regular autoencoder Figure 27 Reference state exponentiation options The loss function in the QFB circuit is a projector onto a pure resource state 0 The exponentiated projector can be constructed either through a unitary transformation of the exponentiated null projector top or through state exponentiation using multiple copies of the resource state bottom62 Here we will describe two dierent loss functions that may be used for training The rst is based on maxi mizing the delity of the trash register with the refer ence state 0 Ha Therefore in order to enact this we will use the projector onto the pure resource state 0 Ha as the loss function L Ig 00a Of course any other pure state would suce but such a state can be related to 0 via a unitary operator which can be absorbed into U Illustrations of this setup are provided in Figures 26 and 27 The eective phase we obtain for this loss function is L trH00a Uga U F trg Uga U 00a 330 Note that throughout this section for convenience of no tation it should be understood that these delity func tions remain operatorvalued since we have not traced over the Hilbert space of the parameters Thus as de sired the eective phase is the negative delity between the state on the trash registers at the output of the algo rithm and the pure reference state One can relate this delity to the delity of reconstruc tion in the following manner Fga ga F ga tra U Saa Uga 00a U Saa U F Uga U traSaa Uga 00a U Saa F trg Uga U trgaSaa Uga 00a U Saa F trg Uga U 00a 331 Note that in the rst step of the above calculation we can pull the unitary out of the partial trace since U does not act on Ha Then we use the unitary invariance property of the delity ie F U U F U U In the second step we use the monotonicity of the delity under trace preserving operations eg partial trace F F In the last step we used the simple fact that trgaSaa ga 00a Saa 00a Hence we see that the algorithm will train to maxi mize an upper bound to the reconstruction delity Al though maximizing an upper bound does not guarantee that we are maximizing the reconstruction delity max imizing the delity of the trash state relative to a pure reference state will indirectly enforce a maximization of purity of the trash state If we consider the entire com pression procedure as a channel ie the composition of enacting the unitary swapping out the trash state for a fresh copy and acting the reverse of the encoder then enforcing the purity of the trash state will enforce a null entropy leakage to the environment The coherent in formation of this channel 84 will be the maximum over isometric extensions of the input state of the dierence between the entropy of the output of the channel minus the entropy of the environment Ic max SB SE 332 where the maximization over denotes maximization over isometric extensions of the input Thus minimizing the entropy leakage to the environment will necessarily increase our coherent mutual information of our channel As a proxy for this entropy we can use the purity as an alternative loss function for training the quantum au toencoder as it is operationally easier to implement as a cost function However note that one must use the state of the trash register at the output of the encoding along with the compressed state in order to later decom press the state By simply maximizing the purity of the trash registers and not training the register to map to a particular state as before we will not be able to de compress unless we also perform state learning on this trash state Thus we see this means of performing the compression task involves splitting the problem into two tasks encoding and state learning In some cases this may prove to be advantageous instead of enforcing a par ticular ancilla state on Ha and training a possibly more complicated encoder U Here we will proceed to de scribe the training of the encoder and one can use the methods of Section VI C 2 to learn the state of the ancilla In order to accomplish the training using the purity as a loss function one must run two copies of the algorithm in parallel but which can be trained simultaneously by tenting the weights We will show that one can ob tain the purity of the trash state as an eective phase by using a swap gate L Saa as a loss function A means of exponentiating this loss function was described in Section VI B Let us denote the state after the parametrized unitary by ga Uga U and the trash state after the compression as a trg ga and similar for g and a The eective phase we obtain for the param63 eters is L trgagaSaa ga ga traaSaa a a traa2 333 ie the purity of the trash state at the output of the autoencoder The QFB circuit for this version of the autoencoder is shown in Figure 28 Figure 28 QFB circuit for the puritybased autoencoder training We run two copies of the feedforward parametrized unitary U in parallel acting on two copies of the input state Both parallel unitaries are controlled by the same pa rameter registers The phase kick is an exponentiated swap operator applied to the trash registers of the two instances After uncomputation the eective phase on the parameters is the negative purity of the trash registers 2 Denoising Quantum Autoencoder In this section we will examine a task similar to the previous The dierence is that we will assume that the state we wish to compress has rst gone through a some noise channel The goal here will be to train an autoen coder to not only compress but also denoise the state after having passed through the noise channel Ideally the network will learn how to lter noise into the trash registers creating a more robust autoencoder The algorithm will be trained as follows The feed forward will consist of applying both the encoding and decoding maps Ideally we would like this encod ingdecoding process to recover the initial state i ie after compression and denoising it should recover the state at the input of the noise channel Therefore we will apply the projector onto this state as a loss function at the output of the decoder as opposed to the previous case in order to penalize the algorithm if it does not output the correct state As before to employ QFB the uncompute will consist of the inverse of the feedforward In this case this inverse is comprised of the encoding swap with the ancilla register and decoding More concretely let us assume that we have multiple copies of the input states i available for the training as well as an oracle for the noise channel N The rst step for the training is to send a copy of i through the noise channel to obtain Nii We then pro ceed analogously to the previous autoencoder We apply a parametrized ansatz for the encoder U acting on H HgHa where g indexes the Hilbert space contain ing the compressed state and a the trash register This is followed by performing a swap operation Saa between the trash register and a pure resource state 0 Ha Now as opposed to the previous case we rst apply the decoding map U before applying the loss function L iiga on the output The eective phase we obtain on the parameters is L trgaaiiga U Saa UNiiga 00a U Saa U trgagag 00a trg0 ga 0a g 334 where we have denoted the noisy input state after the encoding as ga UNiiga U its par tial trace on a as g traga and the noise less pure state i after the encoding as ga U iiga U Then we see that the eective phase is the HilbertSchmidt inner product between the encoded noiseless input state with the compressed state reduced to the g register along with the pure resource state on the trash register An illustration of this procedure is provided in Fig ure 29 3 Quantum Error Correcting Code Learning The nal parametric coding task we will examine is that of quantum channel coding The scheme considered here will rst involve a parametrized encoding unitary U which acts on the logical sector of the Hilbert space Hl as well as the syndrome Hilbert space Hs64 Figure 29 Denoising autoencoder The input states to the de noising autoencoder are created from feeding the desired state through an oracle to the noise channel The feedforward of the QFB circuit consists of the entire encoding swapping out of the trash registers and decoding circuit The loss function is the input state to the noise channel whose exponentiation requires multiple copies The output of this encoding then passes through a noise channel N We then apply a parametrized recovery map W to the output of the channel along with a set of re fresh qubits 0 Hr This is followed by a decoding map consisting of the inverse of the encoding unitary U The goal is to simultaneously train the parametrized en coding and recovery maps to counteract the noise chan nel Hence we want to train these unitaries so that this full channel is the identity channel on the logical sector Let us denote the full channel by lsr trsr Ad U ls Wlsr Nls Ad Uls 335 where for convenience we have included subscripts to denote the Hilbert space factors that each operator acts on Note that we will use the symbol to encompass the parameters of both the encoder and the recovery map although generically these maps would not share any pa rameters Thus we could decompose U W where U are the parameters for U and W are the pa rameters for W Note that this task is essentially the same as the chan nel learning task we have described before As before we will describe two means of training the channel The rst is analogous to supervised channel learning where the channel is trained on a set of logical input states The second is similar to unsupervised channel learning In order to perform training we must have access to an oracle or an implementation of the noise channel N eg this channel could be learned using the channel learning techniques described above Furthermore to apply the uncomputation of the channel we must dilate the noise map with an auxiliary Hilbert space Hp to a unitary op erator In the following we will not need to refer to this unitary operator explicitly but we will assume access to the dilation for QFB Supervised QEC Learning53 For supervised learning one can simply input dierent logical states of Hl into the channel and train the parameters to learn the identity map In order to simplify the imple mentation of the loss function as we shall see below it will be more convenient to describe the generation of these logical states as acting unitaries Vl on some logical reference state 0 Hl For example if Hl C2k then we could choose 0 0k These unitaries can be chosen from a set which forms a unitary 2design in or der to provide a uniform set of sample data for the input in the logical space Without loss of generality let us also denote the input states on Hs and Hr each as 0 The feedforward for the training algorithm involves acting Vl on the logical input reference state applying the channel lsr and then uncomputing the logical op erator Vl Since we want to train the channel to learn the identity on the logical sector one should choose the loss function to be negative projector onto the logical input reference state L 00l With this one can see that the corresponding eective phase is the delity between the logical state after the channel and the input logical state LV trl00l V l lsr Vl 00lsr V l Vl Flsr Vl 00lsr V l Vl 00l V l 336 where 0lsr 0l 0s 0r As discussed in VI E acting an exponential of a projector can be slightly costly to synthesize into gates A good alternating to the loss function L 00l is to use L P kl Zk This loss function shares the same ground state as the projec tor but is an operator of higher rank hence provides a richer error signal for the quantum parameters when the output is far from correct Synthesizing exponentials of such a loss function is more straightforward ei Lj O kl ei Zk 337 which is much easier than synthesizing a singlestate pro jector Now since we would like to learn the identity chan nel for all Vl we could draw these from a unitary 2 design VjN j1 If this were done in parallel or in a minibatch for each element in the 2design then if the eective phases were combined we would obtain an ef fective phase L 1 N N X j1 LVj 338 which would correspond to the negative average code delity Unsupervised QEC Learning The unsupervised version of learning channel codes is more straightforward Instead of generating various logical states at the input using unitaries Vl here we act the channel on one65 of the subsystems of a maximally entangled state Hl Hl where Hl Hl is an auxiliary copy of the logical space Since we want to train the algorithm to learn the iden tity map on the logical sector the loss function should be L ll One can use state exponentiation or some other means to prepare the exponential of this state The eective phase we obtain for this process is L trllll lsrll 00sr trllI 339 where we have denoted lsrll 00sr as the Choi state of the channel from l to l and I as the Choi state of the identity channel If we write the eective phase in this manner we see that similar to unsupervised channel learning it is the HilbertSchmidt inner product between these two Choi states G Generative Adversarial Quantum Circuits 1 Classical Generative Adversarial Networks Review Generative adversarial networks 11 are a class of net works used in classical machine learning which allows for the generation of new samples from a given dataset hence the name generative by pitting two subnetworks against each other in an adversarial game One of these subnetworks is dubbed the generator while the other is dubbed the discriminator The goal of the generator is to mimic samples from the given dataset while the dis criminator attempts to discern which datapoints came from the generator and which came from the dataset By progressively training both the discriminator and the generator the networks can converge to a Nash equilib rium where the generator is particularly good at gen erating convincing samples mimicking the data and the discriminator particularly good at ltering out uncon vincing samples Let us briey review how to train these classical net works The classical approach is to rst sample a set of random noise variables independently from some simple probability distribution Typical choices for this proba bility distribution would be a Bernoulli distribution for discrete random variables or a Gaussian distribution for continuous random variables These samples rjj are used as random seeds for the generator That is they are the input to the generator network G which outputs a candidate datapoint Grj which is supposed to mimic a datapoint from the actual dataset xjj More precisely the generator is trained so that the distribution of out puts Grjj matches the distribution of the dataset xjj Before considering how the generator should be trained let us discuss how to train the discriminator The discriminator network in the most simple case is taken to be a binary classier a network with a sin gle bit as output To train the discriminator network D rst we sample a single Bernoulli random variable coin ip denoted lj 0 1 Based on the value of lj we feed in to the discriminator either a point from the real dataset xj or a fake datapoint from the genera tor Grj This datapoint real or fake is fed forward through the discriminator and the loss at the classiers output is some function which is minimized when the dat apoint is correctly classied as originating from either the real dataset or the generator There is some exibility in the choice of this loss function and the gradient can be backpropagated through the discriminator network At this stage only the parameters of the discriminator net work are trained and the generator network parameters are held xed This process is repeated for a few data points each time ipping a coin to decide whether the input is from the real or fake datasets and then perform minibatch gradient descent on the classier After a few iterations of gradient descent on the dis criminators parameters we can begin to train the gen erator network Training the generator network involves connecting the output of the generator to the input of the discriminator and then maximizing the error of the discriminator by performing gradient ascent on the pa rameters of the generator network while keeping the pa rameters of the discriminator network xed To summarize we can consider the random bit lj dur ing the discriminator training to be the ground truth label for the real or fake datapoint If we denote the output of the discriminator by oj then we can frame the problem as the discriminator trying to enforce cor relation lj oj 0 while the generator tries to en force anticorrelation lj oj 1 where is the binary exclusiveormodulo 2 addition Phrasing the training in this manner will help formulate the quantum version of the problem which we will now consider 2 Generative Adversarial Quantum Circuits Now that we have reviewed how to train a typical clas sical Generative Adversarial Network GAN we can de scribe how to make a quantum parametric circuit equiva lent of these GANs which we call Generative Adversarial Quantum circuits GAQs Similar to GANs GAQs can be used to generate sam ples from a certain distribution Since we are considering quantum data GAQs will be used to replicate samples from a distribution of quantum states The datasets we consider can be a mixture of pure states or mixed states X j pj i ji j or X j pj j 340 The goal of the generator will be to mimic states that are part of this distribution while the discriminator network66 will attempt to discern the real quantum states from the generated ones The generator network will be a parametric quantum circuit which takes in some quantum randomness as in put and outputs candidate quantum states to mimic samples from the data distribution We will denote this generators parametric circuit as Gg where g are the parameters for the generator The randomness is pro vided in the form of a state rge where G and E are the Hilbert spaces of the input to the generator and the en vironment which is simply the purication of this input Since there generally can be entanglement across the G E bipartition the input to the network will generally be a mixed state r BHg We consider the preparation unitary for the puried state rge Ur 0ge to be dependent on a set of preparation hyperparameters Hence we can append these to our preparation hyper parameters which also includes our pointer state preparation hyperparameters Thus for a given set of parameters g the puried output mixed state of the generator is given by Gg rge Tracing this over the environment E gives us the mixed state of samples gen erated by the generator Ggr Gg which we will feed to the discriminator network The discriminator is simply a binary quantum classi er as treated in Subsection VI E with a parametric cir cuit Dd and corresponding parameters d We write the standard basis Pauli operator of the output regis ter to be Zo As a rst version of the GAQ for training the discriminator we can consider having the the ground truth label for the iteration j to be a classical random bit lj We thus sample a random Bernoulli distribution to determine lj If lj 1 we perform the QFB on the discriminator by feeding it a datapoint quantum state sampled from the dataset jj In the case of lj 0 we feed the state output by the generator to perform QFB In both cases the loss function is Ld j 1lj1 Zo 341 Training the parameters to minimize this loss will move to positively correlate the output of the discriminator with the ground truth label lj The eective phase we get on average for the discriminator parameters assuming an unbiased coin ipped for the truth label lj is Ljd 1 2tr Dd Zo Ddj 1 2tr Dd Zo Dd Ggr Gg 342 Note that the traces in all of the formulas in this subsec tion are understood to be taken over everything except the parameter Hilbert spaces The parameter optimiza tion here is only for the discriminator parameters d which are quantum and are optimized quantum dynam ically The generator parameters g can be kept kept classical or equivalently in an eigenstate of g in the case of MoMGrad or can be kept xed no kinetic pulse in the case of QDD For concreteness if we were to train a few iterations using QDD the unitary to be applied would be Uqdd Y j eij 2 deijLjd 343 where d is the vector of canonical conjugate operators of d To train the generator network we connect the gen erator directly into the discriminator network ie the feedforward unitary becomes U Dd Gg 344 which acts upon the input resource quantum random state rge The parameters of the adversary network discriminator are xed ie we can consider the param eters registers d to be classical or to be in an eigen state of parameter values We can perform the Quantum Feedforward and Phase Kick Baqprop QFB procedure on this joint network with the loss function Lg j Zo 345 which when minimized via either of the quantum param eter descent techniques drives the generators parame ters to fool the discriminator That is the generators weights will be optimized so that for current discrimi nators parameters there is an increased chance for the discriminator to output Zo 1 when fed the output of the generator The eective phase induced on the gener ators parameters via the QFB procedure with this loss function is given by Ljg tr Gg Dd Zo Dd Ggr 346 Thus by alternating the training of discriminator net work and the generative network both networks should reach an adversarial equilibirum 11 and near this equi librium the generator should be able to provide good can didate states to mimic the quantum data distribution An option to simplify the number of steps involved in the training algorithm and to train both networks simul taneously is to use a qubit for the ground truth label With this we will be able to make the entire algorithm fully coherent and we can use the same loss function for both networks except that the discriminator will be trained to descend the loss function landscape while the generator will be trained to ascend This setup which we will now proceed to describe is illustrated in Figure 30 We rst replace the Bernoulli random variable lj rep resenting the ground truth label for iteration j with a qubit beginning in a state of uniform superposition of two label values ie l 1 20l 1l Now we keep the generator and quantum randomness seed the same but the input to the discriminator will be swapped in based on the computational value of the label qubit That is we use the label qubit as the control for67 both a Fredkin controlledSWAP gate and a negated Fredkin gate ie a Fredkin gate conjugated by qubit ips Xl as depicted in Figure 30 The rst of these will be used to swap in a sample from the real data set j in the branch of the superposition of the label qubit corresponding to 1l The second will swap in the output of the generator network which along with the quantum randomness seed will remain the same as in the non coherent version of the network With this setup we can phase kick the entire network at the output of the discriminator network with the loss function Lj Zl Zo 347 where Zl is the PauliZ operator for the label qubit and Zo is the PauliZ operator for the output classier of the discriminator circuit Thus this cost function is mini mized when the output of the discriminator is positively correlated with the ground truth label Hence the train ing of the discriminator will aim to minimize this loss whereas the training of the generator will aim to maxi mize it First let us consider the eective phase which can be shown to recover the formula from before Lj 1 2tr Dd Zo Ddj 1 2tr Dd Zo Dd Ggr Gg 348 For training the parameters of the discriminator one can employ either MoMGrad or QDD as in other situations we have considered For the parameters of the generator since we want to ascend the average landscape of this ef fective phase we can act a squared Fourier transform on the parameters of the generator denoted F 2 g before and after the kinetic term of QDD or before the measure ment of the parameters in the case of MoMGrad The squared Fourier transform acts eectively as a NOT gate on the parameter registers and hence will act to update the parameters in the opposite direction as we have seen before Concretely if we were to perform QDD for this network we would enact Uqdd Y jM F 2 g eij 2 F 2 g eijLj 349 where 2 2 g 2 d is the kinetic term for all regis ters while F 2 g is the squared Fourier transform in each of the generators registers Thus negating the phase kick eectively forces the generator network to ascend the cost function rather than descend which will drive the generator network to anticorrelate the output of the discriminator with the ground truth Note that in prac tice although we used the same kicking and kinetic rates for both the generators and discrimnators parameters in 349 it might be best to use dierent rates for both networks as attaining the adversarial equilibrium may re quire some hyperparameter netuning Generative adversarial circuits Figure 30 QFB procedure for the fully coherent Generative Adversarial Quantum circuit The input to the parametercontrolled discriminator Dd is swapped in from either the real data set with sample j or from the output of the generator network Gg The determination of which sample to swap in is controlled by a label qubit in the state The sample j is connected to the discriminator input via a controlledSWAP CSWAP gate with the label qubit as the control The output of the generator network is connected to the discriminator input via a negated CSWAP gate the negation is depicted by a white circle in the control register The phase kick is applied with the loss function acting on the output of the discriminator and the label qubit The phase kick is followed by an uncomputation of the entire feedforward circuit as prescribed by the QFB procedure68 H Parametric Hamiltonian Optimization Parametric Hamiltonian optimization algorithms con sist of a broad class of algorithms 29 39 where the goal is to optimize over a parametrized hypothesis class of states in order to minimize the expectation value of a certain Hamiltonian H That is if we denote the parametrized class of states as then we want to nd argmin H 350 Such algorithms includes the Variational Quantum Eigensolver VQE 29 which is used to nd ap proximate eigenstates of noncommuting Hamiltonians in chemistry the Quantum Approximate Optimization Algorithm QAOA 39 which is used for quantum enhanced optimization as well as other parametric cir cuit ansatze like the Deep Multiscale Entanglement Renormalization Ansatz DMERA 85 which is a hierarchicallystructured ansatz which allows for sam pling statistics of local observables in stronglycorrelated quantum systems Such an optimization problem ts very naturally within the framework introduced in this paper In our case we consider the optimization over the hypothesis class of states as the task of optimizing of a class of quan tum parametric circuits acting upon a reference state U 0 Then we can simply use the Hamil tonian as the loss function we wish to minimize L H 351 The main challenge with the implementation of a general Hamiltonian as a loss function is to construct its expo nentiation ie enacting the operator ei L ei H 352 For a Hamiltonian which is a sum of various terms of index X H X jX Hj 353 the task of exponentiating such a Hamiltonian is the same as that of quantum simulation of the time evolution gen erated by this Hamiltonian 86 This is a task for which there is much literature as it is a core concept of quantum computing 55 There exist many techniques to approx imate such an exponential and for a given desired oper ator norm error the overhead will depend on the locality and operator norms of the Hamiltonian terms 86 A the oretically simple approach is the SuzukiTrotter method which is a divideandconquer method where each term is exponentiated independently ei H Y jX ei HjMM 354 The operator norm error in this approximation 86 87 is 2 P jkX Hj Hk2M O3 Therefore as long as we choose M P jkX Hj Hk we have an error of order O2 Now for the QFB procedure if we begin in a reference state 0 apply the parametric unitary U apply a quantum simulated exponential of H with error fol lowed by an uncomputation of the parametric unitary then we arrive at the eective phase kick on the param eters generated by L 0 U H U 0 355 up to an error of order O Recall that in general the eective phase kick is only accurate to rst order in ie it has an error of order O2 Hence a rst order SuzukiTrotter formula as in equation 354 should suce The circuit to implement the Quantum Feedforward and Baqprop QFB on a single QPU is simple pictured in Figure 31 Note that the implementation of the ex ponential of the Hamiltonian can come with large depth overhead thus it may be convenient to have a method with higher space overhead but with lower depth ie a way to parallelize the accumulation of the gradient over the terms in the Hamiltonian We discuss this in the next subsubsection Figure 31 QFB circuit for Parametric Hamiltonian Optimiza tion in the case of a single QPU The circuit simply consists of a feedforward of the parametric unitary acting on a reference state followed by the simulated Hamiltonian exponentiation as a phase kick and the uncomputation 1 HamiltonianParallelized Gradient Accumulation Here we discuss various methods to parallelize the ac cumulation of phase kicks and gradients in order to re duce circuit depth time overhead at the cost of higher space overhead Notice that since the eective phase from above is an expectation value it is linear and can be split up over the terms of the Hamiltonian L 0 U H U 0 X jX 0 U Hj U 0 356 This trick is the fundamental principle behind the Quan tum Expectation Estimation algorithm 48 which paral69 lelizes the expectation values of each term in the Hamilto nian over dierent QPUsruns Recall that in our case we are looking to obtain a gradient of these eective phases which are expectation values Since the gradient is a linear operator we can accumulate the gradient of the sum by the sum of the gradients Operationally by using multiple sets of parameter reg isters jjX and dividing up the terms in the Hamil tonian into individual loss functions over dierent QPUs we can classically parallelize the accumulation of gra dients ie using classical addition we can sum up the gradient contribution of each term We call this ap proach Gradient Expectation Estimation Parallelization GEEP which is technique mostly relevant to Momen tum Measurement Gradient Descent MoMGrad since the gradient has to be measured to be stored as classi cal information Mathematically by acting a QFB with a loss Lj Hj on each replica we get the following eective QFB phase on replica j Ljj 0 Ljj 0 0 U j Hj Uj 0 357 Thus to rst order in the eective phase kick is eiLjj on each of the parameter sets The corre sponding shift in momenta of each set of parameters is j 7 eiLjj jeiLjj O2 j Ljj j O2 358 Therefore by preparing identical momentum pointer states cenetered at zero momentum in each of the pa rameter registers of the replicas ie 0X for some pointer state 0 and by classically summing up the expectation values of the momenta in each replica we have X jX jj X jX j0 Ljj j 0j O2 0 X jX Lj 0 O2 0 P jX Lj 0 O2 0 L 0 O2 359 Thus we see that by classically adding up the expectation values of the momenta in each replica we get the gradient of the total loss function as if it were applied on a single replica We present the quantumclassical circuit for this GEEP procedure with MoMGrad in Figure 32 To apply a similar parallelization which is applicable to Quantum Dynamical Descent the accumulation of mo menta must be done coherently For this purpose we can Figure 32 MoMGrad GEEP Momentum Measurement Gradient Descent iteration via Gradient Expectation Estima tion Parallelization The parameter pointer states in each replica are prepared using a unitary Up The QFB circuit is applied in each replica with each parametrized unitary con trolled by the corresponding replica parameters and the phase kick generated by the corresponding term in the Hamiltonian The shift in the momenta of the parameters in each replica are measured and after many runs the expectation values are classically added to obtain an averaged gradient of the total loss function use the technique of Coherent Accumulation of Momenta Parallelization CAMP introduced in Section IV A 3 The point is that we can consider the dierent terms in the Hamiltonian to be analogous to datapoints in a batch whose loss functions are exponentiated and coherently accumulated to attain a total loss function comprised of the sum of losses of each term Once again denoting the loss function for each Hamiltonian term as Lj Hj and the associated eective phase for each replica as in 357 we can apply the following unitary for parallelized Hamiltonian Quantum Dynamical Descent Upqdd Y k eik 2 0U tent O j eikLjj Utent Y k eik 2 0 Ucampk 360 where k is an index for the iterations Recall that the TENT unitary is simply a multitarget adder gate as dened in equation 124 Also note that in the above equation 360 the phase kicking rate is k in each replica whereas previously in 129 it is normalized by the mini batch size Finally this unitary is applied on an initial state where the parameter server replica of index 0 with parameters 0 is in a pointer state of choice and the replicas are initialized in a nullparameter eigenstate ie 00 O j 0j 361 We represent the circuit for an iteration of Quantum Dynamical Descent with Coherent Accumulation of Mo menta Parallelization for the Hamiltonian Optimization task in Figure 3370 Figure 33 QDD CAMP for Parametric Hamiltonian Op timization The parameter server is initialized to a pointer state using Up and this is distributed to the replicas us ing the TENT operation The QFB circuit is applied to each replica with each parametrized unitary controlled by the cor responding replica parameters and the phase kick generated by the corresponding term in the Hamiltonian The inverse TENT operation is applied after the QFB circuits to accumu late the phase kicks in the parameter server as a phase kick according to the total loss function The standard QDD ki netic pulse is then applied to the parameter server at the end of the iteration I Hybrid Quantum NeuralCircuit Networks The method for regressionclassication using quan tum parametric circuits outlined in Subsection VI E is eectively a method for learning a quantumtoclassical map One could then imagine having a classical neu ral network taking in this signal to perform some further processing More generally one may wish to perform fur ther processing on the outcomes of the measurement of some observable at the output of a general parametrized quantum circuit Further the methods we describe here will in principle also apply to a general parametrized classical circuit although we will focus on neural net works for concreteness The challenge examined here is to eciently train both the quantum and classical parts of the hybrid network in unison In this section we focus on methods to backpropagate error signals through such a quantumclassical interface ie how to train networks which are hybrids of quan tum parametric circuits connected to classical neural net works We consider two cases rst is to embed the clas sical neural network into a quantum computation ie both parts for the network are trained on a Quantum Processing Unit QPU In the second case we have the classical neural net work being trained on a Classical Processing Unit CPU which is connected to a QPU on which the Quantum Parametric Circuit is being trained We propose two methods for simultaneous quantumclassical training in both cases we propose a way to backpropagate the error signal through the quantumclassical boundary The rst of these latter methods depends only on ex pectation values of the observables of the quantum out put registers which are used as input activations for the neural network Using classical feedforward and back propagation of gradients we can approximate the error signal as a linear potential centered around this expec tation value and enact a linear relative phase kick on the quantum system momentum kick to convert this approximate error signal back to quantum The second method follows a similar philosophy but allows for a more nontrivial error signal tomography hence a higherorder approximation to the error signal The approach relies on sampling various measurement results from the output of the parametric circuit and feeding these through the classical neural network For each sample point a gradient is obtained through classi cal feedforward and backpropagation For sample points that are relatively close to each other low variance of output from quantum regression net a higher order in terpolation of the eective backpropagated cost function can be obtained This can then be applied as a higher order phase kick on the quantum network which can then be leveraged by the usual quantum phase kick backprop agation method for MoMGrad 1 Fully Coherent Hybrid Networks To begin let us examine the case where both the quantumparametrized circuit QPC and the classical neural network NN are trained on a QPU This setup simply involves connecting one of the neural nets from Section V to the output of the QPC The presence of both the QPC and NN on the quantum chip allows one to use QFB in a straightforward manner Of course once the QPC and NN have been trained on the QPU there is an option to do inference with the NN on a CPU Although in essence using QFB in this situation is similar to before it will be worth describing explicitly in order to compare to the subsequent cases Let us write Uqpc as the quantumparametric circuit where are the parameters of the circuit We will also write y f x as the prediction at the output of the classical parametric circuit ie the NN with parameters and input x Once embedded in a quantum chip the input x and parameters are quantum As in Section V the cir cuit for the feedforward in the NN is Uff eifxpy where y denotes the output register of the network pre diction and py its conjugate momentum Recall also that the QFB circuit for the NN is ei Lyfxy The full QFB for the QPC and NN involves the feedforward of the QPC followed in turn by the feedforward of the NN phase kick for the output of the NN backpropagation for the NN and nally backpropagation for the QPC We will nd it illustrative to absorb the middle three steps as simply the QFB circuit for the NN alone U qpcei Lyfxy Uqpc 36271 In this way we can abstract away the entire NN so that insofar as the QPC is concerned the QFB of the NN ap pears as a phase kick on the momentum of the observable x at the output of the QPC In the following we will discuss cases where the NN is implemented on a CPU so that are classical NN pa rameters The tasks that remain are to nd an appropri ate replacement for the input to the classical network x determined from the output of the quantumparametric circuit as well as a means of employing the notion of backpropagation at the quantumclassical interface 2 Hybrid QuantumClassical Networks To obtain a classical number from the output of the QPC one has to perform a measurement In the case of classication or regression this could correspond to a generalized measurement However a generalized mea surement can always be seen as a projective measurement on a larger system Thus without loss of generality we will dene an observable z at the output of the QPC as an operator whose spectrum consists of some encoding of the measurement outcomes After declaring such an observable we need to decide how to use outcomes of measurements of z to feed into the classical circuit That is if we again write the output of the classical circuit as f x then we will describe some choices of maps from measurements of z to values of x For example the rst map we will examine is the expectation value x z The second map we will dis cuss will be able to accommodate some variance in the variable z However simultaneously training a quantum circuit combined with a classical network will typically only work well if the uncertainty in the QPC parame ters is low ie when their distributions are highly concentrated close to their expectation values Once we have chosen such a map we can feedforward the input through the network and perform classical backpropagation to obtain f xx In the previ ous case where the QPC and NN were both placed on the QPU we saw that QFB involved feeding forward the QPC and applying the phase kick ei Lfzy 363 followed by uncomputing the QPC Note that in this for mula we have removed y since we will assume that the register for y is initialized to zero and none of the other circuit elements act on the output register of the NN embedded in the QPU Below we will discuss analogues of the QFB for the NN constructed from the gradients f xx obtained form the classical backpropaga tion in order to obtain a means of propagating the error from the classical network as a phase kick on the output of the quantum circuit Firstorder method The simplest means of map ping the observable z at the output of the QPC to a classical input is to assign x z tr U z U0 Note the trace is taken over the computational Hilbert space as well as the Hilbert space of the parameters In practice this expectation value is obtained from mea suring z over multiple runs of the QPC With this assignment to x one can feedforward the input to obtain the output of the network f x and backpropagate the loss function to obtain a gra dient of the loss function with respect to the input Lf x yxxz This gradient can be used to approximate the QPU version of the phase when the vari ance of z is small since we can then write Lf z y Lf z y z z Lf x y x xz 364 Note that when exponentiated the cnumber terms in this expression simply give global phases to the wave function Therefore the phase kick we should apply at the output of the QPC to backpropagate the error of the classical network consists of a linear phase shift In summary once we have the backpropagation of the clas sical network to the input we can write the QFB for the quantumparametric circuit as U qpceiz Lfxyxx z Uqpc 365 As the gradients for the classical network have already been backpropagated the classical part of the network can simply be trained using these classically backprop agated gradients using gradient descent or any other choice classical gradientbased based optimizer 61 62 This method requires relatively lowdepth circuits and only depends on easilymeasured expectation values Al gorithms of lowdepth which depend on simple expecta tion values have shown to be suciently robust to noise for successful implementation on nearterm devices 46 as such we expect that this algorithm should be imple mentable on nearterm devices An illustration of this rstorder method for training hybrid quantumclassical networks is shown in Figure 34 Higherorder method Instead of inputting the ex pectation value z into the classical network here we will input a sample for the outcome of a measurement of z ie we draw a sample point z from the distribu tion pz trzz U0 U Now if we perform backpropagation on the classical network back to the in put we obtain a gradient gz Lf x y x xz 366 We can repeat this for multiple samples z i N i1 in order to collect multiple gradients gz i N i1 Now the idea is to use this collection of gradients in an interpola tion scheme to obtain an approximation to the quantum72 Figure 34 Concurrent training of a hybrid quantumclassical network using a rstorder method The upperleft diagram shows the feedforward of a parametric quantum classier upon input state j and auxiliary reference state 0 Measure ments are performed for multiple runs of the feedforward on input j to obtain the expectation value zj This expec tation value is fed into the classical neural network top right where classical feedforward and backpropagation is performed to obtain the gradient of the loss function at the output of the classical network with respect to the input gj xLf x yjxxj The gradient gj is then used to employ a phase kick expigj z in the MoMGrad pro cedure for the parametric quantum circuit on the QPU bot tom phase kick Gz gz i N i1 Lf z y 367 For example if the sample points happen to be near one another one could try to reconstruct a secondorder Tay lor approximation to the function L Otherwise if the sample points are too far apart one could use a dier ent interpolation scheme After making a choice for the function G one can write the QFB circuit for the QPC as U qpceiGzgz i N i1 Uqpc 368 VII NUMERICAL EXPERIMENTS In this section we demonstrate the capabilities of the heuristics proposed in sections III by implementing these methods to optimize various quantum neural networks and quantum parametric circuits We compare the per formance of Quantum Dynamical Descent QDD ver sus Momentum Measurement Gradient Descent MoM Grad We begin with the training of a classical deep neural networks on a quantum computer to demonstrate how the algorithm performs for classical computation embedded in quantum computation Following this we show how QDD and MoMGrad can leveraged to enhance quantum Hamiltonian optimization algorithms we use the Quantum Alternating Operator Ansatz parametric circuit as our example To show how the heuristics deal with loss operators that are not Hamiltonianbased but rather statebased we show how one can perform gradi ent ascent on the delity in order to learn a parametric circuit approximating a unitary Finally to show how Quantum Phase Backpropagation interfaces with classi cal backprop we demonstrate the training of a hybrid network on a simulated quantum computer running a quantum parametric circuit connected to a classical neu ral network running on a classical processor All the experiments featured in this section were clas sical numerical simulations of quantum computation which were on the Rigetti Forest Quantum Virtual Ma chine with code written in PyQuil 88 A Quantum Neural Deep Learning In this subsection we train a classical deep neural net work y embedding it into a quantum computation in or der to leverage MoMGrad and QDD In order to demonstrate the capabilities of the quan tum descent algorithms of section III to train a deep neu ral network we chose a problem which is a nonlinearly separable classication task Due to being one of the most elementary canonical counterexample to the learn ing capabilities of singlelayer networks we chose the task of learning the exclusiveor XOR Boolean function us ing a 2layered perceptron network Learning a set of optimal parameters which minimize the loss for this clas sication task counts as deep learning since it requires a neural network of depth at least 2 1 Application Methods Recall the XOR function denoted takes binary pairs of bits and maps them b1 b2 Z2 Z2 and maps them to a single binary value corresponding to their ad dition modulo 2 b1 b2 b1 b2mod 2 In order to learn this function we then have to use a neural network which has 2 input units and 1 output unit The particular network chosen for the implementation in this paper is pictured in gure 35 This network has an input layer a single hidden layer and one neuron constituting the output layer In order to encode this network on the quantum sim ulator we use nitedimensional qudits for each neuron weight and bias In terms of notation we denote the qudit standard basis position operators of the neurons as aj for the jth neuron of the th layer W is the matrix of operators corresponding to the weight param eters for the th layer and b is the vector of operators73 corresponding to the bias parameters for the th layers neurons For simplicity we use a simulated Rectied Linear Unit RELU activation function as it is standard in modern classical deep learning Instead of using separate qudit registers for the input acumulation of the neuron and the activation value of this neurons input we perform the activation insitu by using a modied position operator projected onto its positive values as the generator of shifts see gure 36 Figure 35 Neural network used for learning the XOR func tion Input neurons and output layers neurons are black hidden layer neurons are white while the quantum weights and biases are represented with white dots Both the hidden layer and output layer have biases Let us describe more explicitly the circuit that was applied in order to train the network from gure 35 The QFB circuit which was applied is represented in gure 36 The input and output data registers were kept as classical controls in order to save memory space for the simulation The parametric unitary for the rst layer feedforward was U 1 W1b1 Y jk01 eixmj W1jk pa1k eib1k pa1k 369 where here the xm Z2 2 are the possible input data points This is simply the addition of the weight values conditioned on the input bit values and the addition of the bias values onto the second layers neurons The feedforward operation for the following layer is given by U 2 W2b2 Y j01 eis1j W2j pa2 eib2 pa2 370 where s is a neurons activation value which is the quadrature value projected onto the positive values s1j P a1j P P X a0 aa 371 which is an operator which assigns the ReLU eigenvalue to the neurons input eectively s1j a1j x x x 0 0 x 0 372 To synthesize this operation an ancilla qubit would nor mally be necessary the above option was implemented in order to reduce the eective dimension of the Hilbert space and reduce memory overhead during simulation Now after the feedforward of both layers has been ap plied we apply a phase kick according to the following cost function Pa2 yj Ia22 Pa2 yj Ia2 2yj Pa2 373 where yj Z2 is the classical data bit desired output The above cost function forces the output activation to be positive to indicate a value 1 versus being nonpositive for the output 0 in other words the above loss foces the network to encode the XOR value of the inputs into the eigenvalue of the obersvable P which is the projector onto the positive value qudit states of the output One could consider the P as a step function activation oper ator for the output For the full Quantum Feedforward and Baqprop circuit that was applied see gure 36 2 Implementation Results In this section we present neural network training re sults from leveraging Momentum Measurement Gradi ent Descent MoMGrad and Quantum Dynamical De scent QDD to train the neural network from gure 35 to learn the classical XOR function We use the Quan tum Feedforward and Baqprop circuit presented in gure 36 in order to query the eective phase on the parame ters for the cost function The parameters neurons and bias registers were all chosen to be qudits of dimension 7 in the simulation The parameters to be optimized via MoMGradQDD are the the weights and biases in the notation of section III W b In gure 37 we show the crossentropy Kullback Leibler divergence 1 between the desired output bit value and the value obtained through the feedforward We consider any output of postive eigenvalue of the out puts position quadrature as a 1 and any nonpositive value as 0 eectively like a step function activation In both the QDD and MoMGrad cases we begin with Gaussian wavefunctions for the quantum parameters In terms of hyperparameters the initial means of the Gaussian wavefunctions the components of 0 were sampled randomly from a classical Gaussian distribution of mean 0 and standard deviation 05 while all the mo menta hyperparameters 0 were initialized at 0 In the case of training via QDD the initial standard deviation of the Gaussian wavefunction was chosen to be 0 1 for all parameters in the case of QDD the kicking rate was kept at a constant j 05 j and the kinetic rate for iteration j was adjusted as j 05 01j5 For the MoMGrad case this standard deviation of the Gaussian pointer state was adjusted at each iteration as j 0 095j for the jth iteration the kicking rate held constant at j 05 and kinetic rate held at j 1 for all iterations74 Figure 36 Quantum Feedforward and Baqprop circuit for the Neural Network tasked with learning the XOR function Refer to gure 35 for a schematic of the neural network ar chitecture The data are inputoutput pairs xj yj where xj Z2 2 and yj xj0 xj1 Z2 The solid lines are qu dits simulated qumodes while classical registers are classical bits The legend for the diagram is boxed below the circuit in which c Z2 represents an arbitrary bit P P x0 xx is the projector onto the qudits positiveposition states and s P x P is the position operator projected onto the posi tive states akin to a RELU operator Note the controlled shifts with a are the Hermitian conjugate of their respec tive counterpart with a since they serve to uncompute the feedforward orperations The loss function is the squared dierence between the desired bit value and the truth value whether the output activations is positive of not we thus read out any output activation of positive value as 1 and any of negative value as 0 In gure 38 we show the decision boundary when con sidering a continuous input as opposed to simply bi nary For a given continuously valued input in the range 05 15 05 15 we show the domain where the output is postitive hence would be decided correspond ing to an output 1 versus where the output is negative We see that there is a striped domain characteristic of the nonlinear separability of this domain The QDD seems to have a tighter interval around the two points of XOR value 0 Due to our choice of cost function and due to feeding only binary data points there was no incentive for the network to nd an optimal hyperplane separating the inputs into the 0 and 1 classes For the desired do main the neural network was trained for binary inputs and output the network performs the correct classica tion 0 5 10 15 20 Epoch 00 05 10 15 Cross Entropy MoMGrad QDD Figure 37 Plot of the cross entropy between the neural net works output and the XOR of the input at various iterations The above is the average loss for 3 separate runs at each it eration index for both training via MoMGrad and QDD 0 0 1 1 0 0 1 1 a b Class 0 Class 1 Cl Cl 0 01 02 03 04 05 06 PrC 3 Figure 38 XOR Quantum neural network learning decision boundary obtained from numerical quantum simulations left a is via MoMGrad right b is via QDD The decision boundary was obtained by feeding a continuum of values in the input qudits and observing the value of the output For a positivevalued output the corresponding decision is 1 whereas a nonpositive output is considered as 0 We see that both the QDD and MoMGrad correctly classied the output of the XOR B Quantum Parametric Hamiltonian Optimization As mentioned in section VI H there exists multiple possible applications of Parametric Hamiltonian Opti mization we could chose to implement We choose to focus our numerical experiments on the QAOA since we have established in section IV D that the metalearning problem is technically a QAOAclass problem Given the large overheads of simulation of many parameters on a classical computer testing the metalearning directly for an interesting problem size would be intractable hence by simply showing that our quantumenhanced parame ter optimization methods work for an instance of QAOA we can thereby verify that it would work for a meta learning problem In terms of specic QAOA implementation we look at75 the canonical application of QAOA ie applied to the optimization problem corresponding to nding the Max imum Cut MaxCut of a graph 39 We briey review this application below before showing our results for en hancing this optimization algorithm using MoMGrad and QDD 1 Application Methods Consider a graph G V E of vertices V and edges E A cut set C E is dened as a set of edges which partitions the set of vertices in two The maximum cut is the largest such subset of edges We can consider the following Hamiltonian Hc X jkE 1 2I Zj Zk 374 where each vertex in j V is assigned a qubit with 0j or 1j representing whether a given vertex is in partition 0 or partition 1 Each edge j k E is associated a coupling of the form 1 2I Zj Zk which is an operator of eigenvalue 1 if the both vertices of the edge are of dierent partitions or of eigenvalue 0 if they are in the same partition Thus nding the computational basis state b which is the maximal eigenvalue eigenstate of the Hamiltonian 374 would be equivalent to nding the bitstring b of partition labels for each vertex b bjjV bj Z2j which represents the MaxCut set In order to nd this optimal state we can apply the Quantum Approximate Optimization Algorithm with Hc from 374 as the cost Hamiltonian and Hm X jV Xj 375 as the mixer Hamiltonian The parametric circuit to be applied for the QAOA is then given by U P Y j1 ei2j Hmei2j1 Hc 376 where P is the number of alternating exponential steps and the loss function to be minimized is L Hc We can use MomGrad or QDD to optimize this parametric circuit in order to minimize the loss function maximize the Hamiltonian The canonical choice of initial state onto which one applies the above parametric circuit is the superposition of all bitsrings 0 O jV 377 In general after applying the QAOA parametric circuit for some choice of parameters deemed suciently optimal the nal state should have a certain probablity of being in the MaxCut state or at least a probability of having states with a cut size close to this MaxCut For our particular implementation of QAOA we apply it to nd the MaxCut of the graph depicted in gure 39 which has a maximum cut of size 5 In gure 41 we plot the probability of measuring a state which has a cut size of 4 or more for the expected parameters at various iterations of the optimization We see that the probability of obtaining a nearoptimal cut becomes high PrC 4 08 where C is the eigenvalue of Hc for the measured bit string as the training progresses a sign that the approximate optimization is working For this particular implementation we chose a circuit with P 2 hence with only 4 parameters to be optimized which we depict in gure 40 0 1 2 4 5 3 0 1 3 Figure 39 Graph G V E of V 6 vertices and E 5 edges for which we would like to leverage the QAOA in order to nd the maximum cut The maximum cut is represented on the right with the partition index being represented by the vertex coloring with either black 0 or white 1 Note the MaxCut set has cardinality 5 hence any cut set C for this graph has C 5 2 Implementation Results In this subsection we present training results for the optimization of the QAOA parametric circuit using both MoMGrad and QDD This parametric circuit consists of a P 2 QAOA ansatz depicted in gure 40 with cost Hamiltonian from 374 and mixer Hamiltonian from 375 for the graph depicted in gure 39 Figure 40 QAOA P 2 parametric circuit from equation 376 which was optimized for the results displayed in gure 41 The cost and mixer Hamiltonians are those from equations 374 and 375 for the graph depicted in gure 39 In gure 41 we represent the probability of obtain ing a nearoptimal cut over the training iterations for MoMGrad QDD and a quantumclassical NelderMead76 method 89 for comparison For this implementation the parameters were simulated qudits of dimension d 7 For the hyperparameters the kicking rate for both QAOA and MoMGrad cases was kept at j 035 j The kinetic rate for QDD and MoMGrad were updated as j 098j4 The initial wavefunction for both QDD and MoMGrad was a Gaussian of 0 1 for all pa rameters with a mean 0 with each component sampled from an independent classical Gaussian distribution of standard deviation 05 and mean 0 For MoMGrad the subsequent standard deviation j 0 098j for all com ponents for the jth iteration 0 25 50 75 100 125 150 Iteration 02 04 06 08 PrC 4 Classical NelderMead MoMGrad QDD Figure 41 Training results for optimizing the QAOA circuit from gure 40 via MoMGrad and QDD Displayed is the prob ability of measuring a bitstring which corresponds to a cut set C of size 4 C 5 near the optimum of 5 which is the Max Cut set size Additionally plotted above for comparison is a NelderMead optimized QAOA which converges slower than QDD and MoMGrad We see that all 3 optimizers converge to a probability PrC 4 08 C Quantum Unitary Learning In this subsection we demonstrate the implementa tion of quantum supervised unitary learning see section VI D 1 for more details on this task 1 Application Methods The task of supervised unitary learning as described in section VI D 1 is the following given a set of input output pairs i j o j which are related by a uni tary mapping o j V i j nd a parameteric unitary ansatz U and suciently optimal parameters such that o j U i j so as to generally approximate the unitary U V which should hold ideally for inputoutput pairs which lie outside the given dataset For the implementation in this paper we consider a fairly simple case of learning a random singlequbit uni tary Using a uniform measure on the unit sphere we can sample random points on the Bloch sphere and gen erate uniformly random singlequbit pure states The input states j are thus generated by sampling from the Bloch sphere As for the unitary V to be learned we rst sample a random state on the Bloch sphere call it V Then we dene the unitary V to be learned as the unitary such that V V 0 where 0 is the com putational basis null state of the qubit The parametric ansatz we use is represented in gure 42 it is a sequence of parametric rotations about the x y and z axes of the Bloch sphere in that order Figure 42 Parametric circuit ansatz applied in this imple mentation of supervised unitary learning The rotations are about the x y and z axes of the Bloch sphere in that order 2 Implementation Results Here we describe the details of the implementation of the learning of random singlequbit unitaries via the use of both QDD and MomGrad for the parametric ansatz presented in gure 42 The qudit dimension of the simu lated quantum parameters was d 7 Note that in order to apply the phase kick according the output state projector loss function eio jo j we implement the exponential of these states directly in the numerics rather than with the quantum state exponenti ation tricks described in VI B This was done to minimize the classical memory overhead of simulation The phase kicks were applied in a sequential mini batches see sec IV A of size 10 Now for the hyperparameters chosen for the training For both the QDD and MoMGrad training the kicking rates were kept at 02 for all iterations For both QDD and MoMGrad the initial Gaussian wavefunction over parameters was chosen to have standard deviation 0 09 for all parameters and the initial means P hi0 which were sampled from a normal distribution of null mean and standard deviation 05 For QDD the kinetic rate for iteration j was 02 098j Featured in 43 are the results of the average delity throughout the training averaged over 5 dierent optimization runs with dierent random unitaries to be learned in each case77 0 10 20 30 40 50 60 70 Iteration 06 07 08 09 10 Fidelity MoMGrad QDD Figure 43 Training results for single qubit random unitary learning problem Shown is the average delity between the states generated by applying the true unitary and the current best estimate to the parametric unitary averaged over 5 runs Each run had a dierent random unitary to be learned Both QDD and MoMGrad converge to 9975 delity D Hybrid NeuralCircuit Learning In this section we numerically implement the train ing of a hybrid quantumclassical neuralcircuit hybrid network as described in section VI I 2 That is we con sider having a quantum parametric circuit whose output is connected to a classical neural network 1 Application Methods In this particular implementation we consider asking the hybrid network to learn to readout the momentum eigenvalue of a input momentum eigenstate ie a com putational basis state in the canonical dual Fourier ba sis This can be seen as a hybrid quantum state classi cation task For this particular implementation we look at learning the quantum Fourier transform on 3 qubits Mathematically we prepare a set of states j F j012jZ8 where j N2 k0 jk is the binary repre sentation of the computational basis state of eigenvalue j P2 k0 jk2k F is the 3qubit Quantum Fourier trans form The network is fed quantum states along with their corresponding desired label j j The network is tasked to learn how to correctly classify these states according to their label The task is then eectively to learn decode the eigenvalue of the operator F J F where J X jZ8 j jj 2 X k0 2k1Ik Zk 378 We decompose this task of decoding the spectrum of this operator in such a way to force cooperation between the classical processing unit and the quantum process ing unit in order to obtain correct classication The learning is hybrid as the quantum parametric circuit has to learn the inverse Fourier transform F gate decom position while the classical network learns the correct weighted combination of the readouts from the dierent qubit registers Both networks must be optimized in a joint fashion in order for the composite quantumclassical mapping to guess the correct scalar corresponding to the eigenvalue of F J F for each possible input state The classical network must learn the ane transfor mation which converts vectors of expectation values as z Z0 Z1 Z2 7 P2 k0 2k11 Zk y Meanwhile the quantum parametric circuit must learn the canonical decomposition of the 3qubit inverse QFT To restrict the number of quantum parameters needed to simulate the learning of the inverse Fourier transform we only parametrize the controlledRz rotations of this decomposition the parametric circuit ansatz for this is represented in gure 44 The neural network is a single layer of input activations with one neuron with RELU activation as output Figure 44 Quantum Parametric Circuit Hybridized with a classical Neural Network to learn the Quantum Fourier Trans form Here each parametric rotation is of the form R 00 ei4 11 We use quantumparametric versions of these rotations Rj in order to perform quantum enhanced optimization of the latter via MoMGrad The neu ral network connected to the output of the parametric circuit is a single neuron with rectied linear unit ReLu activation 2 Implementation Results Using a numerical simulation of the QPUCPU inter action we simulate the implementation of the rstorder hybrid quantumclassical MoMGrad described in section VI I 2 The qudit dimension of the simulated quantum parameters was d 7 once again We use a hybrid network stochastic gradient descent where an iteration of gradient descent is performed for each state and label combination jk jkk The loss function to be optimized was the mean squared error for a network prediction at the output of value yk and a desired label value jk the loss function is given by Lyk jk yk jk2 379 The gradient of such a loss function is straightforward to obtain The optimization procedure is that which is de78 scribed in section VI I 2 The results of the hybrid train ing are presented in gure 45 Let us now describe the set of hyperparameters cho sen to generate the results featured in gure 45 The learning rate for the classical network and the kicking rate for the parametric circuit QFB were both kept at 015 throughout the training The quantum param eters initial wavefunction was a Gaussian of 0 065 for all parameters with a mean 0 whose components were each sampled from independent classical Gaussian distributions of standard deviation 05 and mean 0 For the MoMGrad pointer states of further iterations the subsequent standard deviations were j 0 065 098j in all components for the jth iteration 0 5 10 15 20 25 30 Iteration 0 2 4 6 Mean Squared Error Classical SGD BackProp MoMGrad Figure 45 Training results for parametric circuit and classi cal neural network hybrid learning for the network featured in gure 44 Shown is the mean squared error between the neural network output and the true label The training was executed using the hybrid gradient descent technique described in sec tion VI I 2 Towards the end of training the Mean Squared Error average squared distance between labels averaged over the dataset converges to 012 thus indicating successful training VIII DISCUSSION OUTLOOK In this section we discuss potential implementations implications and possible future extensions of this work 1 Nearterm considerations We begin with a discussion of potential nearterm im plementations Parametric circuits have been shown to be successfully implementable on NISQ Noisy Interme diate Scale Quantum devices 49 Due to the variational nature of parametric circuits optimization algorithms in the presence of noise the parametric transformations can adjust in order to partially counter the eects of noise As there is currently no standard criterion quantifying how robust a certain algorithm is to noise and since execu tion performance can vary greatly depending both on the device and the algorithm most approaches have resorted to empirically checking performance on a casewise ba sis The common conception is that algorithms with low depth quantum circuits using a quantumclassical opti mization loop which relies only on expectation values of simple observables tend to be somewhat robust to noise Thus it is dicult to predict whether a certain optimizer and ansatz will perform well under various noise condi tions but in this section we shall speculate as to which protocols have the best chance of being implementable in the nearterm From the optimizers presented in this paper although QDD has the potential for nontrivial tunneling in the optimization landscape MoMGrad is the protocol with the best chance of execution on nearterm devices due to its lower circuit depth requirements In the case of MoMGrad for a lowdepth circuit ansatz assuming hav ing quantum parameters does not increase depth of ex ecution of a parametric gate the quantum feedforward and Baqprop should generally also be a lowdepth cir cuit The MoMGrad circuit includes twice the depth of the original ansatz plus an added depth due the exponen tial of the loss function For simple loss functions such as is the case for quantum classiers for example the exponential of the loss adds very minimal depth while for loss operators with noncommuting terms eg for Hamiltonian optimization one can leverage the Gradi ent Expectation Estimation technique from section VI H to split up the gradient over multiple runs for the various terms As for the ecient execution of quantumparametric gates there are a few options that could be tractable while adding minimal depth relative to a classically parametrized circuit ansatz The most elementary form of a quantum parameter register would be using a sin gle qubit instead of a qudit As mentioned in section IV B one can use a qubit to estimate the phase kickback induced by Baqprop analogous to singlequbit phase es timation at the cost of having to execute multiple runs in order to estimate the gradient to a suciently high precision Generally one could use perhaps only a few qubits eg on the order of 2 or 3 to form a qudit of potentially sucient dimension for multiple applica tions For such low numbers of qubits the Quantum Fourier transform is quite low depth hence the gradient readout should be relatively robust to noise Our numer ical experiments in section VII showed a good perfor mance with only 7dimensional qudits achievable with 3 qubits On the other hand since these were classical sim ulations of quantum computation the expectation values could be extracted directly from the simulator whereas a real quantum computation would necessitate multiple runs Thus using a smalldimensional qudit using a few qubits for the parameters may be sucient for some applications but in general one would expect the perfor mance to decay for many parameters since the readout79 of the gradient value is stochastic and could be greatly inuenced by noise during the execution of the Quantum Fourier transform Using language from section II not only can there be underow error phase kick too small to be well detected but there can also be overow error where the gradient phase kick exceeds the range of the qudit or qubit A possible alternative to qudits and qubits for imple mentation of the quantum parameters would be to use a continuous variable CV quantum mode qumode for each parameter Note that the formalism and deriva tions throughout this paper were compatible with both simulated qudit qumodes and physical qumodes Most current implementations of quantum computing whether it be via superconducting qubits or trapped ions have ways to build and control quantum harmonic oscilla tors onchip 90 91 Using CV modes as quantum pa rameters would require the ability to prepare squeezed states the ability to perform measurements of the po sitionmomentum quadratures and for the execution of qubitbased circuit ansatze the qumodes would need to be able to couple to qubits via an interaction of the form depicted in equation 286 A potential advantage of us ing a physical qumode for readout is its robustness to small perturbations in its phase or position For con trast a small error on one qubit of in a multiqubit Quan tum Fourier transform can lead to a signicant change in the readout value of the qudit position whereas a small nudge of the qumode leads to a small change in read out value Thus one could expect that the readout of the gradient values would be more robust to noise using analog qumodes The eective phase estimation capacity of the qumode will then be determined by its degree of squeezing 92 93 As for the implementation of the quantumcoherent classical neural networks from section V both small dimensional qudits or qumodes could work for the neu rons in the nearterm the same arguments from above concerning the quantum parameters apply One prob lem that may arise chaining many lowdimensional qu dits controlleddisplacements feedforward operations is that any sort of underoverow errors could add up exponentially with the depth On the other hand the current trend in classical machine learning has been to employ lowprecision arithmetic 94 for deep learning which would suggest that not all classical deep learn ing algorithms necessitate highprecision oating points for eective operation and training As such one may consider fewqubit precision quantumcoherent neurons potentially sucient in precision One could even poten tially consider the noise induced from the qudit impre cision as a form of regularization during both the feed forward an Baqprop phases For further details on the inuence of qudit imprecision on the feedforward opera tion and for more details on potential physical CV im plementations see section V C Let us now consider which applications from section VI have the best chance of being nearterm implementable As mentioned above apart from the overheads of us ing quantum parameters to execute the feedforward of the parametric circuit a key component to determin ing whether or not a certain Quantum Feedforward and Baqprop circuit is implementable in the nearterm is the circuit depth required for the execution of the the expo nential of the loss function For any quantum data appli cation which requires quantum state exponentiation one could consider the nearterm implementation of such an algorithm as unlikely mainly due to the large overheads of have multiple Fredkin gates and of batching quan tum state exponentials sequentially On the other hand quantum classication including measurement learning and quantum regression have fairly simple cost functions which can be exponentiated as simple exponentials of standard basis observables Another set of networks with a chance of nearterm implementation are the Quantum classical Hybrid neuralcircuit hybrids which as one may recall from section VI I 2 can be built from paramet ric circuits for quantum classiersregression One may imagine that having some additional classical neural pro cessing after a quantum parametric circuit may reduce the need for depth of the quantum circuit to attain the same transformation or accomplish a given learn ing task many cases Additionally the feedforward step only relies on simple expectation values of simple ob servables hence it should be robust to noise according to our criterion mentioned above Finally Hamiltonian Optimization which includes the Variational Quantum Eigensolver and the Quantum Approximate Optimiza tion Algorithm should be implementable on nearterm hardware with Baqprop For Hamiltonians that are a sum of commuting terms the Quantum Feedforward and Baqprop approach is straightforward and for non commuting terms in the Hamiltonian one can use the Gradient Expectation Estimation technique GEEP sec VI H which allows for parallelization of the gradient ac cumulation over multiple runs 2 Furtherterm considerations We now proceed to considering potential interesting applications in the furtherterm as well as future work In the longterm with the advent of largescale error corrected faulttolerant quantum computers the possi bility of training largescale classical neural networks on quantum computers as presented in section V will be come tractable At that moment one may want to con sider training neural networks with the Quantum Dy namical Descent QDD approach QDD may be more powerful than simple gradient descent in some instances due to being an eective Quantum Approximate Opti mization of the parameters Furthermore with a large scale errorcorrected quantum computer one could test the training of quantum neural networks using Quantum MetaLearning sec IV D for either the optimization of hyperparameters or network architecture to improve80 generalization error If one were to apply the MetaQDD protocol to either of these metalearning applications which would consist of a quantum dynamical simulation of descent with possible tunneling in the space of possi ble network architectures or hyperparameters one could imagine the distribution over such hyperparameters dif cult to simulate Again this is would be due the known diculty of simulating samples from a QAOA 40 Em pirical testing of possible advantages of Quantum Meta Learning via its largescale implementation could yield potentially interesting results As a sidenote although we only treated how to quan tize and train classical feedforward networks the opti mization methods featured in this paper could poten tially be used to train classical Boltzmann machines In recent work it was shown that one could train Quantum Boltzmann machines using QAOAtype quantum para metric circuits 38 The QAOA was used to approxi mately sample from various Gibbs distributions of net works such sampling is a necessary step to perform clas sical gradient descent of the networks weights Thus using techniques developed in this paper one could po tentially consider enhancing the optimization of the para metric QAOA circuit via MoMGrad or QDD such as to leverage Baqprop to accelerate the Gibbs sampling at each gradient descent If one were to go further and also consider the Boltzmann machines weights as quan tum parameters along with the corresponding paramet ric circuits parameters one could then potentially us a metaQDD optimization loop to quantumly the optimize Boltzmann machine weights similar to that featured in section IV D We leave further details of this approach for future work Another interesting avenue of future exploration is the possibility of performing quantum deep learning in a massively quantumparallelized fashion across a quan tum network Very recently the rst experimental demonstration of quantum state transfer between quan tum computing chips was successfully implemented 95 Eventually with Quantum Error Corrected state trans fers parallelization of algorithms across multiple quan tum chips will become a feasibility As we showed in sec tion IV various parallelization and regularizaton proto cols such as the Coherent Accumulation of Momenta Par allelization protocol CAMP sec IV A 3 and the Meta networked Swarm Optimization MISO sec IV C 2 can take advantage of a quantum network of quantum pro cessing units to improve the precision and time require ments of training networks In the particular case of CAMP there is a square root speedup to get the expec tation value of the gradient over a minibatch within a certain precision as compared to classical parallelization In modern classical deep learning parallelization is key to training largescale neural networks in a feasible time frame 65 it is thus to be expected that once quantum algorithms can reach a certain scale parallelization be comes indispensable just as it is in the classical case Now let us mention some avenues for further possible mathematical analyses which could be conducted A rst one is to provide a more detailed analysis of the resource overheads of synthesizing gate sequences for the various protocols studied in this paper As we established multi ple connections with quantum simulation theory perhaps tools from this subeld could be ported over to quantum deep learning In terms of the further analysis of the eective physics of the parameters for QDD one could view the stochastic QFB phase kicks as a repeated in teraction with an environment in this case the compute registers One could then perform an analysis of the eective open system dynamics and disspation terms at higherorders of the kicking rate Finally now that we have added multiple optimization techniques to the repertoire of quantum deep learning tools the key to making quantum deep learning feasible for largescale quantum parametric circuits will require new quantum parametric ansatze As pointed out by McClean et al 51 most current quantum parametric ansatze relying on random circuits of qubits have van ishing gradients Similar to the problem of vanishing gradients in classical machine learning current paramet ric ansatz have exponentially vanishing gradients in the number of degrees of freedom Further study into the mechanism behind the obtention of gradients of paramet ric circuits is necessary in order to allow for the design of new ansatze which could solve this vanishing gradient problem In section V B we explicitly detailed the mech anism for the backward quantum propagation of phase errors through the quantumcoherent neural networks One could then potentially extend the analysis presented in section VI for the layerwise backpropagation of the gradient signal in general quantum parametric circuits such as to provide the same level of detail as to how the gradient phase kick signal travels through the compute registers in order to inuence the parameters By choos ing specic ansatze one could examine the generators of each parametric circuit element and possibly repli cate the level of detail of the analysis from V B for this specic parametric ansatz Such an analysis would have the potential to shed new light on the vanishing gradient problem and point towards solutions We leave an anal ysis of this kind for general parametric quantum circuit ansatze for future work IX CONCLUSION The goal of this paper was to establish a bridge be tween the theories of classical and quantum deep learn ing such as to allow for the exchange of tools and the gain of new insights in both elds In alignment with this goal we took inspiration from classical deep learn ing techniques to create numerous new methods for the quantumenhanced optimization of quantum parametric networks on a quantum computer Furthermore we ex plored various ways classical deep learning can leverage quantum computation for optimization and how classi81 cal and quantum deep learning optimization strategies can directly interface with one another More specically we introduced a unied approach to the optimization of quantum parametric circuits and classical neural networks on a quantum computer based on a universal error backpropagation principle for quan tum parametric networks We then further extended these quantum optimization methods with a compatible set of tools for parallelization regularization and meta learning Furthermore we detailed how to leverage these optimization strategies for the eective training of any classical feedforward neural network on a quantum com puter as well as for numerous quantum parametric cir cuit applications We numerically tested both core op timization algorithms on multiple such applications em pirically demonstrating their eectiveness Finally we introduced a way to merge classical and quantum back propagation between classical and quantum computers opening up the possibility for the eld of truly hybrid quantumclassical deep learning We hope that the work presented in this paper will bol ster further work exploring this nascent eld of Quantum Deep Learning X ACKNOWLEDGEMENTS Quantum circuit simulations featured in this paper were executed on the Rigetti Forest Quantum Virtual Machine with code written in PyQuil 88 The authors would like to thank Rigetti Computing and its team for providing computing infrastructures and continued sup port for Forest The authors would also like to thank Steve Weiss and the Information Technology team at the IQC for providing additional computing infrastructures and IT support for this project The authors would like to thank Atmn Patel for useful discussions as well as Achim Kempf for the support GV and JP acknowledge funding from NSERC 1 I Goodfellow Y Bengio and A Courville Deep Learn ing MIT Press 2016 httpwwwdeeplearningbook org 2 D P Kingma and J Ba ArXiv eprints 2014 arXiv14126980 csLG 3 M D Zeiler ArXiv eprints 2012 arXiv12125701 csLG 4 M Bojarski D Del Testa D Dworakowski B Firner B Flepp P Goyal L D Jackel M Monfort U Muller J Zhang X Zhang J Zhao and K Zieba ArXiv e prints 2016 arXiv160407316 csCV 5 I Sutskever O Vinyals and Q V Le ArXiv eprints 2014 arXiv14093215 csCL 6 A van den Oord S Dieleman H Zen K Si monyan O Vinyals A Graves N Kalchbrenner A Se nior and K Kavukcuoglu ArXiv eprints 2016 arXiv160903499 csSD 7 T Mikolov K Chen G Corrado and J Dean ArXiv eprints 2013 arXiv13013781 csCL 8 K He X Zhang S Ren and J Sun ArXiv eprints 2015 arXiv151203385 csCV 9 V Mnih K Kavukcuoglu D Silver A Graves I Antonoglou D Wierstra and M Riedmiller ArXiv eprints 2013 arXiv13125602 csLG 10 D P Kingma and M Welling ArXiv eprints 2013 arXiv13126114 statML 11 I J Goodfellow J PougetAbadie M Mirza B Xu D WardeFarley S Ozair A Courville and Y Bengio ArXiv eprints 2014 arXiv14062661 statML 12 D E Rumelhart G E Hinton and R J Williams Nature 323 533 1986 13 P Domingos The Master Algorithm How the Quest for the Ultimate Learning Machine Will Remake Our World Penguin Books Limited 2015 14 S P Jordan Physical Review Letters 95 050501 2005 quantph0405146 15 A Gilyen S Arunachalam and N Wiebe ArXiv e prints 2017 arXiv171100465 quantph 16 B Catanzaro M Garland and K Keutzer in Proceed ings of the 16th ACM symposium on Principles and prac tice of parallel programming PPoPP 11 ACM Press 2011 17 N Srivastava G Hinton A Krizhevsky I Sutskever and R Salakhutdinov Journal of Machine Learning Re search 15 1929 2014 18 A Krogh and J A Hertz in Advances in neural infor mation processing systems 1992 pp 950957 19 J Biamonte P Wittek N Pancotti P Rebentrost N Wiebe and S Lloyd Nature 549 195 2017 20 S Lloyd M Mohseni and P Rebentrost Nature Physics 10 631 2014 21 P Rebentrost M Mohseni and S Lloyd Physical re view letters 113 130503 2014 22 M Schuld and N Killoran arXiv preprint arXiv180307128 2018 23 V Havlicek A D Corcoles K Temme A W Har row J M Chow and J M Gambetta arXiv preprint arXiv180411326 2018 24 V Giovannetti S Lloyd and L Maccone Physical re view letters 100 160501 2008 25 S Arunachalam V Gheorghiu T JochymOConnor M Mosca and P V Srinivasan New Journal of Physics 17 123010 2015 26 E Farhi and H Neven arXiv preprint arXiv180206002 2018 27 H Chen L Wossnig S Severini H Neven and M Mohseni arXiv preprint arXiv180508654 2018 28 E Grant M Benedetti S Cao A Hallam J Lockhart V Stojevic A G Green and S Severini arXiv preprint arXiv180403680 2018 29 A Peruzzo J McClean P Shadbolt MH Yung XQ Zhou P J Love A AspuruGuzik and J L Obrien Nature communications 5 4213 2014 30 R Salakhutdinov and H Larochelle in Proceedings of the thirteenth international conference on articial intel ligence and statistics 2010 pp 69370082 31 M H Amin E Andriyash J Rolfe B Kulchytskyy and R Melko Physical Review X 8 021050 2018 32 S H Adachi and M P Henderson arXiv preprint arXiv151006356 2015 33 H Neven G Rose and W G Macready arXiv preprint arXiv08044457 2008 34 M Mohseni and H Neven Constructing and program ming quantum hardware for robust quantum annealing processes 2016 uS Patent App 15109614 35 T F Rnnow Z Wang J Job S Boixo S V Isakov D Wecker J M Martinis D A Lidar and M Troyer Science 345 420 2014 36 M Benedetti J RealpeGomez R Biswas and A PerdomoOrtiz Physical Review A 94 022308 2016 37 H G Katzgraber F Hamze Z Zhu A J Ochoa and H MunozBauza Physical Review X 5 031026 2015 38 G Verdon M Broughton and J Biamonte arXiv preprint arXiv171205304 2017 39 E Farhi J Goldstone and S Gutmann arXiv preprint arXiv14114028 2014 40 E Farhi and A W Harrow arXiv preprint arXiv160207674 2016 41 S Hadeld Z Wang B OGorman E G Rief fel D Venturelli and R Biswas arXiv preprint arXiv170903489 2017 42 E Farhi J Goldstone S Gutmann and M Sipser arXiv preprint quantph0001106 2000 43 E Farhi J Goldstone and S Gutmann arXiv preprint quantph0201031 2002 44 E Crosson E Farhi C YY Lin HH Lin and P Shor arXiv preprint arXiv14017320 2014 45 E Crosson and A W Harrow in Foundations of Com puter Science FOCS 2016 IEEE 57th Annual Sympo sium on IEEE 2016 pp 714723 46 W Zeng N Rubin M Curtis A Polloreno R Smith J Angeles B Bloom M Block S Caldwell W OBrien et al in APS Meeting Abstracts 2017 47 M Benedetti D GarciaPintos Y Nam and A PerdomoOrtiz arXiv preprint arXiv180107686 2018 48 J R McClean J Romero R Babbush and A Aspuru Guzik New Journal of Physics 18 023023 2016 49 J Preskill arXiv preprint arXiv180100862 2018 50 D Gottesman in Quantum information science and its contributions to mathematics Proceedings of Symposia in Applied Mathematics Vol 68 2010 pp 1358 51 J R McClean S Boixo V N Smelyanskiy R Babbush and H Neven arXiv preprint arXiv180311173 2018 52 J Romero J P Olson and A AspuruGuzik Quantum Science and Technology 2 045001 2017 53 P D Johnson J Romero J Olson Y Cao and A AspuruGuzik arXiv preprint arXiv171102249 2017 54 S Lloyd and C Weedbrook arXiv preprint arXiv180409139 2018 55 M A Nielsen and I Chuang Quantum computation and quantum information 2002 56 R D Somma arXiv preprint arXiv150306319 2015 57 D Gross Journal of mathematical physics 47 122107 2006 58 S D Bartlett B C Sanders S L Braunstein and K Nemoto in Quantum Information with Continuous Variables Springer 2002 pp 4755 59 A Messiah and E Q M D B On Quantum mechanics dover books on physics 2014 60 S Ruder arXiv preprint arXiv160904747 2016 61 M D Zeiler arXiv preprint arXiv12125701 2012 62 D P Kingma and J Ba arXiv preprint arXiv14126980 2014 63 L Bottou in Proceedings of COMPSTAT2010 Springer 2010 pp 177186 64 R Horodecki P Horodecki M Horodecki and K Horodecki Reviews of modern physics 81 865 2009 65 J Dean G Corrado R Monga K Chen M Devin M Mao A Senior P Tucker K Yang Q V Le et al in Advances in neural information processing systems 2012 pp 12231231 66 C L Degen F Reinhard and P Cappellaro Reviews of modern physics 89 035002 2017 67 K Marshall R Pooser G Siopsis and C Weedbrook Physical Review A 91 032321 2015 68 HK Lau R Pooser G Siopsis and C Weedbrook Physical review letters 118 080501 2017 69 V Jelic and F Marsiglio European Journal of Physics 33 1651 2012 70 A Neelakantan L Vilnis Q V Le I Sutskever L Kaiser K Kurach and J Martens arXiv preprint arXiv151106807 2015 71 R Vilalta and Y Drissi Articial Intelligence Review 18 77 2002 72 J Bergstra and Y Bengio Journal of Machine Learning Research 13 281 2012 73 S Hochreiter A S Younger and P R Conwell in International Conference on Articial Neural Networks Springer 2001 pp 8794 74 M Andrychowicz M Denil S Gomez M W Homan D Pfau T Schaul B Shillingford and N De Freitas in Advances in Neural Information Processing Systems 2016 pp 39813989 75 C Finn P Abbeel and S Levine ArXiv eprints 2017 arXiv170303400 csLG 76 T Haner M Roetteler and K M Svore arXiv preprint arXiv180512445 2018 77 L B Rall Automatic dierentiation Techniques and ap plications Springer 1981 78 V Kliuchnikov D Maslov and M Mosca IEEE Trans actions on Computers 65 161 2016 79 R Barends J Kelly A Megrant A Veitia D Sank E Jerey T C White J Mutus A G Fowler B Campbell et al Nature 508 500 2014 80 B W Shore and P L Knight Journal of Modern Optics 40 1195 1993 81 PL DallaireDemers and N Killoran arXiv preprint arXiv180408641 2018 82 T R Bromley and P Rebentrost arXiv preprint arXiv180307039 2018 83 S Kimmel C YY Lin G H Low M Ozols and T J Yoder npj Quantum Information 3 13 2017 84 M M Wilde Quantum information theory Cambridge University Press 2013 85 I H Kim and B Swingle arXiv preprint arXiv171107500 2017 86 S Lloyd Science 1073 1996 87 D Poulin A Qarry R Somma and F Verstraete Phys ical review letters 106 170501 2011 88 R S Smith M J Curtis and W J Zeng ArXiv e prints 2016 arXiv160803355 quantph83 89 J C Lagarias J A Reeds M H Wright and P E Wright SIAM Journal on optimization 9 112 1998 90 J Poyatos J Cirac and P Zoller Physical review letters 77 4728 1996 91 M Hofheinz H Wang M Ansmann R C Bialczak E Lucero M Neeley A Oconnell D Sank J Wenner J M Martinis et al Nature 459 546 2009 92 N Liu J Thompson C Weedbrook S Lloyd V Vedral M Gu and K Modi Physical Review A 93 052304 2016 93 G VerdonAkzam Probing Quantum Fields Measure ments and Quantum Energy Teleportation Masters the sis University of Waterloo 2017 94 S Gupta A Agrawal K Gopalakrishnan and P Narayanan in International Conference on Machine Learning 2015 pp 17371746 95 P Kurpiers P Magnard T Walter B Royer M Pechal J Heinsoo Y Salathe A Akin S Storz J Besse et al Nature 558 264 2018