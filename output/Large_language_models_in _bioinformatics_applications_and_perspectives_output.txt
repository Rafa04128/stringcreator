Large language models in bioinformatics applications and perspectives Jiajia Liu1 Mengyuan Yang2 Yankai Yu3 Haixia Xu45 Kang Li5 Xiaobo Zhou167 1Center for Computational Systems Medicine McWilliams School of Biomedical Informatics The University of Texas Health Science Center at Houston Houston Texas 77030 USA 2School of Life Sciences Zhengzhou University Zhengzhou Henan 450001 China 3School of Computing and Artificial Intelligence Southwest Jiaotong University Chengdu Sichuan 611756 China 4The Center of Gerontology and Geriatrics West China Hospital Sichuan University Chengdu Sichuan 610041 China 5West China Biomedical Big Data Center West China Hospital Sichuan University Chengdu Sichuan 610041 China 6McGovern Medical School The University of Texas Health Science Center at Houston Houston TX 77030 USA 7School of Dentistry The University of Texas Health Science Center at Houston Houston TX 77030 USA These authors have contributed equally to this work Address correspondence to Xiaobo Zhou PhD McWilliams School of Biomedical Informatics The University of Texas Health Science Center at Houston 7000 Fannin St Houston TX 77030 Phone 7135003923 Email XiaoboZhouuthtmceduAbstract Large language models LLMs are a class of artificial intelligence models based on deep learning which have great performance in various tasks especially in natural language processing NLP Large language models typically consist of artificial neural networks with numerous parameters trained on large amounts of unlabeled input using selfsupervised or semisupervised learning However their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language In this review we will present a summary of the prominent large language models used in natural language processing such as BERT and GPT and focus on exploring the applications of large language models at different omics levels in bioinformatics mainly including applications of large language models in genomics transcriptomics proteomics drug discovery and single cell analysis Finally this review summarizes the potential and prospects of large language models in solving bioinformatic problems 1 Introduction Significant progress has been made in the field of natural language processing with the advent of large language models Examples of these models include OpenAIs GPTX 1 and Googles BERT 2 models These models are transformative because they can understand generate and manipulate human language at an unprecedented scale Vast Large language models are typically trained on datasets that encompass a significant portion of the internets text enabling them to learn the complexities of language and context These models are built upon a neural network architecture called transformers 3 The transformer architecture revolutionized NLP due to its parallelization scalability and ability to capture longrange dependencies in text Instead of relying on recurrent or convolutional layers transformers use selfattention mechanisms as previously described which allow them to assess the importance of every word in a sentence when understanding context This innovation is key to their remarkable performance The training regimen for large language models comprises two phases pretraining and finetuning During pretraining the model is trained on an extensive corpus of text data to acquire proficiency in grammar factual knowledge reasoning abilities and word understanding Finetuning tailors these models for specific tasks like translation summarization or questionanswering The adaptability of large language models is a major advantage they can excel at various NLP tasks without taskspecific architectures However they have found applications in diverse fieldsbeyond NLP including biology healthcare education finance customer service and more In particular there have been many successful applications of large language models in the field of bioinformatics In this manuscript we focus on the applications of large language models to several bioinformatic tasks through five areas DNA level RNA level protein level drug discovery and single cell analysis respectively corresponding to sections applications of large language models in genomics transcriptomics proteomics drug discovery and single cell analysis Applications of LLMs in genomics focus on LLMs using DNA sequence applications of LLMs in transcriptomics using RNA sequence applications of LLMs in proteomics focus on LLMs using protein sequence applications of LLMs in drug discovery focus on LLMs using Molecular SMILES seq and applications of LLMs in singlecell analysis focus on LLMs using gene expression data from scRNAseq or scMultiomics data Figure 1 2 Large language models in natural language processing The emergence of large language models has brought milestone progress to natural language processing These large language models particularly exemplified by BERT 2 and GPT 1 usually use the pretraining finetuning approach Figure 2 Pretraining is mainly to train a generalfoundation model with strong generalization ability on a largescale text corpus Fine tuning relies on the pretrained model undergoing additional training for a specific task This process allows the model to adjust to the unique data and demands of the task at hand The goal of finetuning is to improve the performance of the model on specific tasks such as sentiment analysis question answering systems abstract text summarization machine translation creative text generation etc in natural language processing Selfattention At the heart of the transformer is selfattention 3 In this mechanism every wordtoken in the input sequence is linked to three vectors the Key vector K the Query vector Q and the Value vector V These vectors are learned during the training of the model For each token the model calculates attention scores for its relationship with other tokens The attention score between a Query Q and Key K pair is calculated through the dot product which is then adjusted by scaling it down with the square root of the dimension of the key vectors to prevent exceedingly large values Subsequently a softmax function is frequently applied to attain normalized scores The attention score is then multiplied by the Value V vector to derive therepresentation through the weighted sum of the attention mechanism The selfattention process is represented as following 3 1 BERT Bidirectional Encoder Representations from Transformers BERT stands as a revolutionary model in the realm of natural language processing reshaping the landscape of deep learning Introduced by Google in 2018 BERT is engineered to comprehend the context and subtleties of human language in unprecedented ways 2 At its core BERT employs a Transformer architecture which enables it to capture the relationships between words in both directions of a sentence making it bidirectional This implies that BERT considers not only the words preceding a given word but also those following it allowing it to grasp the full meaning of a sentence One of BERTs most remarkable features is its pretraining process It learns from an extensive corpus of text from the internet effectively absorbing vast amounts of knowledge The pretrained model can subsequently undergo finetuning for diverse natural language understanding tasks including text classification question answering and language translation BERT has consistently demonstrated stateoftheart performance across a broad spectrum of NLP tasks and benchmarks thanks to its ability to handle context polysemy and longrange dependencies effectively Its versatility and accuracy have made it a pivotal tool in various applications from chatbots and virtual assistants to sentiment analysis and content recommendation systems GPT Generative Pretrained Transformer GPT stands as a remarkable achievement in the realm of natural language processing Introduced by OpenAI GPT is a sophisticated language model designed to generate humanlike text making it a pivotal component in a wide array of applications 1 At its core GPT is built upon the transformer architecture renowned for its attention mechanisms that allow it to comprehend the nuances of context and language structure What sets GPT apart is its pretraining phase where its exposed to an extensive corpus of text from the internet enabling it to absorb an immense amount of linguistic knowledge The real brilliance of GPT lies in its capacity to produce text that is both coherent and contextually relevant Given a prompt it can produce everything from creative stories and naturalsounding dialogue to summaries and translations This versatility has found applications in chatbots content generation language translation and even code generation GPTs knack for understanding context handling ambiguity and generating humanlike text has earned it a place of honor in various domains fromcreative writing and customer support to academic research and data analysis Its adaptability powered by finetuning makes it a potent tool for diverse languagerelated tasks Comparison between BERT and GPT BERT and GPT stand as two exceptional language models that have transformed the landscape of Natural Language Processing NLP While they share some common traits they also exhibit significant differences in their design and applications Both BERT and GPT leverage the transformer architecture employing attention mechanisms to grasp contextual information and dependencies within text data This architecture has demonstrated significant effectiveness in comprehending and generating text that resembles human language Furthermore both models go through a pretraining phase during which they are exposed to extensive amounts of text data sourced from the internet This unsupervised learning process helps them learn language structure grammar and a broad range of linguistic patterns Furthermore both BERT and GPT excel at transfer learning These models undergo pretraining on an extensive text corpus and can be finetuned for particular downstream tasks including text classification sentiment analysis or language generation This makes them versatile and applicable across various NLP tasks However there are notable differences between these models BERT employs a bidirectional approach considering both left and right contexts when learning language representations It aims to create deep contextual embeddings for words In contrast GPT uses a lefttoright approach generating text autoregressively by predicting the next word in a sequence based on the preceding words This design makes GPT more suitable for generation tasks Additionally their architectures differ with BERT using a bidirectional Transformer encoder and a masked language model MLM objective during pretraining while GPT uses a unidirectional transformer decoder and a causal language model objective BERT undergoes training through a masked language model task and next sentence prediction whereas GPT GPT1 is trained to predict the succeeding word in a sequence In practice BERT is often preferred for tasks requiring deep contextual understanding such as questionanswering sentiment analysis and text classification It is widely used for tasks where understanding the context of words is crucial On the other hand GPT shines in text generation tasks including creative writing language translation and dialogue generation It is an excellent choice for tasks that prioritize generating humanlike text 3 Applications of large language models in bioinformatics31 Applications of large language models in genomics Decoding the language embedded in DNA to unveil its concealed instructions has long stood as a primary goal in biological research 4 The universal genetic code elucidating the translation of DNA into proteins has recently garnered attention for deciphering biological functions through models based on BERT or GPT architectures DNABERT 5 for instance employs a transformera robust attentionbased architecture renowned for its prowess in diverse natural language processing tasks An evolution of this DNABERT26 introduces enhancements such as an efficient tokenizer and strategies to address input length constraints thereby optimizing time and memory usage while bolstering model capabilities Notably DNABERT2 introduces the Genome Understanding Evaluation GUE a comprehensive dataset for multispecies genome classification With a threefold increase in efficiency compared to DNABERT DNABERT2 outperforms its predecessor on 23 out of 28 datasets GROVER 7 a DNA language model leveraging bytepair tokenization to scrutinize the human genome GROVERs core objective lies in discerning contextual relationships between tokens facilitating the identification of genomic region structures associated with functional genomics annotation This unique approach positions GROVER as an invaluable tool for researchers delving into the intricacies of the human genome Building on the success of the GPT series in extracting general information from DNA sequences DNAGPT emerges as a GPTbased model for DNA pretrained on a vast dataset exceeding 10 billion base pairs This model can be finely tuned for various DNA sequence analysis tasks Additionally there is the Nucleotide Transformer 8 as another foundational model for DNA sequences has developed four distinct language models of varying sizes These models have been pretrained on three different datasets spanning multiple species DNABERT DNABERT2 GROVER DNAGPT and the Nucleotide Transformer all being pretrained models find application in sequence prediction tasks including predicting promoter regions enhancer regions cisregulatory elements splice sites and transcription factor binding sites Detailed information is available in Figure 3 Table 1 and Supplementary Table 1 311 DNA sequence language models used to predict the genomewide variant effects The significance of DNA sequence mutations in fostering diversity both within and between species cannot be overstated Genomewide association studies GWAS have played a pivotal role in furnishing essential biological insights across various species However a persistent challenge lies in pinpointing the specific causal variants accountable for the associationsuncovered in these studies 4 9 The Genomic Pretrained Network GPN 10 is focused on acquiring knowledge about genomewide variant effects by undergoing unsupervised pretraining on genomic DNA sequences In this process the model is presented with a 512bp DNA sequence wherein certain positions are intentionally masked Its primary objective is to predict the nucleotides at these masked positions Notably GPN excels in predicting the effects of rare variants that conventional GWAS methods may overlook Leveraging the DNA sequence of any species GPN demonstrates its capacity to predict variant effects across the entire genome Furthermore foundational models within the DNA sequence language model realm such as DNABERT DNABERT2 and the Nucleotide Transformer also exhibit the capability to predict variants from DNA sequences This collective advancement underscores the ongoing efforts to enhance our understanding of the intricate relationship between DNA sequence mutations and the resultant diversity in biological landscapes 312 DNA sequence language models used to predict the cisregulatory regions Cisregulatory sequences including enhancers and promoters represent pivotal elements in the regulation of gene expression exerting a significant impact on the development and physiology 11 However identifying these sequences in DNA represents a major challenge which is essential to comprehend their functions and their direct or indirect association with various diseases 12 To address this issue pretrained models such as DNABERT DNABERT2 GROVER and DNAGPT have been developed to accurately predict the promoter regions and their activities These models have demonstrated remarkable accuracy and have emerged as valuable tools in the field of molecular biology for identifying cisregulatory regions in DNA and thus providing useful information on their functions and related diseases BERTPromoter 13 was proposed to identify promoters and their activity It uses a pretrained BERT model for efficient feature representation and SHAP analysis for filtering Different machinelearning algorithms are subsequently applied to construct the final prediction model Notably BERT features in BERTPromoter demonstrate significant performance improvement and enhanced model generalization compared to traditional feature representations Enhancers small DNA segments binding to transcription factor proteins play a crucial role in fortifying gene transcription and influencing gene expression 14 15 iEnhancerBERT 16 presents an innovative transfer learning approach leveraging DNABERT to facilitate enhancer prediction Departing from conventional finetuning methods iEnhancerBERT utilizes the outputof all transformer encoder layers to generate feature vectors These vectors undergo further classification through a Convolutional Neural Network CNN layer Recognizing biological sequences as the natural language for computational modeling signals an emerging trend in the field In conclusion iEnhancerBERT presents a promising avenue for identifying new DNA enhancers 313 DNA sequence language models used to predict the DNAprotein interaction The accurate identification of DNAprotein interactions is vital for regulating gene expression and understanding evolutionary processes 17 Several DNA language models have been developed to predict these interactions with downstream tasks for DNABERT DNABERT2 and GROVER including the prediction of proteinDNA binding based on ChIPseq data Besides TFBert 18 is a pretraining DNAprotein binding model that has the ability to provide satisfactory results with minimal finetuning on a single dataset This model treats DNA sequences as natural sentences and kmer nucleotides as words allowing for the effective extraction of context information from upstream and downstream nucleotides Through pretraining on the 690 ChIPseq datasets TFBert can efficiently accomplish this task MoDNA 19 framework is an innovative approach that incorporates common DNA functional motifs as domain knowledge In the initial stage of self supervised pretraining the MoDNA framework establishes two prediction tasks including kmer tokens prediction and motif prediction Through pretraining on vast amounts of unlabeled genome data the MoDNA framework successfully acquires semanticlevel genome representations which prove useful for promoter prediction and transcription factor binding site prediction In essence the MoDNA framework can be regarded as a biological language model that predicts DNAprotein binding 314 DNA sequence language models used to predict the DNA methylation DNA methylation stands as a fundamental biological process playing a pivotal role in the epigenetic regulation of gene expression 20 This process is linked to various medical conditions and finds diverse applications such as serving as a marker for metagenomic binning The types of DNA methylation vary based on the specific nucleotide to which the methyl group attaches in the sequence 21 Several models have been developed to predict DNA methylation with varying degrees of accuracy and specificity The advancement of these models has led to a better understanding of the mechanisms underlying DNA methylation and its implications in various biological processes Among them the BERT6mA 22 is specifically designed for predicting 6methyadenine 6mA sites iDNAABT 23 iDNAABF 24 and MuLanMethyl 25 are versatile predictors that can be used for various methylation predictions including 6mA 5 hydroxymethylcytosine 5hmC and 4methylcytosine 4mC iDNAABT an advanced deep learning model incorporates adaptive embedding based on BERT along with transductive information maximization TIM While demonstrating potential in identifying species iDNA ABT has yet to fully explore feature representation learnings potential especially in uncovering key sequential patterns critical for understanding DNA methylation mechanisms The iDNAABF approach adopts a multiscale architecture using multiple tokenizers instead of a single one This enables BERT encoders to extract diverse embeddings based on tokenization which are then combined to generate the final evolutionary output feature On the other hand MuLanMethyl introduces a novel deeplearning framework employing five transformerbased language modelsBERT2 DistilBERT26 ALBERT27 XLNet28 and ELECTRA29 to predict three types of methylation sites from DNA sequences and taxonomic information The model generates its output by averaging the prediction probabilities from these language models Its noteworthy that language models find successful application in biological sequence analysis and the joint utilization of different models significantly enhances performance 315 DNA sequence language models used to identify the splice site The precise splicing of premRNA is crucial for ensuring accurate protein translation This intricate process is governed by the selection of splice sites during splicing reactions leading to the creation of diverse isoforms and splicing events However identifying splice sites poses a challenge especially considering the prevalent GTAG sequences in the DNA 30 In response to this challenge DNABERT and DNABERT2 were developed and trained using 10000 donor acceptor and nonsplice site sequences from the human reference genome The objective was to predict splice sites from DNA sequences Notably DNABERT consistently exhibited high attention to intronic regions This observation suggests the presence and functional significance of various intronic splicing enhancers and silencers acting as cisregulatory elements for splicing The focus on intronic regions underscores the importance of these elements in modulating splicing outcomes and highlights the potential of DNABERT models in unraveling the complexities of splicing regulation 32 Applications of large language models in transcriptomicsEfforts to develop BERTbased language models for DNA have faced challenges in accurately capturing evolutionary information from homologous sequences Unlike proteins RNA sequences are less conserved In response to this two notable RNA foundation models have been introduced RNAFM 31 and RNAMSM 32 RNAFM employs selfsupervised learning to predict secondary3D structures leveraging the vast dataset of all 23 million noncoding RNA sequences This approach allows RNAFM to effectively capture diverse structural information providing a comprehensive understanding of RNA sequence features On the other hand RNAMSM utilizes homologous sequences sourced from RNAcmap through an automated pipeline This model excels in accurately mapping to 2D base pairing probabilities and 1D solvent accessibilities The pre trained model can be finetuned for various downstream tasks related to RNA structure and function as shown in Figure 4 Table 1 and Supplementary Table 1 321 RNA sequence language models used to predict the RNA family classification and secondary structure RNA secondary structure prediction poses a substantial challenge for RNA structural biologists requiring focused efforts to better understand RNA folding rules and enhance the accuracy of structure prediction models Such models hold significant potential for facilitating downstream tasks including the development of RNAtargeting drugs 33 RNABERT 34 is designed with three key components tokens and position embedding a transformer model and pretraining tasks Leveraging the pretraining BERT algorithm for noncoding RNA ncRNA RNABERT is specifically tailored for secondary structure prediction and RNA family classification The models architecture and training tasks are strategically crafted to capture the intricate rules governing RNA folding enabling more accurate predictions One notable application of RNABERT lies in addressing the practical need for rapid and precise structural alignment of unknown sequences to existing RNA families This capability makes RNABERT a valuable tool for annotating novel transcripts assisting researchers in understanding the structural characteristics of previously uncharacterized RNA molecules The potential contributions of RNABERT extend beyond structure prediction offering practical solutions for tasks critical to advancing our knowledge of RNA biology and its applications in therapeutic development 322 RNA sequence language models used to predict the RNA splicing RNA splicing is a vital process in the posttranscriptional gene expression of eukaryotic organisms Researchers have made strides in enhancing the sequencebased modeling of RNA splicingthrough the development of a pretrained model known as SpliceBERT 35 This model is trained on precursor messenger RNA sequences derived from 72 vertebrates enabling it to generate embeddings that preserve both the evolutionary information of nucleotides and the functional characteristics of splice sites SpliceBERT serves a dual purpose by not only capturing the nuances of RNA splicing but also facilitating the identification of potential splicedisrupting variants The pretrained model allows for the unsupervised prioritization of such variants based on their impact on the output of SpliceBERT within the sequence context This capability offers a valuable tool for researchers seeking to understand the influence of genetic variations on RNA splicing providing insights that can aid in the identification and prioritization of potentially significant variants in an efficient and datadriven manner 323 RNA sequence language models used to identify the lncRNAs and predict the lncRNAs coding potential Long noncoding RNA lncRNA is a crucial transcript form that plays a substantial regulatory role in the development of cancers and diseases without encoding proteins 36 Initially small Open Reading Frames sORFs in lncRNA were thought to be weak in protein translation However recent studies have revealed that they can indeed encode peptides This discovery adds complexity to the identification of lncRNA particularly those containing sORFs which is crucial for uncovering novel regulatory factors Addressing this challenge LncCat 37 utilizes category boosting and ORFattention features for improved performance on both long ORF and sORF datasets The ORFattention feature positively influences the prediction of lncRNA LncCat employs the BERT model to represent peptide sequences encoded by ORFs as part of the ORF attention features CatBoost38 is utilized to build the prediction model incorporating the aforementioned features to identify lncRNA The effectiveness of LncCat is demonstrated across five species datasets and the Riboseq dataset showcasing its utility in accurately identifying lncRNA with sORFs and contributing to the discovery of novel regulatory elements The prediction of translatable sORFs within lncRNAs referred to as lncRNAsORFs is crucial f or expediting the discovery of peptides encoded by these RNAs Computational prediction metho ds play a vital role in this task In this context LSCPPBERT httpsgithubcomSakuraxiaLSC PPBERT emerges as a novel method designed for predicting the coding potential of lncRNAs ORFs in plants Leveraging pretrained bidirectional encoder representations from transformer m odels LSCPPBERT offers a reliable tool for predicting coding lncRNAsORFs and holds the potential to significantly contribute to drug development and agricultural applications by enhancing our understanding of the coding potential within lncRNAs 324 RNA sequence language models used to predict the RNARBP interactions RNA sequences differ from DNA sequences by a single base thymine to uracil resulting in a singular variance where the syntax and semantics largely remain congruent The versatility of BERT extends beyond DNA to encompass Crosslinking and Immunoprecipitation CLIPseq data providing a valuable tool for predicting the binding preferences of RNAbinding proteins RBPs BERTRBP 39 is a model specifically designed for forecasting RNARBP interactions It adapts the BERT architecture and is pretrained on a human reference genome BERTRBP outperforms contemporary prediction models when assessed against eCLIPseq data from 154 RBPs Additionally the model exhibits the ability to discern both transcript region types and RNA secondary structures based solely on sequence information In essence BERTRBP not only contributes insights into the finetuning mechanisms of BERT in biological contexts but also provides compelling evidence of the models versatility in addressing various challenges related to RNA showcasing its potential in advancing our understanding of RNAprotein interactions 325 RNA sequence language models used to predict the RNA modification Posttranscriptional modifications of RNA play a crucial role in a diverse array of biological processes Among these modifications N7methylguanosine m7G stands out as one of the most prevalent playing an essential role in gene expression regulation 36 The precise identification of m7G sites within the transcriptome is of immense value for a comprehensive understanding of their potential functional mechanisms While highthroughput experimental methods offer precise localization of m7G sites their costliness and timeconsuming nature present challenges In response to this BERTm7G 40 emerges as a transformative computational tool Grounded in the transformer architecture of BERT and utilizing stacking ensemble techniques BERTm7G excels at identifying RNA N7methylguanosine sites solely from RNA sequence information This computational method proves imperative in accurately pinpointing m7G sites providing a more efficient alternative to laborintensive experimental approaches BERTm7G showcases the power of computational approaches in unraveling posttranscriptional modifications offering a valuable tool for researchers seeking to understand the functional implications of m7G in gene expression regulationThe posttranscriptional 2Omethylation Nm RNA modification plays a significant role in diverse cellular processes and is linked to several diseases 41 To gain profound insights into the underlying biological mechanisms the Bert2Ome 42 method proves to be an efficient tool for inferring 2Omethylation RNA modification sites directly from RNA sequences Bert2Ome integrates a BERTbased model with Convolutional Neural Networks CNN to discern the intricate relationship between modification sites and the content of RNA sequences This innovative approach not only reduces the time required for laborintensive biological experiments but also surpasses existing methodologies across various datasets and species demonstrating superior performance across multiple metrics Bert2Ome showcases the power of computational methods in advancing our understanding of posttranscriptional RNA modifications providing a valuable tool for researchers exploring the role of 2Omethylation in diverse cellular contexts and diseases 326 RNA sequence language models used to predict protein expression and mRNA degradation mRNA vaccines have emerged as a costeffective swift and secure alternative to traditional vaccines displaying high potency43 The mechanism of action for mRNA vaccines involves introducing a segment of mRNA corresponding to a viral protein typically derived from the viruss outer membrane Thus CodonBERT44 has been specifically designed for mRNA sequences to predict protein expression Utilizing a multihead attention transformer architecture framework CodonBERT underwent pretraining on a vast dataset comprising 10 million mRNA coding sequences from diverse organisms This extensive pretraining equips CodonBERT to excel in various mRNA prediction tasks including protein expression and mRNA degradation prediction One of CodonBERTs notable strengths lies in its capacity to assimilate new biological information positioning it as an asset for advancing mRNA vaccine design By surpassing existing stateof theart methods CodonBERT contributes to optimizing mRNAbased vaccine development promising improved efficacy and broader applicability in the realm of immunization The models proficiency in predicting protein expression levels enhances its utility in designing mRNA sequences for vaccines ultimately impacting the efficiency and effectiveness of the vaccine development process 33 Applications of large language models in proteomicsProtein is an indispensable molecule in life assuming a pivotal role in the construction and sustenance of vital processes As the field of protein research advances there has been a substantial surge in the accumulation of protein data 45 In this context the utilization of large language models emerges as a viable approach to extract pertinent and valuable information from these vast reservoirs of data Several pretrained protein language models PPLMs have been proposed to learn the characteristic representations of proteins data eg protein sequences gene ontology annotations property descriptions then applied to different tasks by finetuning adding or altering downstream networks such as protein structure posttranslational modifications PTMs and biophysical properties which align with corresponding downstream tasks like secondary structure prediction major PTMs prediction and stability prediction 46 47 Even though antibodies are classified as proteins the datasets of antibodies and subsequent tasks differ significantly from those of proteins Through the establishment and continuous updates of the Observed Antibody Space OAS database 48 a substantial amount of antibody sequence data has become available which can be utilized to facilitate the development of pretrained antibody large language models PALMs PALMs primarily delve into downstream topics encompassing therapeutic antibody binding mechanisms immune evolution and antibody discovery which correspond to tasks like paratope prediction B cell maturation analysis and antibody sequence classification Figure 5 In this section some of the popular proteinrelated large language models of recent years are introduced as well as corresponding important downstream tasks It is important to emphasize that the capabilities of both PPLMs and PALMs extend beyond the specific downstream tasks outlined in this section For further details additional information can be referenced within Table 2 and Supplementary Table 2 331 Protein language models for secondary structure and contact prediction The structure of proteins plays a crucial and decisive role in their function and interactions 49 Nonetheless the conventional laboratorybased techniques employed for protein structure analysis are frequently characterized by their timeconsuming laborintensive nature In addition to traditional templatebased and physicsbased methods with the development of deep learning the use of large language models to predict protein structures has gradually shown advantages in computational speed and prediction accuracy 50MSA Transformer 51 presents a protein language model that takes a set of sequences as input in the form of a multiple sequence alignment MSA This model employs a unique mechanism of interleaved row and column attention across the input sequences After model trained with a variant of MLM objective across many protein families it outperformed other unsupervised approaches at the time Furthermore it exhibits superior parameter efficiency compared to prior stateoftheart protein language models When using PPLMs to predict secondary structure or contact based on the experience brought by BERT it seems that using a language model with a larger number of parameters is easier to achieve better performance Few models seem to have more parameters than the largest models in ProtTrans 52 ProtTrans trains a series of large language models based on two autoregressive models TransformerXL 53 XLNet 28 and four automatic encoder models BERT 2 Albert 27 Electra 29 T5 54 on data from UniRef 55 and BFD 56 57 containing up to 393 billion amino acids The parameters of models range from millions to billions In addition to predicting secondary structure it is worth highlighting that ProtTrans achieved a significant breakthrough in perresidue predictions For the first time the transfer of the most informative embeddings ProtT5 outperformed the stateoftheart methods without relying on evolutionary information thus bypassing the need for costly database searches 332 Protein language models for protein sequence generation Generation of protein has broad application prospects in fields such as drug design and protein engineering 58 By using methods such as machine learning or deep learning protein sequences can be generated The generated sequences are hoped to have good foldability so that they can form stable threedimensional structures Moreover the desired proteins are expected to exhibit specific functional properties including enzyme activity and antibody binding capability The advancement of large language models and the integration of conditional models have significantly propelled the progress in the field of protein generation 59 The model referred to as ProGen 60 incorporates UniprotKB Keywords as conditional tags in 2020 These tags encompass a vocabulary consisting of various categories including biological process cellular component and molecular function In total the conditional tags encompass over 1100 distinct terms When assessing protein sequences generated by ProGen using metrics for sequence similarity secondary structure accuracy and conformational energy they exhibit desired structural properties In 2022 inspired by the remarkable achievements of generative Transformerbased language models like the GPTx series the development of ProtGPT2 61emerged Notably the proteins generated by ProtGPT2 exhibit amino acid propensities t following the principles of natural ones Assessments involving disorder and secondary structure prediction reveal that a substantial majority 88 of ProtGPT2generated proteins possess globular characteristics aligning with the attributes found in natural sequences Employing AlphaFold 62 63 on ProtGPT2 sequences yields wellfolded nonidealized structures encompassing the presence of extensive loops and the emergence of previously unseen topologies that are absent from current structure databases It appears that ProtGPT2 has acquired the language specific to proteins 333 Protein language models for protein function prediction Proteins are molecules that play a crucial role in various aspects of cellular metabolism signal transduction and structural support in living organisms A deep understanding of the function of proteins in living organisms is of great significance for drug development and disease mechanism analysis The diversity and complexity of proteins make it difficult to accurately predict and annotate their functions Fortunately PPLMs can effectively address these challenges 64 65 Taking into account the substantial presence of local semantics within protein sequences a novel approach to pretraining modeling referred to as SPRoBERTa 66 was introduced in 2022 This method takes protein sequences as inputs and offers the flexibility of straightforward finetuning for diverse proteinrelated tasks encompassing prediction tasks at the proteinlevel tasks such as remote homology prediction and protein function prediction as well as amino acid level eg secondary structure prediction and amino acid pairlevel eg contact prediction In the next year ProtST 67 introduced a multimodal training framework for proteins which involves the integration of a protein language model PLM whose input is protein sequences and a biomedical language model BLM whose input is protein property descriptions into a large multimodal model This integration is achieved through three pretraining tasks unimodal mask prediction multimodal representation alignment and multimodal mask prediction The proposed model demonstrates exceptional performance in diverse downstream tasks related to protein representation In addition to finishing the task of protein function annotation it shows the effectiveness on zeroshot protein classification Furthermore the model possesses the capability to facilitate the retrieval of functional proteins from a vastscale database even in the absence of any functional annotation 334 Protein language models for major posttranslational modification predictionPosttranslational modification PTM refers to the process in which the structure and function of proteins are changed through a series of chemical modification reactions after translation is completed including various chemical changes such as phosphorylation methylation acetylation and glycosylation of proteins These modifications can significantly impact protein stability subcellular localization interactions and functional expression Indepth investigation of PTMs yields valuable insights for disease diagnosis and therapeutic interventions 68 69 Language models can perform tasks such as signal peptide prediction and major PTMs prediction effectively ProteinBERT 70 is not really a large language model in terms of parameters only 16M but thanks to the introduction of the GO annotation prediction task and the interaction of GO with protein sequences compared with other deep learning models with larger parameters this model has achieved considerable or even better performance on multiple benchmarks covering diverse protein properties including major PTMs prediction 335 Protein language models for evolution and mutation prediction During the process of biological evolution the sequence and structure of proteins undergo changes Evolution and mutation serve as vital mechanisms that generate functional diversity in proteins 71 Gaining insights into the process of protein evolution and mutation can reveal strategies for organisms to adapt to environmental changes and survival competition as well as the origin and evolution of protein function and provide new ideas for drug development and disease treatment 72 In the context of protein sequence inputs early protein language models treated an entire sequence as either a paragraph or a sentence with individual amino acids representing individual words 73 74 In 2019 a model known as UniRep 75 built upon the Long ShortTerm Memory LSTM architecture emerged This model underwent training using the UniRef50 55 dataset and exhibited a remarkable improvement in efficiency surpassing other models in several tasks including remote homology detection and mutational effect prediction Since 2020 more and more large language models of protein have been proposed to perform prediction in evolution and mutation In 2020 a deep transformer model known as ESM1b 76 underwent training on a vast and diverse dataset comprising 250 million sequences This training enabled the model to acquire protein sequence representations encompassing essential characteristics The architecture of the model comprised 33 layers housing approximately 650 million parameters To facilitate its training ESM1b utilized selfsupervised strategy masking language modeling objective Thisapproach allowed the model to learn and capture crucial patterns and dependencies within the protein sequences thereby enhancing its overall performance and representation capabilities 336 Protein language models for biophysical properties prediction The biophysical properties of proteins include fluorescence landscapes stability landscapes and so on 77 Accurate prediction of these properties plays a crucial role in advancing our understanding of protein folding mechanisms stability conformational alterations and so on This is of great significance for the development of drug design protein engineering enzyme engineering and other relevant fields The progressive advancements in deep learning have enabled the efficient utilization of the continuously evolving PPLMs for the precise prediction of biophysical properties associated with proteins A significant development known as Tasks Assessing Protein Embeddings TAPE 78 emerged in 2019 which introduced a comprehensive benchmark of protein bioinformatics tasks This paper aimed to establish a standardized evaluation system for protein transfer learning by providing well defined tasks curated datasets and rigorous assessment metrics The task set encompassed five distinct problems including fluorescence landscape prediction and stability landscape prediction and spanning three major aspects of protein analysis protein structure prediction remote protein homolog detection and protein design This systematic approach facilitated the rigorous evaluation and comparison of different methodologies and models within the field of protein transfer learning In 2022 PromptProtein 79 taking protein sequences as inputs stands as the pioneering promptbased pretrained protein model aiming to address various levels of protein structures through promptguided multitask pretraining Furthermore it incorporates a prompt finetuning module enabling downstream tasks to effectively leverage specific levels of structural information as required Through extensive experimentation in the domains of function prediction and biophysical properties prediction PromptProtein demonstrates significant superiority over existing methods showcasing substantial performance gains 337 Protein language models for proteinprotein interaction and binding affinity prediction Proteinprotein interaction PPI constitutes a fundamental molecularlevel process in biological activities and its prediction holds profound significance in the realm of drug discovery and design Investigating the interaction between proteins can help us discover new drug targets and design drugs with high efficiency and selectivity PPLMs can help us efficiently and relatively accurately obtain proteinprotein interaction types and binding affinities between proteins 80 81The underlying motivation behind the KeAP 82 model aligns with that of ProtST as both aim to incorporate more finegrained information compared to OntoProtein 83 KeAP adopts a triplet format consisting of Protein Relation Attribute as input which is subsequently processed by distinct encoders and a specially designed cascaded decoder based on the Transformer architecture The model employs Masked Language Modeling MLM as the primary pretraining task facilitating efficient training Leveraging its unique crossattention fusion mechanism the model excels in capturing intricate protein information at a finer granularity As a result KeAP exhibits exceptional performance across nine diverse downstream tasks including proteinprotein interaction identification and proteinprotein binding affinity estimation 338 Antibody large language models for antigenreceptor binding and antigenantibody binding prediction Antigen proteins break down in the cytoplasm forming neoantigen peptides These peptides bind to the Major Histocompatibility Complex MHC creating pMHC complexes After a series of steps these complexes reach the cell membrane for presentation Subsequently the Tcell Receptor TCR recognizes the pMHC complex stimulating B cells to produce antibodies triggering an immune response 84 The application of language models in this process aims at accurately predicting the binding of peptides to HLA molecules as a key objective85 86 Peptides serve as a lifes language with large language models excelling in extracting context particularly in pMHC binding and presentation prediction For instance MHCRoBERTa 87 utilizes the pretrained BERT to model the input amino acid sequences Through effectively learning the biological meanings of each token the MHCRoBERTa model is able to distinguish between different alleles However MHCRoBERTa primarily focused on pMHCI prediction BERTMHC 88 is a specific pMHC II binding predicting method that incorporated 2413 MHCpeptide pairs encompassing 47 MHC class II alleles into the training data This effectively fills a gap in the field of pMHCII binding prediction Another key goal is predicting the binding specificity of adaptive immune receptors AIRs for antigens This variability in specificity mainly arises from the flexibility of three complementarity determining region CDR loops CDR13 with CDR3 being crucial for binding to antigenic peptides89 TCRBERT 90 leverages unlabeled TCR CDR3 sequences to learn a general representation of TCR sequences This enables downstream applications for predicting the antigenspecificity in TCR recognition However TCRBERT trains each individual AIR chain separately and fails to understand the paired interaction between the two chains of AIR Jianhua Yao et al effectively addressed this issue by pretraining a specially designed BERT model SCAIRBERT 91 which outperforms other stateoftheart methods in both TCR and BCR antigenbinding specificity prediction tasks After the antigen recognition process by T cells concludes it stimulates B cells to produce specific antibodies that bind to the corresponding antigen92 In antibody language models section three recent studies from PALMs will be introduced PALMs are presented independently mainly because of the differences between their downstream tasks and those of PPLMs It is also worth mentioning that the research on the large language model of antibodies is also a hot topic of recent research AbLang 93 an antibody language model built upon RoBERTa 94 was developed with the hypothesis that models trained on antibody databases would exhibit superior performance in addressing antibodyrelated challenges One of the specific problems AbLang aims to solve is the restoration of residues that are lost during the sequencing process due to errors Comparative evaluations have demonstrated that AbLang outperforms both IMGT germlines 95 and the general protein language model ESM1b 76 in terms of accurately restoring the missing residues in antibody sequences Furthermore AbLang achieves this with increased efficiency demonstrating a higher processing speed compared to the aforementioned models AntiBERTa 96 leverages the latent vectors derived from protein sequences the model exhibits a discernible comprehension of the antibody language to a certain degree as evidenced by the visual representations generated The models ability is exemplified through a diverse array of tasks including tracing the B cell origin of the antibody quantifying immunogenicity and predicting the antibodys binding site The EATLM 97 is a straightforward architecture composed of a series of stacked transformer layers Its uniqueness lies in the pretraining tasks it employs Alongside the conventional Masked Language Modeling MLM the model introduces additional pretraining tasks namely Ancestor Germline Prediction AGP and Mutation Position Prediction MPP These tasks aim to incorporate specific biological mechanisms into the pretraining phase The most important contribution of the paper that proposed this model was to propose a reliable antibodyspecific benchmark to evaluate different preprotein language models and antibody language models34 Applications of large language models in drug discovery Drug discovery is an expensive and longterm process that exhibits a low success rate During the early stages of drug discovery computeraided drug discovery employing empirical or expert knowledge algorithms machine learning algorithms and deep learning algorithms serve to accelerate the generation and screening of drug molecules and their lead compounds 98100 It speeds up the entire drug discovery process especially the development of small molecule drugs Among commonly used medications small molecule drugs can account for up to 98 of the total 101 The structure of small molecule drugs exhibits excellent spatial dispersibility and their chemical properties determine their good druglike properties and pharmacokinetic properties 102 With the development of deep learning and the proposal of large language models it has become easy to apply these methods to discover hidden patterns of molecules and interactions between molecules for drugs such as small molecules and targets such as proteins and RNA that can be easily represented as sequence data The Simplified MolecularInput LineEntry System SMILES string and chemical fingerprint are commonly used to represent molecules Additionally through the pooling process of graph neural networksGNN small molecules can be transformed into sequential representations 103 With the protein sequence large language models can engage in drug discovery through various inputs Within this section key tasks within the early drug discovery process that have effectively leveraged large language models will be introduced Figure 6 Table 3 Supplementary Table 3 341 Large language models for druglike molecular properties prediction During the drug discovery process significant attention is devoted to specific properties associated with candidate molecules such as Absorption Distribution Metabolism Excretion and Toxicology ADMET and Pharmacokinetics PK The objective is to facilitate the development of more efficacious accessible and safe drugs 104 105 The utilization of large language models in molecular property prediction encompasses downstream tasks that involve predicting these properties Since the input of molecular SMILES representation is consistent it is easy to improve and finetune the model based on specific datasets according to the requirements properties of interest to researchers In contrast to its predecessors SMILESBERT 106 departed from the usage of knowledgebased molecular fingerprints as input Instead it adopted a representation method where molecules wereencoded as SMILES sequences and employed as input for both pretraining and finetuning within a BERTbased model This novel approach yielded superior outcomes across various downstream molecular property prediction tasks surpassing the performance of previous models reliant on molecular fingerprints ChemBERTa 107 is also a network based on BERT which is essentially not much different from previous methods and even not the most advanced in terms of performance at that time However it emphasizes the scalability of models based on large language models and explores the impact of pretraining dataset size tokenizer and string representation which provides several directions for discussions on molecular property prediction methods based on large language models KBERT 108 similarly based on the BERT architecture distinguishes itself through the adoption of three distinct pretraining tasks in its pretraining phase atom feature prediction molecular feature prediction and contrastive learning This unique approach empowers the model to transcend the mere discovery of the SMILES paradigm and comprehend the underlying essence of SMILES representations As a result KBERT exhibits remarkable performance across 15 drug datasets showcasing its competence in the field of drug discovery It is worth noting that with the growth of data more and more molecules can be easily represented as graphs and processed through graph neural networks Given the importance of graph neural networks in the development of molecular pretraining models A BERTbased molecular pre training network will be briefly introduced to demonstrate the use of large language model variants The design of pretraining tasks greatly affects the performance of large language models Drawing inspiration from the MLM task in BERT MoleBERT 109 a BERTbased graphbased pre training neural network introduces atomlevel Masked Atoms Modeling MAM task and graph level Triplet Masked Contrastive Learning TMCL task These tasks enable the network to acquire a comprehensive understanding of the language embedded within molecular graphs By adopting this approach the network achieves exceptional performance across eight downstream data task datasets 342 Large language models for druglike molecules generation It is very difficult to chase the full coverage of the enormous druglike chemical space estimated at more than 1063 compounds and traditional virtual screening libraries usually contain less than 107 compounds and are sometimes not available In such circumstances the utilization of deep learning methods to generate molecules exhibiting druglike properties emerges as a viable approach 110 111 Inspired by the generative pretraining model GPT MolGPT 112 modelwas introduced In addition to performing the next token prediction task MolGPT incorporates an extra training task for conditional prediction facilitating the capability of conditional generation Beyond its capacity to generate innovative and efficacious molecules the model has demonstrated an enhanced ability to capture the statistical characteristics within the dataset 343 Large language models for drugtarget interaction predictions The investigation of DrugTarget Interaction DTI holds paramount significance in the realm of drug development and the optimization of drug therapy By attaining a profound comprehension of the interaction between drugs and their target proteins it offers valuable guidance for the design and development of pharmaceutical agents This expedites the drug development process and mitigates the expenditure of time and resources entailed in laboratory experimentation and trial anderror methodologies 113 114 During the exploration of DTI diligent focus is placed on the prediction of drugtarget binding affinity A proficiently trained DTI language model possesses the capability to conduct high throughput drug screening thereby expediting the drug discovery process DTIBERT employs a finetuned ProtBERT 115 model to process protein sequences while employing discrete wavelet transform for the processing of molecular fingerprints of drug molecules Subsequently by acquiring the hidden states of the corresponding pairs the ultimate outcome is achieved through concatenation and subsequent neural network processing This approach is simple and effective TransDTI 116 is a multiclass classification and regression workflow In contrast to DTIBERT this model not only uses finetuned SMILESBERT to extract drug features but also expands the selection of finetuned large protein models After acquiring potential representations of drug target pairs the authors subject the representations to downstream neural networks for the completion of a multiclassification task Additionally the paper employs molecular docking and dynamic analysis as means of verifying the models predictions Hyeunseok Kang et al did similar work using different pretrained models in 2022 117 The ChemicalChemical ProteinProtein Transferred DTA C2P2 method uses pretrained protein and molecular large language models to capture the interaction information within molecules similar to previous methods Given the relatively limited scale of the DTI dataset C2P2 leverages proteinprotein interaction PPI and chemicalchemical interaction CCI tasks to acquire knowledge of intermolecular interactions and subsequently transfer this knowledge to affinity prediction tasks The incorporation of this trainingframework undeniably enhances the networks ability to predict the binding affinity between the two molecules as evidenced by the experimental outcomes 118 It is worth highlighting that in scenarios involving the docking or when emphasizing the spatial structure of a complex methodologies incorporating 3D convolution networks point cloudsbased networks and graph networks are often employed 119122 Although these methods can better capture the intermolecular interactions they inevitably consume more computing resources In situations where the molecular structure is unknown but the sequence is available the prediction of DTI using largescale models still holds significant promise 344 Large language models for drug synergistic effects predictions Combination therapy is common for complex diseases like cancer infections and neurological disorders often surpassing singledrug treatments Predicting drug pair synergy where combining drugs boosts therapeutic effects is vital in drug development However its challenging due to many drug combinations and complex biology 123 124 Various computational methods including machine learning help predict drug pair synergy Wei Zhang et al 125 introduced DCEDForest a model for predicting drug combination synergies It uses a pretrained drug BERT model to encode the drug SMILES and then predicts synergistic effects based on the embedding vectors of drugs and cell lines using the deep forest method Mengdie Xua et al 126 utilized a finetuned pretrained large language model and a dual feature fusion mechanism to predict synergistic drug combinations Its input includes hashed atom pair molecular fingerprints of drugs SMILES string encodings and cell line gene expressions They conducted ablation analyses on the dual feature fusion network for drugdrug synergy prediction highlighting the significant role of fingerprint inputs in ensuring highquality drug synergy predictions 35 Applications of large language models in single cell analysis The emergence of singlecell RNA sequencing scRNAseq has marked the onset of a revolutionary era in genomics and biomedical research Unlike traditional bulk RNA sequencing methods scRNAseq allows us to delve into the intricacies of gene expression at singlecell resolution offering unprecedented insights and paving the way for numerous groundbreaking advancements 127130 One of the most significant changes brought about by scRNAseq is itsability to uncover cellular heterogeneity within tissues and organisms It facilitates the identification and recognition of diverse cell types subpopulations and rare cell states that were previously concealed in bulk measurements As we discussed in the previous sections large language models have found successful applications in the domains of genomics transcriptomics proteomics and drug discovery In this section our attention turns to singlecell language models that can be employed for various downstream tasks in singlecell analysis These tasks include identifying cell types and states discovering novel cell populations inferring gene regulation networks and integrating singlecell multiomics scMultiomics data among others Figure 7 Table 4 Supplementary Table 4 351 Singlecell language models for cell clustering based on scRNAseq data Cell clustering for singlecell RNA sequencing scRNAseq is crucial steps in deciphering the complex landscape of cellular heterogeneity within biological samples 131136 Singlecell clustering aims to group individual cells into clusters according to their gene expression profiles Traditional single cell clustering methods mainly use clustering algorithms such as hierarchical clustering kmeans algorithm neural networkbased methods etc to divide cells into clusters Large language models enable cell clustering on large amount of scRNAseq data from different tissues species organs sequencing technologies platforms etc For example tGPT 137 is a generative pretraining model from transcriptomes that requires input genes ranked in descending order of their expression tGPT can be trained to learn the feature representation of scRNAseq based on highexpressed genes The learned feature representation is applied to cell clustering on atlasscale data including Human Cell Atlas HCA 138 Human Cell Landscape HCL 139 Tabula Muris 136 and Macaque Retina 140 datasets using Leiden algorithm 141 scFoundation 142 is the currently largest large language model in singlecell field featuring 100 million parameters across 50 million gene expression profiles scFoundation introduces a novel pretraining task known as readdepthaware RDA modeling based on Bayesian down sampling which selects genes of a low readdepth variant in the same cell instead of directly using neighbor genes to predict masked genes scFoundation has a transformerbased encoderdecoder structure Only nonzero and nonmasked gene were fed into the encoder to learn cell embedding for cell clustering 352 Singlecell language models for cell type annotation based on scRNAseq dataSinglecell annotation focuses on assigning biological labels typically cell type or cell state to individual cells or clusters It plays a pivotal role in understanding cell function identifying diseasespecific cell populations and unraveling the intricacies of tissue development and homeostasis However annotating cell types in singlecell RNA sequencing data is very challenging due to the high levels of noise dropout events and batch effects inherent in scRNA seq data The remarkable success of large language models in natural language processing and computer vision opens new avenues for addressing cell type annotation in singlecell RNA sequencing data Currently there have been emerging some computational tools utilizing large language models for cell type annotation using scRNAseq data such as CIForm 143 TOSICA 144 scTransSort 145 TransCluster 146 scBERT 147 and scGPT 148 CIForm 143 is designed for cell type annotation on largescale datasets with multiple reference data based on transformer After a transformer encoder to learn the cell embeddings a multilayer neural networkbased classifier is employed in CIForm to predict the cell types of single cells CIForm was evaluated on both intradatasets and interdatasets considering the annotation on different species organs tissues and technologies reference and query data from different sequencing platforms or studies which we refer to batch size effect in single cell analysis and even multi reference data from different sources TOSICA 144 developed an interpretable cell type annotation method that converts gene tokens into pathwayregulons tokens by adding a knowledgebased mask matrix from GSEA to the fully connected weight matrix in the gene embedding step Only highly variable genes are used as input in TOSICA and the class token CLS in the output of the transformer is utilized to predict the cell type probabilities using the whole conjunction neural network cell type classifier In addition to interpretable cell type annotation TOSICA can also discover new cell types perform interpretable trajectory analysis and be immune to batch effects in scRNAseq data scTransSort 145 and TransCluster 146 were designed by the same author team scTransSort 145 proposed a gene patch embedding that uses CNN to generate a sequence of flattened 2D gene embedding patches to alleviate the problem of scRNA seq data sparsity and avoid using HVGs Positional embedding representing the relative positions between genes is added to each patch and then passed through a transformer consisting of a multi head selfattention mechanism and a fully connected feedforward to obtain the learned cell embedding and then a linear classifier is used for supervised cell type classification training The transformer structure in TransCluster 146 included both selfattentionbased encoder anddecoder combined with a onedimensional CNN to extract features from input The linear classifier is the final step to conduct cell type annotation scBERT 147 acquires a broad understanding of the syntax of genegene interactions during the pretraining phase aiming to eliminate batch effects across datasets and enhance generalizability In the finetuning step models parameters guided by reference datasets are retained since a classifier is added to the pre trained performer The excellent design of scBERT lies in giving up the utilization of HVGs and dimensionality reduction Instead scBERT replaces the transformer encoder employed in BERT with Performer 149 to enhance the models scalability Consequently scBERT enables the unbiased datadriven discovery of gene expression patterns and longerrange dependencies for cell type annotation scGPT 148 was developed based on a generative pretrained foundation model to learn cell and gene representations from a variety of singlecell data after HVG selection In the pretraining step scGPT employs stacked transformer layers with multihead attention enabling the simultaneous learning of cell and gene embeddings scGPT was trained in an autoregressive manner via zeroshot learning initially generating gene expression values from cell embeddings and then gradually learning to generate gene expression of cells by leveraging existing knowledge of gene expressions During the finetuning step scGPT used a supervised model to annotate unknown cells from their cell representation by introducing a multilayer perceptronbased MLPbased classifier to the generative pretrained foundation model Notably scGPT employed special tokens batch tokens and modality tokens to eliminate batch effects and modality differences in the pretraining singlecell reference datasets to improve the annotation accuracy 353 Singlecell language models for gene function analysis based on scRNAseq data In addition to celllevel tasks such as cell clustering and cell type annotation the attention mechanism in the transformer can learn the relationship between genes and the transformer can output the learned gene embedding after pretraining and finetuning which can be used for gene function analysis for scRNAseq data As mentioned in section 352 scGPT 148 is a generalizable feature extractor based on zeroshot learning that enables scGPT to be applied to gene expression prediction and genetic perturbation prediction The attention matrix learned by scGPT is used to infer gene regulation network Similar to scGPT scFoundation 142 is a foundation model that can learn both cell representation and gene representation Zeroexpressed genes and masked genes are combined with the the output from the transformerbased encoderThis combined information is then input into the decoder and projected to gene expression values through a multilayer perceptron MLP The gene context expression is employed to formulate a cellspecific gene graph facilitating the prediction of perturbations using the GEARS 150 model It is worth mentioning that another large language model Geneformer 151 is pretrained on a vast scale of singlecell transcriptomes All genes of each cell are reordered according to their gene expression and input into the transformer for training Subsequently it undergoes finetuning for diverse downstream tasks encompassing the prediction of dosagesensitive disease genes and downstream targets forecasting chromatin dynamics and anticipating network dynamics This finetuning process leverages the pretrained weights transferred to the taskspecific models with limited data 354 Singlecell language models for singlecell multiomics data Studying singlecell multiomics data involves integrating information from different omics technologies eg genomics transcriptomics epigenomics and proteomics at the singlecell level which has multiple advantages compared to studying singleomics data types The adaptability generalization capabilities and feature extraction abilities of large language models make them valuable tools to find solutions for featurevariance sparsity and cell heterogeneity that scMuti omics data suffers from A crucial phase in the analysis of singlecell multiomics data involves the integration of such diverse datasets scGPT 148 utilizes supplementary sets of tokens to signify distinct sequencing modalities in the context of scMultiomics integration tasks The modality tokens are associated with input features such as genes regions and proteins They are added to the transformer output either at the feature or cell level before proceeding with specific finetuning objectives This deliberate inclusion helps prevent the transformer from amplifying attention within features of the same modalities while simultaneously downplaying the significance of those associated with different modalities scMVP 152 is designed specific for integration of paired singlecell RNAseq and ATACseq data where gene expression and chromatin accessibility are in the same cell 153156 scMVP uses mask attentionbased scRNA encoders and transformer multihead selfattentionbased scATAC encoders to project scRNAseq and scATACseq into latent space The distribution of latent embedding representing the joint profiling of scRNA and scATAC is the GMM distribution A cell typeguided attention module calculates correlations between scRNA and scATAC in the same cell Subsequently scRNAseq and scATACseq data are reconstructed and imputed by learning parameters of the negativebinomial NB and zeroinflated Poisson ZIP distributions using a twochannel decoder network with a structure similar to the encoder network DeepMAPS 157 is a graph transformerbased method but it is designed for data integration and inference of biological networks from scMulti omics data encompassing scRNAseq scATACseq and CITEseq data The graph constructed by DeepMAPS consists of nodes for genes and cells so all other modalities should map their features to genes The transformer in DeepMAPS aims to learn both local and global features to build cellcell and genegene relations combined with RNA velocity The cellcell relation can further be used to infer cellcell communications In recent years advancements in technology have enabled the concurrent characterization of different modalities in the same cell This progress has given rise to computational tools capable of predicting one modality from another One such tool is scTranslator 158 which translates singlecell transcriptome to proteome scTranslator is pretrained on both paired bulk data and paired singlecell data then it is finetuned to infer protein abundance from scRNAseq data by minimizing the mean squared error MSE loss between predicted and actual proteins The learned attention matrix is applied to infer integrative genegene proteinprotein and geneprotein regulatory Another method called scMoFormer 159 can not only translate gene expression to protein abundance but is also applicable to multiomics predictions including protein abundance to gene expression chromatin accessibility to gene expression gene expression to chromatin accessibility using graph transformers Taking protein prediction task as an example scMoFormer constructs cellgene graph genegene graph proteinprotein graph and geneprotein graph based on gene expression profiles and prior knowledge from STRING database 160 Each modality has a separate transformer to learn the global information that may not be included in prior knowledge Messagepassing graph neural networks GNNs link nodes across various graphs while transformers are employed to precisely map gene expression to protein abundance 4 Conclusion Pretrained large language models have been used in multiple biological tasks In this review we discuss the applications of LLMs in genomics transcriptomics proteomics singlecell analysis and drug discovery As discussed above LLMs can learn the DNA and RNA sequencing pattern to predict DNA and RNAbased modification and regulation LLMs can be pretrained using proteins to achieve protein structure prediction protein generation protein function annotationand protein interaction prediction LLMs can learn cell and gene embeddings from scRNAseq and scMultiomics data to annotate cell types integrate datasets and predict generelated functional analysis LLMs can be trained to predict molecular properties or generate molecules based on molecular scaffolds and specified molecular properties predict interactions between drugs and targets as well as drug synergies In genomics and transcriptomics large language models have been used in plenty of biological tasks Currently DNA and RNA sequences are recognized as similar languages existing works have largely hinged on kmer fixedlength permutations of A G C and TU as the token of the genome language due to its simplicity LLMs were asked to learn the complex statistical properties of existing biological systems the pretrained foundational LLM models have made significant strides in this area for effectively addressing diverse categories of downstream tasks DNABERT is a DNA pretrained on DNA languages however some RNA languages use DNABERT instead of RNA foundation models to train the RNA sequences for specific biological tasks For example M6ABERTStacking serves as a tissuespecific predictor designed to identify RNA N6 methyladenosine sites utilizing DNABERT and a Stacking Strategy In its approach M6ABERT Stacking utilizes pretrained DNABERT and finetuned DNABERT attention models Notably it has been observed that DNABERT effectively directs attention to critical regions within known m6A sites and captures informative feature representations from input sequences Whats more some biological tasks can be predicted from DNA or RNA sequences Like DNABERT and DNABERT2 can predict the splice site from the reference genome however SpliceBERT was trained in the RNA sequence of RNAseq and predicted the splice site of specific premRNA Since DNA and RNA sequences both consist of four letters their foundational sequences language model might be able to be used in both DNA and RNArelated tasks In summary todays LLMs are sufficiently advanced to model molecular biology However we expect it to learn onestep causality relationships which can be learned from the correlations across modalities such as DNA variation and mRNA abundance In proteomics the protein language models take sequence information as input and produce fine tuned output results tailored to different downstream tasks associated with proteins In terms of the models parameter magnitude the large parameter quantity of the language model establishes a solid foundation for effectively addressing diverse categories of downstream tasks Nonetheless the excessive parameters often pose deployment challenges for ordinary researchers One approachinvolves utilizing large models online akin to our daily use of GPT albeit at the expense of additional deployment costs Another method entails employing knowledge distillation or other algorithms to capture the refined knowledge that researchers deem valuable from the large model Considering the models input perspective the most basic protein language model is limited to handling inputs such as MSA and protein sequences However incorporating other modalities of information such as 3D structural information of proteins introduces challenges in training and utilizing large language models One approach involves transforming the additional modalities of information into a sequencebased format necessitating careful consideration of the effectiveness and rationality of such information conversion across modalities Another method involves integrating other large models to collectively capture the multimodal information pertaining to proteins This approach requires consideration of multimodal fusion techniques fusion timing and related concerns The increase of diverse types of proteinrelated data has driven the protein language models to evolve from an independent model to an integral component of larger protein models In computeraided drug discovery the key steps include processes such as docking scoring and screening When employing large language models it becomes feasible to solely consider the sequence information of molecules leading to heightened prediction efficiency However the absence of spatial structural information significantly impacts prediction accuracy Therefore the use of deep learning for drug discovery is more based on structure which means that the sequence features extracted by large language models are usually input as prior knowledge into the network Drawing inspiration from the construction of other large language models in the field of bioinformatics it is also plausible to construct large models based on molecular structure such as large graph models to predict binding affinity based on the CrossDocked2020 dataset Additionally the experience gained from the prediction of PPI and CCI can be transferred to the prediction of DTI using techniques like transfer learning Computeraided drug discovery also involves molecular generation The generation of drug molecules needs to consider properties such as effectiveness novelty and druglikeness While existing methods do incorporate these properties to some extent there remains a dearth of extensive research on the generated molecules as well as a scarcity of practical chemical or biological experimental verification to support their viabilityIn singlecell analysis large language models are pretrained on a large scale of gene expression to apply to celllevel and genelevel downstream tasks There are many differences from NLP in singlecell applications of LLMs First scRNAseq data suffers from high sparsity which is also one of the difficulties to be solved in large language models applied to singlecell RNAseq data CIForm TOSICA TransCluster and scGPT select highly variable genes as input to alleviate the problem of scRNAseq data sparsity and reduce the training burden of large language models Second gene expression differs from human language and sequence data it is continuous values and does not have orders between gene tokens Thus how to define the genes position is a critical problem of LLMs in singlecell analysis tGPT and Geneformer sort genes from high to low according to their gene expression CIForm and TransCluster apply sine and cosine functions to determine the position information scBERT uses gene2vec to transfer gene expression to discrete values Third batch effects can pose a significant challenge in cell type clustering annotation and integration especially when the input comprises multiple datasets from distinct sequencing batches or technologies The generalizability of large language models can adapt to different biological datasets and applications making them versatile tools to be applied to various single cell datasets It is worth mentioning that the combination of Graph Neural Networks GNNs and transformers has brought about significant advancements and meaningful contributions to the field of singlecell analysis such as scMoFormer uses graph transformers to construct cellgene graph genegene graph proteinprotein graph and geneprotein graph for multiomics predictions DeepMAPS constructs a graph of cells and genes and uses graph transformer to estimate the importance of genes to cells The combination of GNNs and Transformers allows for a more comprehensive representation of the intricate relationships and dependencies present in singlecell data GNNs excel at capturing local interactions within cellular neighborhoods while Transformers effectively capture longrange dependencies This synergy enables a holistic understanding of the cellular landscape leading to improved feature learning In summary large language models excel at extracting meaningful features from raw data in singlecell analysis They can learn representations of gene expression patterns cell types and other relevant information from the data even without prior domainspecific knowledge In conclusion todays large language models have reached a level of sophistication that enables them to effectively model the intricacies of molecular biology With continuous advancements in singlecell technologies and the expanding landscape of omics sciences including proteomicsmetabolomics lipidomics and otheromic assays there is a growing capacity for conducting increasingly detailed and efficient measurements These developments contribute to our ability to unravel the complexities inherent in the diverse molecular layers spanning from DNA to the intricacies of human physiology As we delve deeper into the realms of these cuttingedge technologies we unlock new insights that pave the way for a more comprehensive understanding of the dynamic interplay within the molecular landscape Acknowledgements We would like to express our gratitude to our colleagues and friends who provided invaluable advice and support throughout the duration of this study Funding This work was partially supported by the National Institutes of Health R01GM123037 U01AR06939501A1 R01CA241930 to XZ and the National Science Foundation 2217515 2326879 to XZ MY was supported by the China Postdoctoral Science Foundation 2022M712900 2023T160590 The funders had no role in study design data collection and analysis decision to publish or preparation of the manuscript Funding for open access charge Dr Mrs Carl V Vartian Chair Professorship Funds to Dr Zhou from the University of Texas Health Science Center at Houston Conflict of interest statement None declared References 1 Radford A et al Improving language understanding by generative pretraining 2018 2 Devlin J et al Bert Pretraining of deep bidirectional transformers for language understanding arXiv preprint arXiv181004805 2018 3 Vaswani A et al Attention is all you need Advances in neural information processing systems 2017 30 4 Sarkar S Decoding coding Information and DNA BioScience 1996 4611 p 857864 5 Ji Y et al DNABERT pretrained Bidirectional Encoder Representations from Transformers model for DNAlanguage in genome Bioinformatics 2021 3715 p 21122120 6 Zhou Z et al Dnabert2 Efficient foundation model and benchmark for multispecies genome arXiv preprint arXiv230615006 2023 7 Sanabria M J Hirsch and AR Poetsch The human genomes vocabulary as proposed by the DNA language model GROVER bioRxiv 2023 p 202307 195496778 DallaTorre H et al The nucleotide transformer Building and evaluating robust foundation models for human genomics bioRxiv 2023 p 202301 11523679 9 Sinden RR and RD Wells DNA structure mutations and human genetic disease Current opinion in biotechnology 1992 36 p 612622 10 Benegas G SS Batra and YS Song DNA language models are powerful zeroshot predictors of genomewide variant effects bioRxiv 2022 p 202208 22504706 11 Wittkopp PJ and G Kalay Cisregulatory elements molecular mechanisms and evolutionary processes underlying divergence Nature Reviews Genetics 2012 131 p 5969 12 Yella VR A Kumar and M Bansal Identification of putative promoters in 48 eukaryotic genomes on the basis of DNA free energy Scientific reports 2018 81 p 4520 13 Le NQK et al BERTPromoter An improved sequencebased predictor of DNA promoter using BERT pretrained model and SHAP feature selection Computational Biology and Chemistry 2022 99 p 107732 14 Claringbould A and JB Zaugg Enhancers in disease molecular basis and emerging treatment strategies Trends in Molecular Medicine 2021 2711 p 10601073 15 Nasser J et al Genomewide enhancer maps link risk variants to disease genes Nature 2021 5937858 p 238243 16 Luo H et al iEnhancerBERT A novel transfer learning architecture based on DNALanguage model for identifying enhancers and their strength in International Conference on Intelligent Computing 2022 Springer 17 Ferraz RAC et al DNAprotein interaction studies a historical and comparative analysis Plant Methods 2021 171 p 121 18 Luo H et al Improving language model of human genome for DNAprotein binding prediction based on taskspecific pretraining Interdisciplinary Sciences Computational Life Sciences 2023 151 p 3243 19 An W et al MoDNA motiforiented pretraining for DNA language model in Proceedings of the 13th ACM International Conference on Bioinformatics Computational Biology and Health Informatics 2022 20 Moore LD T Le and G Fan DNA methylation and its basic function Neuropsychopharmacology 2013 381 p 2338 21 Zhang L et al Comprehensive analysis of DNA 5methylcytosine and N6adenine methylation by nanopore sequencing in hepatocellular carcinoma Frontiers in cell and developmental biology 2022 10 p 827391 22 Tsukiyama S et al BERT6mA prediction of DNA N6methyladenine site using deep learning based approaches Briefings in Bioinformatics 2022 232 p bbac053 23 Yu Y et al iDNAABT advanced deep learning model for detecting DNA methylation with adaptive features and transductive information maximization Bioinformatics 2021 3724 p 46034610 24 Jin J et al iDNAABF multiscale deep biological language learning model for the interpretable prediction of DNA methylations Genome biology 2022 231 p 123 25 Zeng W A Gautam and DH Huson MuLanMethylMultiple Transformerbased Language Models for Accurate DNA Methylation Prediction bioRxiv 2023 p 202301 04522704 26 Sanh V et al DistilBERT a distilled version of BERT smaller faster cheaper and lighter arXiv preprint arXiv191001108 2019 27 Lan Z et al Albert A lite bert for selfsupervised learning of language representations arXiv preprint arXiv190911942 2019 28 Yang Z et al Xlnet Generalized autoregressive pretraining for language understanding Advances in neural information processing systems 2019 32 29 Clark K et al Electra Pretraining text encoders as discriminators rather than generators arXiv preprint arXiv200310555 202030 Wilkinson ME C Charenton and K Nagai RNA splicing by the spliceosome Annual review of biochemistry 2020 89 p 359388 31 Chen J et al Interpretable RNA foundation model from unannotated data for highly accurate RNA structure and function predictions bioRxiv 2022 p 202208 06503062 32 Zhang Y et al Multiple sequencealignmentbased RNA language model and its application to structural inference bioRxiv 2023 p 202303 15532863 33 Zhang J et al Advances and opportunities in RNA structure experimental determination and computational modeling Nature Methods 2022 1910 p 11931207 34 Kalicki CH and ED Haritaoglu RNABERT RNA Family Classification and Secondary Structure Prediction with BERT pretrained on RNA sequences 35 Chen K et al Selfsupervised learning on millions of premRNA sequences improves sequence based RNA splicing prediction bioRxiv 2023 p 202301 31526427 36 Malbec L et al Dynamic methylome of internal mRNA N 7methylguanosine and its regulatory role in translation Cell research 2019 2911 p 927941 37 Feng H et al LncCat An ORF attention model to identify LncRNA based on ensemble learning strategy and fused sequence information Computational and Structural Biotechnology Journal 2023 21 p 14331447 38 Prokhorenkova L et al CatBoost unbiased boosting with categorical features Advances in neural information processing systems 2018 31 39 Yamada K and M Hamada Prediction of RNAprotein interactions using a nucleotide language model Bioinformatics Advances 2022 21 p vbac023 40 Zhang L et al BERTm7G a transformer architecture based on BERT and stacking ensemble to identify RNA N7Methylguanosine sites from sequence information Computational and Mathematical Methods in Medicine 2021 2021 41 Gibb EA CJ Brown and WL Lam The functional role of long noncoding RNA in human carcinomas Molecular cancer 2011 101 p 117 42 Soylu NN and E Sefer BERT2OME Prediction of 2Omethylation Modifications from RNA Sequence by Transformer Architecture Based on BERT IEEEACM Transactions on Computational Biology and Bioinformatics 2023 43 Pardi N et al mRNA vaccinesa new era in vaccinology Nature reviews Drug discovery 2018 174 p 261279 44 Babjac AN Z Lu and SJ Emrich CodonBERT Using BERT for Sentiment Analysis to Better Predict Genes with Low Expression in Proceedings of the 14th ACM International Conference on Bioinformatics Computational Biology and Health Informatics 2023 45 Gong H et al Integrated mRNA sequence optimization using deep learning Brief Bioinform 2023 241 46 Ding W K Nakai and H Gong Protein design via deep learning Briefings in bioinformatics 2022 233 p bbac102 47 Qiu Y and GW Wei Artificial intelligenceaided protein engineering from topological data analysis to deep protein language models arXiv preprint arXiv230714587 2023 48 Kovaltsuk A et al Observed antibody space a resource for data mining nextgeneration sequencing of antibody repertoires The Journal of Immunology 2018 2018 p 25022509 49 Schauperl M and RA Denny AIbased protein structure prediction in drug discovery impacts and challenges Journal of Chemical Information and Modeling 2022 6213 p 31423156 50 David A et al The AlphaFold database of protein structures a biologists guide Journal of molecular biology 2022 4342 p 167336 51 Rao RM et al MSA transformer in International Conference on Machine Learning 2021 52 Elnaggar A et al ProtTrans Towards Cracking the Language of Lifes Code Through Self Supervised Deep Learning and High Performance Computing IEEE Transactions on Pattern Analysis and Machine Intelligence 2021 p 1153 Dai Z et al Transformerxl Attentive language models beyond a fixedlength context arXiv preprint arXiv190102860 2019 54 Raffel C et al Exploring the limits of transfer learning with a unified texttotext transformer The Journal of Machine Learning Research 2020 211 p 54855551 55 UniProt the universal protein knowledgebase in 2021 Nucleic acids research 2021 49D1 p D480D489 56 Steinegger M and J Sding Clustering huge protein sequence sets in linear time Nature communications 2018 91 p 2542 57 Steinegger M M Mirdita and J Sding Proteinlevel assembly increases protein sequence recovery from metagenomic samples manyfold Nature methods 2019 167 p 603606 58 Strokach A and PM Kim Deep generative modeling for protein design Current opinion in structural biology 2022 72 p 226236 59 Ferruz N and B Hcker Controllable protein design with language models Nature Machine Intelligence 2022 46 p 521532 60 Madani A et al Progen Language modeling for protein generation arXiv preprint arXiv200403497 2020 61 Ferruz N S Schmidt and B Hcker ProtGPT2 is a deep unsupervised language model for protein design Nature communications 2022 131 p 4348 62 Mirdita M et al ColabFold making protein folding accessible to all Nature methods 2022 196 p 679682 63 Jumper J et al Highly accurate protein structure prediction with AlphaFold Nature 2021 5967873 p 583589 64 Zhou X et al ITASSERMTD a deeplearningbased platform for multidomain protein structure and function prediction Nature Protocols 2022 1710 p 23262353 65 Ferruz N et al From sequence to function through structure Deep learning for protein design Computational and Structural Biotechnology Journal 2023 21 p 238250 66 Wu L et al SPRoBERTa protein embedding learning with local fragment modeling Briefings in Bioinformatics 2022 236 p bbac401 67 Xu M et al Protst Multimodality learning of protein sequences and biomedical texts arXiv preprint arXiv230112040 2023 68 Wang H et al Protein posttranslational modifications in the regulation of cancer hallmarks Cancer Gene Therapy 2023 304 p 529547 69 de Brevern AG and J Rebehmed Current status of PTMs structural databases applications limitations and prospects Amino Acids 2022 544 p 575590 70 Brandes N et al ProteinBERT a universal deeplearning model of protein sequence and function Bioinformatics 2022 388 p 21022110 71 Savino S T Desmet and J Franceus Insertions and deletions in protein evolution and engineering Biotechnology Advances 2022 60 p 108010 72 Horne J and D Shukla Recent advances in machine learning variant effect prediction tools for protein engineering Industrial engineering chemistry research 2022 6119 p 62356245 73 Asgari E and MRK Mofrad Continuous distributed representation of biological sequences for deep proteomics and genomics PloS one 2015 1011 p e0141287 74 Suzek BE et al UniRef clusters a comprehensive and scalable alternative for improving sequence similarity searches Bioinformatics 2015 316 p 926932 75 Alley EC et al Unified rational protein engineering with sequencebased deep representation learning Nature methods 2019 1612 p 13151322 76 Rives A et al Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences bioRxiv 2019 77 Pucci F M Schwersensky and M Rooman Artificial intelligence challenges for predicting the impact of mutations on protein stability Current opinion in structural biology 2022 72 p 161 16878 Rao R et al Evaluating protein transfer learning with TAPE Advances in neural information processing systems 2019 32 79 Wang Z et al Multilevel Protein Structure Pretraining via Prompt Learning in The Eleventh International Conference on Learning Representations 2022 80 Tang T et al Machine learning on proteinprotein interaction prediction models challenges and trends Briefings in Bioinformatics 2023 242 p bbad076 81 Durham J et al Recent advances in predicting and modeling proteinprotein interactions Trends in biochemical sciences 2023 82 Zhou HY et al Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling bioRxiv 2023 p 202301 83 Zhang N et al Ontoprotein Protein pretraining with gene ontology embedding arXiv preprint arXiv220111147 2022 84 Janeway C et al Immunobiology the immune system in health and disease Vol 2 2001 Garland Pub New York 85 Peters B M Nielsen and AJARoI Sette T cell epitope predictions 2020 38 p 123145 86 ODonnell TJ A Rubinsteyn and UJCs Laserson MHCflurry 20 improved panallele prediction of MHC class Ipresented peptides by incorporating antigen processing 2020 111 p 4248 e7 87 Wang F et al MHCRoBERTa panspecific peptideMHC class I binding prediction through transfer learning with labelagnostic protein sequences Brief Bioinform 2022 233 88 Cheng J et al BERTMHC improved MHCpeptide class II interaction prediction with transformer and multiple instance learning Bioinformatics 2021 3722 p 41724179 89 Greiff V G Yaari and LGJCOiSB Cowell Mining adaptive immune receptor repertoires for biological and clinical information using machine learning 2020 24 p 109119 90 Wu K et al TCRBERT learning the grammar of Tcell receptors for flexible antigenbinding analyses 2021 91 Zhao Y et al SCAIRBERT a pretrained singlecell model for predicting the antigenbinding specificity of the adaptive immune receptor Brief Bioinform 2023 244 92 Davies DR EA Padlan and SJArob Sheriff Antibodyantigen complexes 1990 591 p 439473 93 Olsen TH IH Moal and CM Deane AbLang an antibody language model for completing antibody sequences Bioinformatics Advances 2022 21 p vbac046 94 Liu Y et al Roberta A robustly optimized bert pretraining approach arXiv preprint arXiv190711692 2019 95 Giudicelli V D Chaume and MP Lefranc IMGTGENEDB a comprehensive database for human and mouse immunoglobulin and T cell receptor genes Nucleic acids research 2005 33suppl1 p D256D261 96 Leem J et al Deciphering the language of antibodies using selfsupervised learning Patterns 2022 37 97 Wang D F Ye and H Zhou On pretrained language models for antibody bioRxiv 2023 p 202301 98 Askr H et al Deep learning in drug discovery an integrative review and future challenges Artificial Intelligence Review 2023 567 p 59756037 99 Xiaobo Z and STC Wong High content cellular imaging for drug development IEEE Signal Processing Magazine 2006 232 p 170174 100 Sun X et al Multiscale agentbased brain cancer modeling and prediction of TKI treatment response incorporating EGFR signaling pathway and angiogenesis BMC Bioinformatics 2012 13 p 218 101 Vargason AM AC Anselmo and SJNbe Mitragotri The evolution of commercial drug delivery technologies 2021 59 p 951967102 Leeson PD and BJNrDd Springthorpe The influence of druglike concepts on decision making in medicinal chemistry 2007 611 p 881890 103 zelik R et al Structurebased Drug discovery with Deep Learning ChemBioChem 2022 p e202200776 104 Li Z et al Deep learning methods for molecular representation and property prediction Drug Discovery Today 2022 p 103373 105 Chen W et al Artificial intelligence for drug discovery Resources methods and applications Molecular TherapyNucleic Acids 2023 106 Wang S et al Smilesbert large scale unsupervised pretraining for molecular property prediction in Proceedings of the 10th ACM international conference on bioinformatics computational biology and health informatics 2019 107 Chithrananda S G Grand and B Ramsundar ChemBERTa largescale selfsupervised pretraining for molecular property prediction arXiv preprint arXiv201009885 2020 108 Wu Z et al Knowledgebased BERT a method to extract molecular features like computational chemists Briefings in Bioinformatics 2022 233 p bbac131 109 Xia J et al Molebert Rethinking pretraining graph neural networks for molecules in The Eleventh International Conference on Learning Representations 2022 110 Bilodeau C et al Generative models for molecular discovery Recent advances and challenges Wiley Interdisciplinary Reviews Computational Molecular Science 2022 125 p e1608 111 Meyers J B Fabian and N Brown De novo molecular design and generative models Drug Discovery Today 2021 2611 p 27072715 112 Bagal V et al MolGPT molecular generation using a transformerdecoder model Journal of Chemical Information and Modeling 2021 629 p 20642076 113 Abbasi K et al Deep learning in drug target interaction prediction current and future perspectives Current Medicinal Chemistry 2021 2811 p 21002113 114 Zhang Z et al Graph neural network approaches for drugtarget interactions Current Opinion in Structural Biology 2022 73 p 102327 115 Zheng J X Xiao and WR Qiu DTIBERT identifying drugtarget interactions in cellular networking based on BERT and deep learning method Frontiers in Genetics 2022 13 p 859188 116 Kalakoti Y S Yadav and D Sundar TransDTI transformerbased language models for estimating DTIs and building a drug recommendation workflow ACS omega 2022 73 p 2706 2717 117 Kang H et al Finetuning of bert model to accurately predict drugtarget interactions Pharmaceutics 2022 148 p 1710 118 Nguyen TM T Nguyen and T Tran Mitigating coldstart problems in drugtarget affinity prediction with interaction knowledge transferring Briefings in Bioinformatics 2022 234 p bbac269 119 Ragoza M et al Proteinligand scoring with convolutional neural networks Journal of chemical information and modeling 2017 574 p 942957 120 Li S et al Structureaware interactive graph neural networks for the prediction of proteinligand binding affinity in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining 2021 121 Jiang D et al InteractionGraphNet a novel and efficient deep graph representation learning framework for accurate proteinligand interaction predictions Journal of medicinal chemistry 2021 6424 p 1820918232 122 Wang Y et al A point cloudbased deep learning strategy for proteinligand binding affinity prediction Briefings in Bioinformatics 2022 231 p bbab474 123 Hecht JR et al A randomized phase IIIB trial of chemotherapy bevacizumab and panitumumab compared with chemotherapy and bevacizumab alone for metastatic colorectal cancer 2009 275 p 672680124 Tol J et al Chemotherapy bevacizumab and cetuximab in metastatic colorectal cancer 2009 3606 p 563572 125 Zhang W et al DCEDForest a deep forest model for the prediction of anticancer drug combination effects Computational and Mathematical Methods in Medicine 2022 2022 126 Xu M et al DFFNDDS prediction of synergistic drug combinations with dual feature fusion networks Journal of Cheminformatics 2023 151 p 112 127 Kowalczyk MS et al Singlecell RNAseq reveals changes in cell cycle and differentiation programs upon aging of hematopoietic stem cells Genome Res 2015 2512 p 186072 128 Macaulay IC CP Ponting and T Voet SingleCell Multiomics Multiple Measurements from Single Cells Trends Genet 2017 332 p 155168 129 Papalexi E and R Satija Singlecell RNA sequencing to explore immune cell heterogeneity Nat Rev Immunol 2018 181 p 3545 130 Tanay A and A Regev Scaling singlecell genomics from phenomenology to mechanism Nature 2017 5417637 p 331338 131 Cao J et al Comprehensive singlecell transcriptional profiling of a multicellular organism Science 2017 3576352 p 661667 132 Cao J et al The singlecell transcriptional landscape of mammalian organogenesis Nature 2019 5667745 p 496502 133 Fincher CT et al Cell type transcriptome atlas for the planarian Schmidtea mediterranea Science 2018 3606391 134 Han X et al Mapping the Mouse Cell Atlas by MicrowellSeq Cell 2018 1725 p 10911107 e17 135 Plass M et al Cell type atlas and lineage tree of a whole complex animal by singlecell transcriptomics Science 2018 3606391 136 Tabula Muris C et al Singlecell transcriptomics of 20 mouse organs creates a Tabula Muris Nature 2018 5627727 p 367372 137 Shen H et al Generative pretraining from largescale transcriptomes for singlecell deciphering iScience 2023 265 p 106536 138 Regev A et al The human cell atlas white paper arXiv preprint arXiv181005192 2018 139 Han X et al Construction of a human cell landscape at singlecell level Nature 2020 5817808 p 303309 140 Peng YR et al Molecular Classification and Comparative Taxonomics of Foveal and Peripheral Cells in Primate Retina Cell 2019 1765 p 12221237 e22 141 Traag VA L Waltman and NJ Van Eck From Louvain to Leiden guaranteeing wellconnected communities Scientific reports 2019 91 p 5233 142 Minsheng H et al Large Scale Foundation Model on Singlecell Transcriptomics bioRxiv 2023 p 20230529542705 143 Xu J et al CIForm as a Transformerbased model for celltype annotation of largescale single cell RNAseq data Brief Bioinform 2023 244 144 Chen J et al Transformer for one stop interpretable cell type annotation Nat Commun 2023 141 p 223 145 Jiao L et al scTransSort Transformers for Intelligent Annotation of Cell Types by Gene Embeddings Biomolecules 2023 134 146 Song T et al TransCluster A CellType Identification Method for singlecell RNASeq data using deep learning based on transformer Front Genet 2022 13 p 1038919 147 Yang F et al scBERT as a largescale pretrained deep language model for cell type annotation of singlecell RNAseq data Nature Machine Intelligence 2022 410 p 852866 148 Cui H et al scGPT Towards Building a Foundation Model for SingleCell Multiomics Using Generative AI bioRxiv 2023 p 202304 30538439 149 Choromanski K et al Rethinking attention with performers arXiv preprint arXiv200914794 2020150 Roohani Y K Huang and J Leskovec GEARS Predicting transcriptional outcomes of novel multigene perturbations BioRxiv 2022 p 202207 12499735 151 Theodoris CV et al Transfer learning enables predictions in network biology Nature 2023 6187965 p 616624 152 Li G et al A deep generative model for multiview profiling of singlecell RNAseq and ATAC seq data Genome Biol 2022 231 p 20 153 Ma S et al Chromatin Potential Identified by Shared SingleCell Profiling of RNA and Chromatin Cell 2020 1834 p 11031116 e20 154 Cao J et al Joint profiling of chromatin accessibility and gene expression in thousands of single cells Science 2018 3616409 p 13801385 155 Chen S BB Lake and K Zhang Highthroughput sequencing of the transcriptome and chromatin accessibility in the same cell Nat Biotechnol 2019 3712 p 14521457 156 Zhu C et al An ultra highthroughput method for singlecell joint analysis of open chromatin and transcriptome Nat Struct Mol Biol 2019 2611 p 10631070 157 Ma A et al Singlecell biological network inference using a heterogeneous graph transformer Nat Commun 2023 141 p 964 158 Linjing L et al A pretrained large language model for translating singlecell transcriptome to proteome bioRxiv 2023 p 20230704547619 159 Tang W et al SingleCell Multimodal Prediction via Transformers arXiv preprint arXiv230300233 2023 160 Szklarczyk D et al The STRING database in 2023 proteinprotein association networks and functional enrichment analyses for any sequenced genome of interest Nucleic Acids Res 2023 51D1 p D638D646Legends of figures Figure 1 Summary of the application of large language models in bioinformatics in this review Applications of large language models in bioinformatics include applications in genomics transcriptomics proteomics drug discovery and single cell analysis Applications of LLMs in genomics focus on LLMs using DNA sequence applications of LLMs in transcriptomics using RNA sequence applications of LLMs in proteomics focus on LLMs using protein sequence applications of LLMs in drug discovery focus on LLMs using molecular data and applications of LLMs in singlecell analysis focus on LLMs using gene expression data from scRNAseq or scMultiomics data Each corresponds to a variety of biological downstream tasks Figure 2 Schematic diagram of large language model The input of large language models is tokenized and fed into embedding layers Large language models particularly exemplified by BERTbased transformer encoder and GPTbased transformer decoder models their core is attention mechanism The training process of large language models usually includes pretraining and finetuning Pretraining is mainly to train a generalfoundation model with strong generalization ability on a largescale unlabeled reference dataset using selfsupervised learning Finetuning relies on the pretrained model undergoing additional training for a specific task Figure 3 Applications of large language models in genomics The DNA language models take DNA sequence as input use transformer BERT GPT models to solve multiple biological tasks including genomewide variant effects prediction DNA cisregulatory regions prediction DNA protein interaction prediction DNA methylation 6mA4mC 5hmC prediction splice sites prediction from DNA sequence Figure 4 Applications of large language models in transcriptomics The RNA language models take RNA sequences as input use transformer BERT GPT models to solve multiple biological tasks including RNA 2D3D structure prediction RNA structural alignment RNA family clustering RNA splice sites prediction from RNA sequence RNA N7methylguanosine modification prediction RNA 2Omethylation modifications prediction multiple types of RNA modifications prediction predicting the association between miRNA lncRNA and diseaseidentifying lncRNAs lncRNAs coding potential prediction protein expression and mRNA degradation prediction Figure 5 Applications of large language models in proteomics The protein language models take multiple sequence alignment protein sequence gene ontology and proteinrelationattribute as input use transformer BERT GPT models to solve multiple biological tasks including predicting secondary structure predicting protein generation predicting protein function predicting posttranslational modifications predicting evolution and mutation predicting biophysical properties predicting proteinprotein interaction and predicting antigenreceptor or antigenantibody binding Figure 6 Applications of large language models in drug discovery The language models for drug discovery take molecular SMILES protein sequence molecular fingerprints and molecular graphs as input use transformer BERT GPT models to solve multiple biological tasks including predicting molecular properties predicting drugtarget interaction generating molecules and predicting synergistic effects Figure 7 Applications of large language models in single cell analysis The single cell language models take gene expression or single cell multiomics data as input use transformer BERT GPT models to solve multiple biological tasks including cell type annotation batch effect removal multiomics integration gene regulation network inference perturbation prediction dropout imputationFigure 1Figure 2Figure 3Figure 4Figure 5Figure 6Figure 7These are horizontal tables Table 1 Large language models for genomic and transcriptomic tasks Input data Biological tasks Models DNA sequence Genomewide variant effects prediction DNABERT DNABERT2 GPN Nucleotide Transformer DNA cisregulatory regions prediction DNABERT DNABERT2 BERTPromoter iEnhancer BERT Nucleotide Transformer DNAprotein interaction prediction DNABERT DNABERT2 TFBert GROVER and MoDNA DNA methylation 6mA4mC 5hmC prediction BERT6mA iDNAABF iDNAABT and MuLan Methyl RNA splice sites prediction from DNA sequence DNABERT DNABERT2 RNA sequence RNA 2D3D structure prediction RNAFM RNAMSM and RNAFM RNA structural alignment RNA family clustering RNABERT RNA splice sites prediction from RNA sequence SpliceBERT RNA N7Methylguanosine modification prediction BERTm7G RNA 2Omethylation Modifications prediction Bert2Ome Multiple types of RNA modifications prediction RmLR Predicting the association between miRNA lncRNA and disease BertNDA Identifying lncRNAs LncCat lncRNAs coding potential prediction LSCPPBERT Protein expression and mRNA degradation prediction CodonBERTTable 2 Large language models for proteomic tasks Input data Downstream tasks Models Protein sequences MSAs Gene ontology annotations Triplets of proteinrelation attribute Protein property descriptions Secondary structure and contact prediction MSA Transformer ProtTransSPRoBERTa TAPE KeAP Protein sequence generation ProGen ProtGPT2 Protein function prediction SPRoBERTa ProtST PromptProtein Major PTMs prediction ProteinBERT Evolution and mutation prediction SPRoBERTa UniRep ESM1b TAPE Biophysical properties prediction TAPE PromptProtein Proteinprotein interaction and binding affinity prediction KeAP AntigenRecepter binding prediction MHCRoBERTa BERTMHC TCRBERT SC AIRBERT AntigenAntibody binding prediction AbLang AntiBERTa EATLM The input of models is one or several types of protein language dataTable 3 Large language models for drug discovery tasks Input data Downstream tasks Models Molecular SMILES Predicting Molecular Properties SMILESBERT ChemBERTa KBERT Generating Molecules MolGPT Molecular graphs Predicting Molecular Properties MOLEBERT Molecular fingerprints and protein sequences Predicting DrugTarget Interaction DTIBERT Molecular SMILES and protein sequences Predicting Synergistic Effects TransDTI C2P2 Hyeunseok Kang et alTable 4 Large language models for single cell tasks Input data Downstream tasks Models scRNAseq data Cell clustering tGPT scFoundation Cell type annotation CIForm TOSICA scTransSort TransCluster scBERT scGPT Gene function analyses Gene expression prediction gene network inference gene perturbation prediction discovery of key network regulators and identifying candidate therapeutic targets scGPT scFounfation Geneformer scMutiomics data Singlecell multiomics integration scGPT scMVP DeepMAPS Biological network inference DeepMAPS Cellcell communications Data imputation scMVP Translating gene expression to protein abundance scTranslator scMoFormer Singlecell multimodal prediction scMoFormer Integrative regulatory inference scTranslator