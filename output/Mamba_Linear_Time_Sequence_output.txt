Mamba LinearTime Sequence Modeling with Selective State Spaces Albert Gu 1 and Tri Dao 2 1Machine Learning Department Carnegie Mellon University 2Department of Computer Science Princeton University agucscmuedu tritridaome Abstract Foundation models now powering most of the exciting applications in deep learning are almost universally based on the Transformer architecture and its core attention module Many subquadratictime architectures such as linear attention gated convolution and recurrent models and structured state space models SSMs have been developed to address Transformers computational ineciency on long sequences but they have not performed as well as attention on important modalities such as language We identify that a key weakness of such models is their inability to perform contentbased reasoning and make several improvements First simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token Second even though this change prevents the use of ecient convolutions we design a hardwareaware parallel algorithm in recurrent mode We integrate these selective SSMs into a simplied endtoend neural network architecture without attention or even MLP blocks Mamba Mamba enjoys fast inference 5 higher throughput than Transformers and linear scaling in sequence length and its performance improves on real data up to millionlength sequences As a general sequence model backbone Mamba achieves stateoftheart performance across several modalities such as language audio and genomics On language modeling our Mamba3B model outperforms Transformers of the same size and matches Transformers twice its size both in pretraining and downstream evaluation 1 Introduction Foundation models FMs or large models pretrained on massive data then adapted for downstream tasks have emerged as an eective paradigm in modern machine learning The backbone of these FMs are often sequence models operating on arbitrary sequences of inputs from a wide variety of domains such as language images speech audio time series and genomics Brown et al 2020 Dosovitskiy et al 2020 Ismail Fawaz et al 2019 Oord et al 2016 Poli et al 2023 Sutskever Vinyals and Quoc V Le 2014 While this concept is agnostic to a particular choice of model architecture modern FMs are predominantly based on a single type of sequence model the Transformer Vaswani et al 2017 and its core attention layer Bahdanau Cho and Bengio 2015 The ecacy of selfattention is attributed to its ability to route information densely within a context window allowing it to model complex data However this property brings fundamental drawbacks an inability to model anything outside of a nite window and quadratic scaling with respect to the window length An enormous body of research has appeared on more ecient variants of attention to overcome these drawbacks Tay Dehghani Bahri et al 2022 but often at the expense of the very properties that makes it eective As of yet none of these variants have been shown to be empirically eective at scale across domains Recently structured state space sequence models SSMs Gu Goel and R 2022 Gu Johnson Goel et al 2021 have emerged as a promising class of architectures for sequence modeling These models can be interpreted as a combination of recurrent neural networks RNNs and convolutional neural networks CNNs with inspiration from classical state space models Kalman 1960 This class of models can be computed very eciently as either a recurrence or convolution with linear or nearlinear scaling in sequence length Additionally they have principled Equal contribution 1mechanisms for modeling longrange dependencies Gu Dao et al 2020 in certain data modalities and have dominated benchmarks such as the Long Range Arena Tay Dehghani Abnar et al 2021 Many avors of SSMs Gu Goel and R 2022 Gu Gupta et al 2022 Gupta Gu and Berant 2022 Y Li et al 2023 Ma et al 2023 Orvieto et al 2023 Smith Warrington and Linderman 2023 have been successful in domains involving continuous signal data such as audio and vision Goel et al 2022 Nguyen Goel et al 2022 Saon Gupta and Cui 2023 However they have been less eective at modeling discrete and informationdense data such as text We propose a new class of selective state space models that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length Selection Mechanism First we identify a key limitation of prior models the ability to eciently select data in an inputdependent manner ie focus on or ignore particular inputs Building on intuition based on important synthetic tasks such as selective copy and induction heads we design a simple selection mechanism by parameterizing the SSM parameters based on the input This allows the model to lter out irrelevant information and remember relevant information indenitely Hardwareaware Algorithm This simple change poses a technical challenge for the computation of the model in fact all prior SSMs models must be time and inputinvariant in order to be computationally ecient We overcome this with a hardwareaware algorithm that computes the model recurrently with a scan instead of convolution but does not materialize the expanded state in order to avoid IO access between dierent levels of the GPU memory hierarchy The resulting implementation is faster than previous methods both in theory scaling linearly in sequence length compared to pseudolinear for all convolutionbased SSMs and on modern hardware up to 3 faster on A100 GPUs Architecture We simplify prior deep sequence model architectures by combining the design of prior SSM architectures Dao Fu Saab et al 2023 with the MLP block of Transformers into a single block leading to a simple and homogenous architecture design Mamba incorporating selective state spaces Selective SSMs and by extension the Mamba architecture are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences i High quality selectivity brings strong performance on dense modalities such as language and genomics ii Fast training and inference computation and memory scales linearly in sequence length during training and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements iii Long context the quality and eciency together yield performance improvements on real data up to sequence length 1M We empirically validate Mambas potential as a general sequence FM backbone in both pretraining quality and domainspecic task performance on several types of modalities and settings Synthetics On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models Mamba not only solves them easily but can extrapolate solutions indenitely long 1M tokens Audio and Genomics Mamba outperforms prior stateoftheart models such as SaShiMi Hyena and Transform ers on modeling audio waveforms and DNA sequences both in pretraining quality and downstream metrics eg reducing FID on a challenging speech generation dataset by more than half In both settings its performance improves with longer context up to millionlength sequences Language Modeling Mamba is the rst lineartime sequence model that truly achieves Transformerquality performance both in pretraining perplexity and downstream evaluations With scaling laws up to 1B parameters we show that Mamba exceeds the performance of a large range of baselines including very strong modern Transformer training recipes based on LLaMa Touvron et al 2023 Our Mamba language model has 5 generation throughput compared to Transformers of similar size and Mamba3Bs quality matches that of Transformers twice its size eg 4 points higher avg on common sense reasoning compared to Pythia3B and even exceeding Pythia7B Model code and pretrained checkpoints are opensourced at httpsgithubcomstatespacesmamba 2Project Discretize Selection Mechanism GPU SRAM GPU HBM Selective State Space Model with Hardwareaware State Expansion Figure 1 Overview Structured SSMs independently map each channel eg 5 of an input to output through a higher dimensional latent state eg 4 Prior SSMs avoid materializing this large efective state times batch size and sequence length through clever alternate computation paths requiring timeinvariance the A B C parameters are constant across time Our selection mechanism adds back inputdependent dynamics which also requires a careful hardwareaware algorithm to only materialize the expanded states in more efcient levels of the GPU memory hierarchy 2 State Space Models Structured state space sequence models S4 are a recent class of sequence models for deep learning that are broadly related to RNNs and CNNs and classical state space models They are inspired by a particular continuous system 1 that maps a 1dimensional function or sequence through an implicit latent state Concretely S4 models are dened with four parameters A B C which dene a sequencetosequence trans formation in two stages A B 1a C 1b A1 B 2a C 2b C C C 3a 3b Discretization The rst stage transforms the continuous parameters A B to discrete parameters A B through xed formulas A A and B A B where the pair is called a discretization rule Various rules can be used such as the zeroorder hold ZOH dened in equation 4 A expA B A1expA I B 4 Discretization has deep connections to continuoustime systems which can endow them with additional properties such as resolution invariance Nguyen Goel et al 2022 and automatically ensuring that the model is properly normalized Gu Johnson Timalsina et al 2023 Orvieto et al 2023 It also has connections to gating mechanisms of RNNs Gu Gulcehre et al 2020 Tallec and Ollivier 2018 which we will revisit in Section 35 However from a mechanical point of view discretization can simply be viewed as the rst step of the computation graph in the forward pass of an SSM Alternate avors of SSMs can bypass the discretization step and parameterize A B directly instead Zhang et al 2023 which may be easier to reason about Computation After the parameters have been transformed from A B C A B C the model can be computed in two ways either as a linear recurrence 2 or a global convolution 3 3Commonly the model uses the convolutional mode 3 for ecient parallelizable training where the whole input sequence is seen ahead of time and switched into recurrent mode 2 for ecient autoregressive inference where the inputs are seen one timestep at a time Linear Time Invariance LTI An important property of equations 1 to 3 is that the models dynamics are constant through time In other words A B C and consequently A B as well are xed for all timesteps This property is called linear time invariance LTI which is deeply connected to recurrence and convolutions Informally we think of LTI SSMs as being equivalent to any linear recurrence 2a or convolution 3b and use LTI as an umbrella term for these classes of models Thus far all structured SSMs have been LTI eg computed as convolutions because of fundamental eciency constraints discussed in Section 33 However a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data and our technical contributions involve removing the LTI constraint while overcoming the eciency bottlenecks Structure and Dimensions Finally we note that structured SSMs are so named because computing them eciently also requires imposing structure on the A matrix The most popular form of structure is diagonal Gu Gupta et al 2022 Gupta Gu and Berant 2022 Smith Warrington and Linderman 2023 which we also use In this case the A B 1 C 1 matrices can all be represented by numbers To operate over an input sequence of batch size and length with channels the SSM is applied independently to each channel Note that in this case the total hidden state has dimension per input and computing it over the sequence length requires time and memory this is the root of the fundamental eciency bottleneck addressed in Section 33 General State Space Models We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state It has been used to refer to many disparate concepts in dierent disciplines including Markov decision processes MDP reinforcement learning Hafner et al 2020 dynamic causal modeling DCM computational neuroscience Friston Harrison and Penny 2003 Kalman lters controls Kalman 1960 hidden Markov models HMM and linear dynamical systems LDS machine learning and recurrent and sometimes convolutional models at large deep learning Throughout this entire paper we use the term SSM to refer exclusively to the class of structured SSMs or S4 models Gu Goel and R 2022 Gu Gupta et al 2022 Gupta Gu and Berant 2022 Hasani et al 2023 Ma et al 2023 Smith Warrington and Linderman 2023 and use these terms interchangeably For convenience we may also include derivatives of such models such as those focusing on either the linearrecurrence or globalconvolution viewpoints Y Li et al 2023 Orvieto et al 2023 Poli et al 2023 and clarify nuances when necessary SSM Architectures SSMs are standalone sequence transformations that can be incorporated into endtoend neural network architectures We also sometimes call SSM architectures SSNNs which are to SSM layers as CNNs are to linear convolution layers We discuss some of the most wellknown SSM architectures many of which will also serve as our primary baselines Linear attention Katharopoulos et al 2020 is an approximation of selfattention involving a recurrence which can be viewed as a degenerate linear SSM H3 Dao Fu Saab et al 2023 generalized this recurrence to use S4 it can be viewed as an architecture with an SSM sandwiched by two gated connections Figure 3 H3 also inserts a standard local convolution which they frame as a shiftSSM before the main SSM layer Hyena Poli et al 2023 uses the same architecture as H3 but replaces the S4 layer with an MLPparameterized global convolution Romero et al 2021 RetNet Y Sun et al 2023 adds an additional gate to the architecture and uses a simpler SSM allowing an alternative parallelizable computation path using a variant of multihead attention MHA instead of convolutions 4RWKV B Peng et al 2023 is a recent RNN designed for language modeling based on another linear attention approximation attentionfree Transformer S Zhai et al 2021 Its main WKV mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs Other closely related SSMs and architectures are discussed further in an extended related work Appendix B We highlight in particular S5 Smith Warrington and Linderman 2023 QRNN Bradbury et al 2016 and SRU Lei et al 2017 which we view as the most closely related methods to our core selective SSM 3 Selective State Space Models We motivate our selection mechanism using intuition from synthetic tasks Section 31 then explain how to incorporate this mechanism into state space models Section 32 The resulting timevarying SSMs cannot use convolutions presenting a technical challenge of how to compute them eciently We overcome this with a hardwareaware algorithm that exploits the memory hierarchy on modern hardware Section 33 We then describe a simple SSM architecture without attention or even MLP blocks Section 34 Finally we discuss some additional properties of selection mechanisms Section 35 31 Motivation Selection as a Means of Compression We argue that a fundamental problem of sequence modeling is compressing context into a smaller state In fact we can view the tradeos of popular sequence models from this point of view For example attention is both eective and inecient because it explicitly does not compress context at all This can be seen from the fact that autoregressive inference requires explicitly storing the entire context ie the KV cache which directly causes the slow lineartime inference and quadratictime training of Transformers On the other hand recurrent models are ecient because they have a nite state implying constanttime inference and lineartime training However their eectiveness is limited by how well this state has compressed the context To understand this principle we focus on two running examples of synthetic tasks Figure 2 The Selective Copying task modies the popular Copying task Arjovsky Shah and Bengio 2016 by varying the position of the tokens to memorize It requires contentaware reasoning to be able to memorize the relevant tokens colored and lter out the irrelevant ones white The Induction Heads task is a wellknown mechanism hypothesized to explain the majority of incontext learning abilities of LLMs Olsson et al 2022 It requires contextaware reasoning to know when to produce the correct output in the appropriate context black These tasks reveal the failure mode of LTI models From the recurrent view their constant dynamics eg the A B transitions in 2 cannot let them select the correct information from their context or aect the hidden state passed along the sequence an in inputdependent way From the convolutional view it is known that global convolutions can solve the vanilla Copying task Romero et al 2021 because it only requires timeawareness but that they have diculty with the Selective Copying task because of lack of contentawareness Figure 2 More concretely the spacing between inputstooutputs is varying and cannot be modeled by static convolution kernels In summary the eciency vs eectiveness tradeo of sequence models is characterized by how well they compress their state ecient models must have a small state while eective models must have a state that contains all necessary information from the context In turn we propose that a fundamental principle for building sequence models is selectivity or the contextaware ability to focus on or lter out inputs into a sequential state In particular a selection mechanism controls how information propagates or interacts along the sequence dimension see Section 35 for more discussion 32 Improving SSMs with Selection One method of incorporating a selection mechanism into models is by letting their parameters that aect interactions along the sequence eg the recurrent dynamics of an RNN or the convolution kernel of a CNN be inputdependent 5Input Output Output Copying Selective Copying Input Induction Heads Solution Perfectly solved by LTI eg convolutional models that do not need to look at the actual inputs Figure 2 Left The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by timeinvariant models such as linear recurrences and global convolutions Right Top The Selective Copying task has random spacing in between inputs and requires timevarying models that can selectively remember or ignore inputs depending on their content Right Bottom The Induction Heads task is an example of associative recall that requires retrieving an answer based on context a key ability for LLMs Algorithm 1 SSM S4 Input Output 1 A Represents structured matrix 2 B 3 C 4 5 A B A B 6 A B C Timeinvariant recurrence or convolution 7 return Algorithm 2 SSM Selection S6 Input Output 1 A Represents structured matrix 2 B 3 C 4 5 A B A B 6 A B C Timevarying recurrence scan only 7 return Algorithms 1 and 2 illustrates the main selection mechanism that we use The main dierence is simply making several parameters B C functions of the input along with the associated changes to tensor shapes throughout In particular we highlight that these parameters now have a length dimension meaning that the model has changed from timeinvariant to timevarying Note that shape annotations were described in Section 2 This loses the equivalence to convolutions 3 with implications for its eciency discussed next We specically choose 1 and where is a parameterized projection to dimension The choice of and is due to a connection to RNN gating mechanisms explained in Section 35 33 Efcient Implementation of Selective SSMs Hardwarefriendly architectures such as convolutions Krizhevsky Sutskever and Hinton 2012 and Transform ers Vaswani et al 2017 enjoy widespread application Here we aim to make selective SSMs ecient on modern hardware GPU as well The selection mechanism is quite natural and earlier works attempted to incorporate special cases of selection such as letting vary over time in recurrent SSMs Gu Dao et al 2020 However as previously mentioned a core limitation in the usage of SSMs is their computational eciency which was why S4 and all derivatives used LTI nonselective models most commonly in the form of global convolutions 331 Motivation of Prior Models We rst revisit this motivation and overview our approach to overcome limitations of prior methods At a high level recurrent models such as SSMs always balance a tradeo between expressivity and speed as discussed in Section 31 models with larger hidden state dimension should be more eective but slower Thus 6we want to maximize hidden state dimension without paying speed and memory costs Note that the recurrent mode is more exible than the convolution mode since the latter 3 is derived from expanding the former 2 Gu Goel and R 2022 Gu Johnson Goel et al 2021 However this would require computing and materializing the latent state with shape much larger by a factor of the SSM state dimension than the input and output of shape Thus the more ecient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel 3a of only Prior LTI SSMs leverage the dual recurrentconvolutional forms to increase the eective state dimension by a factor of 10 100 much larger than traditional RNNs without eciency penalties 332 Overview of Selective Scan HardwareAware State Expansion The selection mechanism is designed to overcome the limitations of LTI models at the same time we therefore need to revisit the computation problem of SSMs We address this with three classical techniques kernel fusion parallel scan and recomputation We make two main observations The naive recurrent computation uses FLOPs while the convolutional computation uses log FLOPs and the former has a lower constant factor Thus for long sequences and nottoolarge state dimension the recurrent mode can actually use fewer FLOPs The two challenges are the sequential nature of recurrence and the large memory usage To address the latter just like the convolutional mode we can attempt to not actually materialize the full state The main idea is to leverage properties of modern accelerators GPUs to materialize the state only in more ecient levels of the memory hierarchy In particular most operations except matrix multiplication are bounded by memory bandwidth Dao Fu Ermon et al 2022 Ivanov et al 2021 Williams Waterman and Patterson 2009 This includes our scan operation and we use kernel fusion to reduce the amount of memory IOs leading to a signicant speedup compared to a standard implementation Concretely instead of preparing the scan input A B of size in GPU HBM highbandwidth memory we load the SSM parameters A B C directly from slow HBM to fast SRAM perform the discretization and recurrence in SRAM and then write the nal outputs of size back to HBM To avoid the sequential recurrence we observe that despite not being linear it can still be parallelized with a workecient parallel scan algorithm Blelloch 1990 Martin and Cundy 2018 Smith Warrington and Linderman 2023 Finally we must also avoid saving the intermediate states which are necessary for backpropagation We carefully apply the classic technique of recomputation to reduce the memory requirements the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM As a result the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention Details of the fused kernel and recomputation are in Appendix D The full Selective SSM layer and algorithm is illustrated in Figure 1 34 A Simplifed SSM Architecture As with structured SSMs selective SSMs are standalone sequence transformations that can be exibly incorporated into neural networks The H3 architecture is the basis for the most wellknown SSM architectures Section 2 which are generally comprised of a block inspired by linear attention interleaved with an MLP multilayer perceptron block We simplify this architecture by combining these two components into one which is stacked homogenously Figure 3 This is inspired by the gated attention unit GAU Hua et al 2022 which did something similar for attention This architecture involves expanding the model dimension by a controllable expansion factor For each block most of the parameters 32 are in the linear projections 22 for input projections 2 for output projection while the inner SSM contributes less The number of SSM parameters projections for B C and 7H3 Gated MLP Mamba Linear projection Sequence transformation Nonlinearity activation or multiplication X X X X Conv SSM X Conv SSM Figure 3 Architecture Our simplifed block design combines the H3 block which is the basis of most SSM architectures with the ubiquitous MLP block of modern neural networks Instead of interleaving these two blocks we simply repeat the Mamba block homogenously Compared to the H3 block Mamba replaces the frst multiplicative gate with an activation function Compared to the MLP block Mamba adds an SSM to the main branch For we use the SiLU Swish activation Hendrycks and Gimpel 2016 Ramachandran Zoph and Quoc V Le 2017 the matrix A are much smaller in comparison We repeat this block interleaved with standard normalization and residual connections to form the Mamba architecture We always x to 2 in our experiments and use two stacks of the block to match the 122 parameters of a Transformers interleaved MHA multihead attention and MLP blocks We use the SiLU Swish activation function Hendrycks and Gimpel 2016 Ramachandran Zoph and Quoc V Le 2017 motivated so that the Gated MLP becomes the popular SwiGLU variant Chowdhery et al 2023 Shazeer 2020 Touvron et al 2023 Finally we additionally use an optional normalization layer we choose LayerNorm J L Ba Kiros and Hinton 2016 motivated by RetNets usage of a normalization layer in a similar location Y Sun et al 2023 35 Properties of Selection Mechanisms The selection mechanism is a broader concept that can be applied in dierent ways such as to more traditional RNNs or CNNs to dierent parameters eg A in Algorithm 2 or using dierent transformations 351 Connection to Gating Mechanisms We highlight the most important connection the classical gating mechanism of RNNs is an instance of our selection mechanism for SSMs We note that the connection between RNN gating and the discretization of continuoustime systems is well established Funahashi and Nakamura 1993 Tallec and Ollivier 2018 In fact Theorem 1 is an improvement of Gu Johnson Goel et al 2021 Lemma 31 generalizing to the ZOH discretization and inputdependent gates proof in Appendix C More broadly in SSMs can be seen to play a generalized role of the RNN gating mechanism In line with prior work we adopt the view that discretization of SSMs is the principled foundation of heuristic gating mechanisms Theorem 1 When 1 A 1 B 1 and then the selective SSM recurrence Algorithm 2 takes the form 1 1 5 As mentioned in Section 32 our specic choices of is from this connection In particular note that if a given input should be completely ignored as necessary in the synthetic tasks all channels should ignore it and so we project the input down to 1 dimension before repeatingbroadcasting with 8352 Interpretation of Selection Mechanisms We elaborate on two particular mechanistic eects of selection Variable Spacing Selectivity allows ltering out irrelevant noise tokens that may occur between inputs of interest This is exemplied by the Selective Copying task but occurs ubiquitously in common data modalities particularly for discrete data for example the presence of language llers such as um This property arises because the model can mechanistically lter out any particular input for example in the gated RNN case Theorem 1 when 0 Filtering Context It has been empirically observed that many sequence models do not improve with longer context F Shi et al 2023 despite the principle that more context should lead to strictly better performance An explanation is that many sequence models cannot eectively ignore irrelevant context when necessary an intuitive example are global convolutions and general LTI models On the other hand selective models can simply reset their state at any time to remove extraneous history and thus their performance in principle improves monotonicly with context length eg Section 432 Boundary Resetting In settings where multiple independent sequences are stitched together Transformers can keep them separate by instantiating a particular attention mask while LTI models will bleed information between the sequences Selective SSMs can also reset their state at boundaries eg or Theorem 1 when 1 These settings may occur articially eg packing documents together to improve hardware utilization or naturally eg episode boundaries in reinforcement learning Lu et al 2023 Additionally we elaborate on eects of each selective parameter Interpretation of In general controls the balance between how much to focus or ignore the current input It generalizes RNN gates eg in Theorem 1 mechanically a large resets the state and focuses on the current input while a small persists the state and ignores the current input SSMs 12 can be interpreted as a continuous system discretized by a timestep and in this context the intuition is that large represents the system focusing on the current input for longer thus selecting it and forgetting its current state while a small 0 represents a transient input that is ignored Interpretation of A We remark that while the A parameter could also be selective it ultimately aects the model only through its interaction with via A expA the discretization 4 Thus selectivity in is enough to ensure selectivity in A B and is the main source of improvement We hypothesize that making A selective in addition to or instead of would have similar performance and leave it out for simplicity Interpretation of B and C As discussed in Section 31 the most important property of selectivity is ltering out irrelevant information so that a sequence models context can be compressed into an ecient state In an SSM modifying B and C to be selective allows nergrained control over whether to let an input into the state or the state into the output These can be interpreted as allowing the model to modulate the recurrent dynamics based on content input and context hidden states respectively 36 Additional Model Details Real vs Complex Most prior SSMs use complex numbers in their state which is necessary for strong performance on many tasks Gu Goel and R 2022 However it has been empirically observed that completely realvalued SSMs seem to work ne and possibly even better in some settings Ma et al 2023 We use real values as the default which work well for all but one of our tasks we hypothesize that the complexreal tradeo is related to the continuousdiscrete spectrum in data modalities where complex numbers are helpful for continuous modalities eg audio video but not discrete eg text DNA 9Initialization Most prior SSMs also suggest special initializations particularly in the complexvalued case which can help in several settings such as lowdata regimes Our default initialization for the complex case is S4DLin and for the real case is S4DReal Gu Gupta et al 2022 which is based on the HIPPO theory Gu Dao et al 2020 These dene the th element of A as 12 and 1 respectively However we expect many initializations to work ne particularly in the largedata and realvalued SSM regimes some ablations are considered in Section 46 Parameterization of We dened the selective adjustment to as 1 which was motivated by the mechanics of Section 35 We observe that it can be generalized from dimension 1 to a larger dimension We set this to be a small fraction of which uses a negligible number of parameters compared to the main Linear projections in the block We additionally note that the broadcasting operation can instead be viewed as another Linear projection initialized to a specic pattern of 1s and 0s if this projection is trainable this leads to the alternative which can be viewed as a lowrank projection In our experiments the parameter which can be viewed as a bias term is initialized to 1 0001 01 following prior work on SSMs Gu Johnson Timalsina et al 2023 Remark 31 For brevity in our experimental results we sometimes abbreviate selective SSMs as S6 models because they are S4 models with a selection mechanism and computed with a scan 4 Empirical Evaluation In Section 41 we test Mambas ability to solve the two synthetic tasks motivated in Section 31 We then evaluate on three domains each evaluated on autoregressive pretraining as well as downstream tasks Section 42 language model pretraining scaling laws and zeroshot downstream evaluation Section 43 DNA sequence pretraining and netuning on a longsequence classication task Section 44 audio waveform pretraining and the quality of autoregressively generated speech clips Finally Section 45 shows Mambas computational eciency at both training and inference time and Section 46 ablates various components of the architecture and selective SSMs 41 Synthetic Tasks Full experiment details for these tasks including task details and training protocol are in Appendix E1 411 Selective Copying The Copying task is one of the most wellstudied synthetic tasks for sequence modeling originally designed to test the memorization abilities of recurrent models As discussed in Section 31 LTI SSMs linear recurrences and global convolutions can easily solve this task by only keeping track of time instead of reasoning about the data for example by constructing a convolution kernel of exactly the right length Figure 2 This was explicitly validated in earlier work on global convolutions Romero et al 2021 The Selective Copying task prevents this shortcut by randomizing the spacing between tokens Note that this task has been introduced before as the Denoising task Jing et al 2019 Note that many previous works argue that adding architecture gating multiplicative interactions can endow models with datadependence and solve related tasks Dao Fu Saab et al 2023 Poli et al 2023 However we nd this explanation insucient intuitively because such gating does not interact along the sequence axis and cannot aect the spacing between tokens In particular architecture gating is not an instance of a selection mechanism Appendix A Table 1 conrms that gated architectures such as H3 and Mamba only partially improve performance while the selection mechanism modifying S4 to S6 easily solves this task particularly when combined with these more powerful architectures 10Model Arch Layer Acc S4 No gate S4 183 No gate S6 970 H3 H3 S4 570 Hyena H3 Hyena 301 H3 S6 997 Mamba S4 564 Mamba Hyena 284 Mamba Mamba S6 998 Table 1 Selective Copying Accuracy for combinations of architectures and inner sequence layers 102 103 104 105 106 Test Sequence Length 00 02 04 06 08 10 Accuracy Induction Heads Extrapolation MHAAbsolute MHARoPE MHAxPos H3 Hyena Mamba Random Train Length Table 2 Induction Heads Models are trained on sequence length 28 256 and tested on increasing sequence lengths of 26 64 up to 220 1048576 Full numbers in Table 11 412 Induction Heads Induction heads Olsson et al 2022 is a simple task from the mechanistic interpretability lens Elhage et al 2021 that is surprisingly predictive of the incontext learning ability of LLMs It requires models to perform associative recall and copy for example if the model has seen a bigram such as Harry Potter in the sequence then the next time Harry appears in the same sequence the model should be able to predict Potter by copying from history Dataset We train a 2layer model on the induction heads task at sequence length 256 with a vocab size of 16 which is comparable to prior work on this task Dao Fu Saab et al 2023 but with longer sequences We additionally investigate generalization and extrapolation abilities by evaluating on a range of sequence lengths from 26 64 up to 220 1048576 at test time Models Following established work on induction heads we use 2 layer models which allows attention to mechanistically solve the induction heads task Olsson et al 2022 We test both multihead attention 8 heads with various positional encodings and SSM variants We use a model dimension of 64 for Mamba and 128 for the other models Results Table 2 shows that Mambaor more precisely its selective SSM layerhas the ability to solve the task perfectly because of its ability to selectively remember the relevant token while ignoring everything else in between It generalizes perfectly to millionlength sequences or 4000 longer than it saw during training while no other method goes beyond 2 Out of positional encoding variants for attention models xPos which was designed for length extrapolation is slightly better than the others also note that all attention models were only tested up to sequence length 214 16384 due to memory limitations Out of other SSMs H3 and Hyena are similar contrary to the ndings in Poli et al 2023 42 Language Modeling We evaluate the Mamba architecture on standard autoregressive language modeling against other architectures on both pretraining metrics perplexity and zeroshot evaluations We set the model sizes depth and width to mirror GPT3 specications We use the Pile dataset L Gao Biderman et al 2020 and follow the training recipe described in Brown et al 2020 All training details are in Appendix E2 421 Scaling Laws For baselines we compare against the standard Transformer architecture GPT3 architecture as well as the strongest Transformer recipe we know of here referred to as Transformer based on the PaLM and LLaMa 111019 1020 FLOPs log scale 101 6 100 2 101 Perplexity log scale Scaling Laws on The Pile Sequence Length 2048 Hyena RWKV Transformer RetNet H3 Transformer Mamba 1019 1020 FLOPs log scale 101 6 100 2 101 Perplexity log scale Scaling Laws on The Pile Sequence Length 8192 Hyena RWKV Transformer RetNet H3 Transformer Mamba Figure 4 Scaling Laws Models of size 125 to 13 parameters trained on the Pile Mamba scales better than all other attentionfree models and is the frst to match the performance of a very strong Transformer recipe that has now become standard particularly as the sequence length grows architectures eg rotary embedding SwiGLU MLP RMSNorm instead of LayerNorm no linear bias and higher learning rates We also compare against other recent subquadratic architectures Figure 4 All model details are in Appendix E2 Figure 4 shows scaling laws under the standard Chinchilla Homann et al 2022 protocol on models from 125 to 13 parameters Mamba is the rst attentionfree model to match the performance of a very strong Transformer recipe Transformer that has now become standard particularly as the sequence length grows We note that full results on context length 8k are missing for the RWKV and RetNet baselines prior strong recurrent models that can also be interpreted as SSMs due to a lack of ecient implementation leading to outofmemory or unrealistic computation requirements 422 Downstream Evaluations Table 3 shows the performance of Mamba on a range of popular downstream zeroshot evaluation tasks We compare against the most wellknown open source models at these sizes most importantly Pythia Biderman et al 2023 and RWKV B Peng et al 2023 which were trained with the same tokenizer dataset and training length 300B tokens as our models Note that Mamba and Pythia are trained with context length 2048 while RWKV was trained with context length 1024 43 DNA Modeling Motivated by the success of large language models there has been recent exploration into using the foundation model paradigm for genomics DNA has been likened to language in that it consists of sequences of discrete tokens with a nite vocab It is also known for requiring longrange dependencies to model Avsec et al 2021 We investigate Mamba as a FM backbone for pretraining and netuning in the same setting as recent works on longsequence models for DNA Nguyen Poli et al 2023 In particular we focus on two explorations of scaling laws across model size and sequence length Figure 5 and a dicult downstream synthetic classication task requiring long context Figure 6 For pretraining we largely follow a standard causal language modeling next token prediction setup for the training and model details see also Appendix E2 For the dataset we largely follow the setup of HyenaDNA Nguyen Poli et al 2023 which uses the HG38 dataset for pretraining consisting of a single human genome with about 45 billion tokens DNA base pairs in the training split 431 Scaling Model Size In this experiment we investigate the scaling properties of genomics foundation models with various model backbones Figure 5 Left Training To advantage the baselines we train on a short sequence length of 1024 as shown in Section 432 we expect results to favor Mamba even more at longer sequence lengths We x a global batch size of 1024 for a 12Table 3 Zeroshot Evaluations Best results for each size in bold We compare against open source LMs with various tokenizers trained for up to 300B tokens Pile refers to the validation split comparing only against models trained on the same dataset and tokenizer GPTNeoX20B For each model size Mamba is bestinclass on every single evaluation result and generally matches baselines at twice the model size Model Token Pile LAMBADA LAMBADA HellaSwag PIQA ArcE ArcC WinoGrande Average ppl ppl acc acc acc acc acc acc acc Hybrid H3130M GPT2 8948 2577 317 642 444 242 506 401 Pythia160M NeoX 2964 3810 330 302 614 432 241 519 406 Mamba130M NeoX 1056 1607 443 353 645 480 243 519 447 Hybrid H3360M GPT2 1258 480 415 681 514 247 541 480 Pythia410M NeoX 995 1084 514 406 669 521 246 538 482 Mamba370M NeoX 828 814 556 465 695 551 280 553 500 Pythia1B NeoX 782 792 561 472 707 570 271 535 519 Mamba790M NeoX 733 602 627 551 721 612 295 561 571 GPTNeo 13B GPT2 750 572 489 711 562 259 549 524 Hybrid H313B GPT2 1125 496 526 713 592 281 569 530 OPT13B OPT 664 580 537 724 567 296 595 550 Pythia14B NeoX 751 608 617 521 710 605 285 572 552 RWKV15B NeoX 770 704 564 525 724 605 294 546 543 Mamba14B NeoX 680 504 649 591 742 655 328 615 597 GPTNeo 27B GPT2 563 622 558 721 611 302 576 565 Hybrid H327B GPT2 792 557 597 733 656 323 614 580 OPT27B OPT 512 636 606 748 608 313 610 587 Pythia28B NeoX 673 504 647 593 740 641 329 597 591 RWKV3B NeoX 700 524 639 596 737 678 331 596 596 Mamba28B NeoX 622 423 692 661 752 697 363 635 633 GPTJ6B GPT2 410 683 663 754 670 366 641 630 OPT67B OPT 425 677 672 763 656 349 655 629 Pythia69B NeoX 651 445 671 640 752 673 355 613 617 RWKV74B NeoX 631 438 672 655 761 678 375 610 625 total of 220 1 tokens per batch Models were trained for 10 gradient steps for a total of 10 tokens Results Figure 5 Left shows that Mambas pretraining perplexity improves smoothly with model size and that Mamba scales better than both HyenaDNA and Transformer For example at the largest model size of 40 parameters the curve shows that Mamba can match the Transformer and HyenaDNA models with roughly 3 to 4 fewer parameters 432 Scaling Context Length In the next DNA experiment we investigate the scaling properties of models with respect to sequence length We only compare the HyenaDNA and Mamba models as quadratic attention becomes prohibitively expensive at longer sequence lengths We pretrain models on sequence lengths 210 1024 212 4096 214 16384 216 65536 218 262144 220 1048576 We x a model size of 6 layers by width 128 about 13M14M parameters Models were trained for 20 gradient steps for a total of 330 tokens The longer sequence lengths used sequence length warmup similar to Nguyen Poli et al 2023 Results Figure 5 Right shows that Mamba is able to make use of longer context even up to extremely long sequences of length 1M and its pretraining perplexity improves as the context increases On the other hand the HyenaDNA model gets worse with sequence length This is intuitive from the discussion in Section 35 on properties of the selection mechanism In particular LTI models cannot selectively ignore information from a convolutional perspective a very long convolution kernel is aggregating all information across a long sequence 13106 107 Parameters log scale 27 28 29 30 31 Perplexity Scaling Laws on the Human Genome HG38 HyenaDNA Mamba Transformer 103 104 105 106 Sequence Length 275 280 285 290 295 300 Perplexity Scaling Laws Sequence Length HG38 HyenaDNA 14M Mamba 14M Mamba 7M Figure 5 DNA Scaling Laws Pretraining on the HG38 human genome dataset Left Fixing short context length 210 1024 and increasing size from 200 to 40 parameters Mamba scales better than baselines Right Fixing model size and increasing sequence lengths while keeping tokensbatch and total training tokens fxed Unlike baselines the selection mechanism of Mamba facilitates better performance with increasing context length 103 104 105 106 Sequence Length 02 03 04 05 06 07 08 Accuracy Finetuning Accuracy Species DNA Classification HyenaDNA 14M Mamba 14M Mamba 7M Random Figure 6 Great Apes DNA Classifcation Accuracy after fnetuning on sequences of length 210 1024 up to 220 1048576 using pretrained models of the same context length Nu merical results in Table 13 104 105 106 Sequence Length 1300 1325 1350 1375 1400 1425 1450 1475 Bits Per Byte Scaling Laws Sequence Length YouTubeMix S4FFN Mamba Figure 7 Audio Pretraining Mamba improves performance over prior stateoftheart Sashimi in autoregressive audio mod eling while improving up to minutelong context or million length sequences controlling for computation which may be very noisy Note that while HyenaDNA claims to improve with longer context their results do not control for computation time 433 Synthetic Species Classifcation We evaluate models on a downstream task of classifying between 5 dierent species by randomly sampling a contigu ous segment of their DNA This task is adapted from HyenaDNA which used the species human lemur mouse pig hippo We modify the task to be signicantly more challenging by classifying between the ve great apes species human chimpanzee gorilla orangutan bonobo which are known to share 99 of their DNA 44 Audio Modeling and Generation For the audio waveform modality we compare primarily to the SaShiMi architecture and training protocols Goel et al 2022 This model comprises 1 a UNet backbone with two stages of pooling by a factor that doubles the model dimension per stage 2 alternating S4 and MLP blocks in each stage We consider replacing the S4MLP blocks with Mamba blocks Experiment details are in Appendix E4 441 LongContext Autoregressive Pretraining We evaluate pretraining quality autoregressive nextsample prediction on YouTubeMix DeepSound 2017 a standard piano music dataset used by prior work consisting of 4 hours of solo piano music sampled at a rate of 1416000 Hz Pretraining details largely follow the standard language modeling setup Section 42 Figure 7 evaluates the eect of increasing training sequence lengths from 213 8192 to 220 106 while keeping computation xed There are some slight edge cases to the way the data is curated which may lead to kinks in the scaling curves For example only minutelong clips were available so the maximum sequence length is actually bounded by 60 16000 960000 Both Mamba and the SaShiMi S4MLP baseline improve consistently with longer context lengths Mamba is better throughout and the gap widens at longer lengths The main metric is bits per byte BPB which is a constant factor log2 of the standard negative loglikelihood NLL loss for pretraining other modalities We note one important detail this is the only experiment in this paper in which we switched from the real parameterization to complex Section 36 We show additional ablations in Appendix E4 442 Autoregressive Speech Generation SC09 is a benchmark speech generation dataset Donahue McAuley and Puckette 2019 Warden 2018 consisting of 1second clips sampled at 16000 Hz of the digits zero through nine with highly variable characteristics We largely follow the autoregressive training setup and generation protocol of Goel et al 2022 Table 4 shows automated metrics of the MambaUNet model compared to a variety of baselines from Goel et al 2022 WaveNet Oord et al 2016 SampleRNN Mehri et al 2017 WaveGAN Donahue McAuley and Puckette 2019 DiWave Z Kong et al 2021 and SaShiMi A small Mamba model outperforms the stateoftheart and much larger GAN and diusion based models A larger model parametermatched to the baselines further improves on delity metrics dramatically Table 5 takes the small Mamba model and investigates combinations of dierent architectures for the outer stages and center stage It shows that Mamba is consistently better than S4MLP in the outer blocks and Mamba S4MLP MHAMLP in the center blocks Table 4 SC09 Automated metrics for unconditional generation on a challenging dataset of fxedlength speech clips Top to Bottom Autoregressive baselines nonautoregressive baselines Mamba and dataset metrics Model Params NLL FID IS mIS AM SampleRNN 350M 2042 896 171 302 176 WaveNet 42M 1925 508 227 580 147 SaShiMi 58M 1873 199 513 4257 074 WaveGAN 191M 203 490 3610 080 DifWave 241M 192 526 5121 068 SaShiMi 230M 142 594 6917 059 Mamba 61M 1852 094 626 8854 052 Mamba 243M 1860 067 733 1449 036 Train 000 856 2925 016 Test 002 833 2576 019 Table 5 SC09 Model Ablations Models with 6M parameters In SaShiMis UNet backbone there are 8 center blocks operat ing on sequence length 1000 sandwiched on each side by 8 outer blocks on sequence length 4000 sandwiched by 8 outer blocks on sequence length 16000 40 blocks total The architecture of the 8 center blocks are ablated independently of the rest Note that Transformers MHAMLP were not tested in the more im portant outer blocks because of efciency constraints Outer Center NLL FID IS mIS AM S4MLP MHAMLP 1859 145 506 4703 070 S4MLP S4MLP 1867 143 542 5354 065 S4MLP Mamba 1859 142 571 5651 064 Mamba MHAMLP 1850 137 563 5823 062 Mamba S4MLP 1853 107 605 7334 055 Mamba Mamba 1852 094 626 8854 052 45 Speed and Memory Benchmarks We benchmark the speed of the SSM scan operation state expansion 16 as well as the endtoend inference throughput of Mamba in Figure 8 Our ecient SSM scan is faster than the best attention implementation that we know of FlashAttention2 Dao 2023 beyond sequence length 2K and up to 2040 faster than a standard scan implementation in PyTorch Mamba achieves 45 higher inference throughput than a Transformer of similar size since without the KV cache it can use much higher batch sizes For example a Mamba69B untrained would have higher inference throughput than a 5 smaller Transformer13B Details in Appendix E5 which additionally includes a benchmark of memory consumption 15512 1k 2k 4k 8k 16k 32k 64k 128k 256k 512k Sequence length 01 1 10 100 1000 Time ms Scan vs Convolution vs Attention time A100 80GB PCIe FlashAttention2 Convolution Scan PyTorch Scan ours OOM 1 2 4 8 16 32 64 128 Batch size 500 1000 1500 Throughput tokens s 140 247 441 744 1089 1445 1688 1814 79 132 199 265 323 364 OOM OOM 58 101 172 261 364 443 490 515 46 66 91 109 120 OOM OOM OOM Inference throughput on A100 80GB prompt length 2048 Mamba 14B Transformer 13B Mamba 69B Transformer 67B Figure 8 Efciency Benchmarks Left Training our efcient scan is 40 faster than a standard implementation Right Inference as a recurrent model Mamba can achieve 5 higher throughput than Transformers 46 Model Ablations We perform a series of detailed ablations on components of our model focusing on the setting of language modeling with size 350M models at Chinchilla token counts same setting as Figure 4 461 Architecture Table 6 investigates the eects of the architecture block and its inner SSM layer Figure 3 We nd that Among previous nonselective LTI SSMs which are equivalent to global convolutions performance is very similar Replacing the complexvalued S4 variant from previous work with a realvalued one does not aect performance much suggesting that at least for LM realvalued SSMs may be a better choice when accounting for hardware eciency Replacing any of these with a selective SSM S6 signicantly improves performance validating the motivation of Section 3 The Mamba architecture performs similarly to the H3 architecture and seems slightly better when using a selective layer We also investigate interleaving the Mamba block with other blocks such as MLP a traditional architecture MHA a hybrid attention architecture in Appendix E22 462 Selective SSM Table 7 ablates the selective SSM layer by considering dierent combinations of selective B and C param eters Algorithm 2 showing that is the most important parameter due to its connection to RNN gating Theorem 1 Table 8 considers dierent initializations of the SSM which have been shown to make a large dierence in some data modalities and settings Gu Goel and R 2022 Gu Gupta et al 2022 On language modeling we nd that simpler realvalued diagonal initializations S4DReal row 3 instead of more standard complexvalued parameterizations S4DLin row 1 perform better Random initializations also work well consistent with ndings from prior work Mehta et al 2023 Table 9 and Table 10 consider varying the dimension of the and B C projections respectively Changing them from static to selective provides the most benet while increasing the dimensions further generally improves performance modestly with a small increase in parameter count Of particular note is the dramatic improvement of the selective SSM when the state size is increased with over a 10 perplexity improvement for a cost of only 1 additional parameters This validates our core motivation in Sections 31 and 33 16Table 6 Ablations Architecture and SSM layer The Mamba block performs similarly to H3 while being simpler In the inner layer there is little diference among diferent parameterizations of LTI models while selective SSMs S6 provide a large improvement More specifcally the S4 real variant is S4DReal and the S4 complex variant is S4DLin Model Arch SSM Layer Perplexity Hyena H3 Hyena 1024 H3 H3 S4 complex 1030 H3 S4 real 1034 H3 S6 895 Model Arch SSM Layer Perplexity Mamba Hyena 1075 Mamba S4 complex 1054 Mamba S4 real 1056 Mamba Mamba S6 869 Table 7 Ablations Selective parameters is the most im portant parameter Theorem 1 but using multiple selective pa rameters together synergizes Selective Selective B Selective C Perplexity 1093 1015 998 981 871 Table 8 Ablations Parameterization of A The more standard initializations based on S4DLin Gu Gupta et al 2022 perform worse than S4DReal or a random initializa tion when the SSM is selective A Initialization Field Perplexity A 1 2 Complex 916 A 12 Real 885 A 1 Real 871 A exp0 1 Real 871 Table 9 Ablations Expressivity of The selection mechanism of constructs it with a projection of the input Project ing it even to dim 1 provides a large in crease in performance increasing it fur ther provides further improvements at the cost of a modest increase in parameters State size fxed to 16 Size of proj Params M Perplexity 3589 912 1 3591 897 2 3593 897 4 3597 891 8 3605 883 16 3621 884 32 3652 880 64 3715 871 Table 10 Ablations SSM state dimension Top Constant B and C Bottom Selective B and C Increasing the SSM state dimension which can be viewed as an expansion factor on the dimension of the recurrent state can signifcantly improve performance for a negligible cost in parametersFLOPs but only when B and C are also selective Size of projection fxed to 64 State dimension Params M Perplexity 1 3671 988 2 3674 986 4 3680 982 8 3691 982 16 3715 981 1 3671 973 2 3674 940 4 3680 909 8 3691 884 16 3715 871 5 Discussion We discuss related work limitations and some future directions Related Work Appendix A discusses how the selection mechanism relates to similar concepts Appendix B has an extended related work of SSMs and other related models No Free Lunch ContinuousDiscrete Spectrum Structured SSMs were originally dened as discretizations of continuous systems 1 and have had a strong inductive bias toward continuoustime data modalities such as perceptual signals eg audio video As discussed in Sections 31 and 35 the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA but this conversely can impede their performance 17on data that LTI SSMs excel on Our ablations on audio waveforms examine this tradeo in more detail Downstream Afordances Transformerbased foundation models particularly LLMs have a rich ecosystem of properties and modes of interaction with pretrained models such as netuning adaptation prompting incontext learning instruction tuning RLHF quantization and so on We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and aordances Scaling Our empirical evaluation is limited to small model sizes below the threshold of most strong open source LLMs eg Llama Touvron et al 2023 as well as other recurrent models such as RWKV B Peng et al 2023 and RetNet Y Sun et al 2023 which have been evaluated at the 7B parameter scale and beyond It remains to assess whether Mamba still compares favorably at these larger sizes We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper 6 Conclusion We introduce a selection mechanism to structured state space models allowing them to perform contextdependent reasoning while scaling linearly in sequence length When incorporated into a simple attentionfree architecture Mamba achieves stateoftheart results on a diverse set of domains where it matches or exceeds the performance of strong Transformer models We are excited about the broad applications of selective state space models to build foundation models for dierent domains especially in emerging modalities requiring long context such as genomics audio and video Our results suggest that Mamba is a strong candidate to be a general sequence model backbone Acknowledgments We thank Karan Goel Arjun Desai and Kush Bhatia for helpful feedback on the draft References 1 Martin Arjovsky Amar Shah and Yoshua Bengio Unitary Evolution Recurrent Neural Networks In The International Conference on Machine Learning ICML 2016 pp 11201128 2 iga Avsec Vikram Agarwal Daniel Visentin Joseph R Ledsam Agnieszka GrabskaBarwinska Kyle R Taylor Yannis Assael John Jumper Pushmeet Kohli and David R Kelley Efective Gene Expression Prediction from Sequence by Integrating Longrange Interactions In Nature Methods 1810 2021 pp 11961203 3 Jimmy Ba Geofrey E Hinton Volodymyr Mnih Joel Z Leibo and Catalin Ionescu Using Fast Weights to Attend to the Recent Past In Advances in Neural Information Processing Systems NeurIPS 29 2016 4 Jimmy Lei Ba Jamie Ryan Kiros and Geofrey E Hinton Layer Normalization In arXivpreprintarXiv160706450 2016 5 Dzmitry Bahdanau Kyunghyun Cho and Yoshua Bengio Neural Machine Translation by Jointly Learning to Align and Translate In The International Conference on Learning Representations ICLR 2015 6 David Balduzzi and Muhammad Ghifary Stronglytyped Recurrent Neural Networks In International Con ference on Machine Learning PMLR 2016 pp 12921300 7 Stella Biderman Hailey Schoelkopf Quentin Gregory Anthony Herbie Bradley Kyle OBrien Eric Hallahan Mohammad Afah Khan Shivanshu Purohit USVSN Sai Prashanth Edward Raf et al Pythia A Suite for Analyzing Large Language Models across Training and Scaling In The International Conference on Machine Learning ICML PMLR 2023 pp 23972430 8 Yonatan Bisk Rowan Zellers Jianfeng Gao Yejin Choi et al PIQA Reasoning about Physical Commonsense in Natural Language In Proceedings of the AAAI conference on Artifcial Intelligence Vol 34 05 2020 pp 7432 7439 9 Guy E Blelloch Prefx Sums and Their Applications In 1990 10 James Bradbury Stephen Merity Caiming Xiong and Richard Socher Quasirecurrent Neural Networks In arXiv preprint arXiv161101576 2016 1811 Tom Brown Benjamin Mann Nick Ryder Melanie Subbiah Jared D Kaplan Prafulla Dhariwal Arvind Nee lakantan Pranav Shyam Girish Sastry Amanda Askell et al Language Models are Fewshot Learners In Advances in Neural Information Processing Systems NeurIPS 33 2020 pp 18771901 12 Aydar Bulatov Yuri Kuratov and Mikhail S Burtsev Scaling Transformer to 1M tokens and Beyond with RMT In arXiv preprint arXiv230411062 2023 13 Rewon Child Scott Gray Alec Radford and Ilya Sutskever Generating Long Sequences with Sparse Trans formers In arXiv preprint arXiv190410509 2019 14 Krzysztof Choromanski Valerii Likhosherstov David Dohan Xingyou Song Andreea Gane Tamas Sarlos Pe ter Hawkins Jared Davis Afroz Mohiuddin Lukasz Kaiser et al Rethinking Attention with Performers In The International Conference on Learning Representations ICLR 2021 15 Aakanksha Chowdhery Sharan Narang Jacob Devlin Maarten Bosma Gaurav Mishra Adam Roberts Paul Barham Hyung Won Chung Charles Sutton Sebastian Gehrmann et al PaLM Scaling Language Modeling with Pathways In Journal of Machine Learning Research 24240 2023 pp 1113 url httpjmlrorg papersv24221144html 16 Junyoung Chung Caglar Gulcehre KyungHyun Cho and Yoshua Bengio Empirical Evaluation of Gated Re current Neural Networks on Sequence Modeling In arXiv preprint arXiv14123555 2014 17 Peter Clark Isaac Cowhey Oren Etzioni Tushar Khot Ashish Sabharwal Carissa Schoenick and Oyvind Tafjord Think you have Solved Question Answering Try ARC the AI2 Reasoning Challenge In arXiv preprint arXiv180305457 2018 18 Tri Dao FlashAttention2 Faster Attention with Better Parallelism and Work Partitioning In 2023 19 Tri Dao Daniel Y Fu Stefano Ermon Atri Rudra and Christopher R FlashAttention Fast and Memory Efcient Exact Attention with IOAwareness In Advances in Neural Information Processing Systems NeurIPS 2022 20 Tri Dao Daniel Y Fu Khaled K Saab Armin W Thomas Atri Rudra and Christopher R Hungry Hungry Hippos Towards Language Modeling with State Space Models In The International Conference on Learning Representations ICLR 2023 21 Yann N Dauphin Angela Fan Michael Auli and David Grangier Language Modeling with Gated Convolu tional Networks In The International Conference on Machine Learning ICML PMLR 2017 pp 933941 22 DeepSound SampleRNN httpsgithubcomdeepsoundprojectsamplernnpytorch 2017 23 Jiayu Ding Shuming Ma Li Dong Xingxing Zhang Shaohan Huang Wenhui Wang and Furu Wei LongNet Scaling Transformers to 1000000000 Tokens In arXiv preprint arXiv230702486 2023 24 Chris Donahue Julian McAuley and Miller Puckette Adversarial Audio Synthesis In The International Conference on Learning Representations ICLR 2019 25 Alexey Dosovitskiy Lucas Beyer Alexander Kolesnikov Dirk Weissenborn Xiaohua Zhai Thomas Unterthiner Mostafa Dehghani Matthias Minderer Georg Heigold Sylvain Gelly et al An Image is Worth 16x16 Words Transformers for Image Recognition at Scale In The International Conference on Learning Representations ICLR 2020 26 Nelson Elhage Neel Nanda Catherine Olsson Tom Henighan Nicholas Joseph Ben Mann Amanda Askell Yuntao Bai Anna Chen Tom Conerly Nova DasSarma Dawn Drain Deep Ganguli Zac HatfeldDodds Danny Hernandez Andy Jones Jackson Kernion Liane Lovitt Kamal Ndousse Dario Amodei Tom Brown Jack Clark Jared Kaplan Sam McCandlish and Chris Olah A Mathematical Framework for Transformer Circuits In Transformer Circuits Thread 2021 httpstransformercircuitspub2021frameworkindexhtml 27 Mahan Fathi Jonathan Pilault PierreLuc Bacon Christopher Pal Orhan Firat and Ross Goroshin Block State Transformer In arXiv preprint arXiv230609539 2023 28 Yassir Fathullah Chunyang Wu Yuan Shangguan Junteng Jia Wenhan Xiong Jay Mahadeokar Chunxi Liu Yangyang Shi Ozlem Kalinli Mike Seltzer et al MultiHead State Space Model for Sequence Modeling In INTERSPEECH 2023 29 Karl J Friston Lee Harrison and Will Penny Dynamic Causal Modelling In Neuroimage 194 2003 pp 1273 1302 30 Daniel Y Fu Elliot L Epstein Eric Nguyen Armin W Thomas Michael Zhang Tri Dao Atri Rudra and Christo pher R Simple Hardwareefcient Long Convolutions for Sequence Modeling In The International Confer ence on Machine Learning ICML 2023 31 Kenichi Funahashi and Yuichi Nakamura Approximation of Dynamical Systems by Continuous Time Recur rent Neural Networks In Neural Networks 66 1993 pp 801806 1932 Leo Gao Stella Biderman Sid Black Laurence Golding Travis Hoppe Charles Foster Jason Phang Horace He Anish Thite Noa Nabeshima Shawn Presser and Connor Leahy The Pile An 800GB Dataset of Diverse Text for Language Modeling In arXiv preprint arXiv210100027 2020 33 Leo Gao Jonathan Tow Stella Biderman Sid Black Anthony DiPof Charles Foster Laurence Golding Jefrey Hsu Kyle McDonell Niklas Muennighof Jason Phang Laria Reynolds Eric Tang Anish Thite Ben Wang Kevin Wang and Andy Zou A Framework for Fewshot Language Model Evaluation Version v001 Sept 2021 doi 105281zenodo5371628 url httpsdoiorg105281zenodo5371628 34 Karan Goel Albert Gu Chris Donahue and Christopher R Its Raw Audio Generation with StateSpace Models In The International Conference on Machine Learning ICML 2022 35 Albert Gu Tri Dao Stefano Ermon Atri Rudra and Christopher R HIPPO Recurrent Memory with Optimal Polynomial Projections In Advances in Neural Information Processing Systems NeurIPS 2020 36 Albert Gu Karan Goel and Christopher R Efciently Modeling Long Sequences with Structured State Spaces In The International Conference on Learning Representations ICLR 2022 37 Albert Gu Caglar Gulcehre Tom Le Paine Matt Hofman and Razvan Pascanu Improving the Gating Mech anism of Recurrent Neural Networks In The International Conference on Machine Learning ICML 2020 38 Albert Gu Ankit Gupta Karan Goel and Christopher R On the Parameterization and Initialization of Diag onal State Space Models In Advances in Neural Information Processing Systems NeurIPS 2022 39 Albert Gu Isys Johnson Karan Goel Khaled Saab Tri Dao Atri Rudra and Christopher R Combining Recur rent Convolutional and Continuoustime Models with the Linear State Space Layer In Advances in Neural Information Processing Systems NeurIPS 2021 40 Albert Gu Isys Johnson Aman Timalsina Atri Rudra and Christopher R How to Train Your HIPPO State Space Models with Generalized Basis Projections In The International Conference on Learning Representations ICLR 2023 41 Ankit Gupta Albert Gu and Jonathan Berant Diagonal State Spaces are as Efective as Structured State Spaces In Advances in Neural Information Processing Systems 35 2022 pp 2298222994 42 David Ha Andrew Dai and Quoc V Le HyperNetworks In The International Conference on Learning Rep resentations ICLR 2017 43 Danijar Hafner Timothy Lillicrap Jimmy Ba and Mohammad Norouzi Dream to Control Learning Behav iors by Latent Imagination In The International Conference on Learning Representations ICLR 2020 44 Ramin Hasani Mathias Lechner TsunHsuan Wang Makram Chahine Alexander Amini and Daniela Rus Liquid Structural StateSpace Models In The International Conference on Learning Representations ICLR 2023 45 Mikael Henaf Arthur Szlam and Yann LeCun Recurrent Orthogonal Networks and LongMemory Tasks In The International Conference on Machine Learning ICML 2016 46 Dan Hendrycks and Kevin Gimpel Gaussian Error Linear Units GELUs In arXiv preprint arXiv160608415 2016 47 Sepp Hochreiter and Jrgen Schmidhuber Long ShortTerm Memory In Neural Computation 98 1997 pp 17351780 48 Jordan Hofmann Sebastian Borgeaud Arthur Mensch Elena Buchatskaya Trevor Cai Eliza Rutherford Diego de Las Casas Lisa Anne Hendricks Johannes Welbl Aidan Clark et al An Empirical Analysis of Compute Optimal Large Language Model Training In Advances in Neural Information Processing Systems NeurIPS 35 2022 pp 3001630030 49 Weizhe Hua Zihang Dai Hanxiao Liu and Quoc Le Transformer Quality in Linear Time In The Interna tional Conference on Machine Learning ICML PMLR 2022 pp 90999117 50 Hassan Ismail Fawaz Germain Forestier Jonathan Weber Lhassane Idoumghar and PierreAlain Muller Deep Learning for Time Series Classifcation A Review In Data Mining and Knowledge Discovery 334 2019 pp 917963 51 Andrei Ivanov Nikoli Dryden Tal BenNun Shigang Li and Torsten Hoefer Data Movement is All You Need A Case Study on Optimizing Transformers In Proceedings of Machine Learning and Systems 3 2021 pp 711 732 52 Li Jing Caglar Gulcehre John Peurifoy Yichen Shen Max Tegmark Marin Soljacic and Yoshua Bengio Gated Orthogonal Recurrent Units On Learning to Forget In Neural Computation 314 2019 pp 765783 53 Rudolph Emil Kalman A New Approach to Linear Filtering and Prediction Problems In 1960 2054 Angelos Katharopoulos Apoorv Vyas Nikolaos Pappas and Franois Fleuret Transformers are RNNs Fast Autoregressive Transformers with Linear Attention In International Conference on Machine Learning PMLR 2020 pp 51565165 55 Zhifeng Kong Wei Ping Jiaji Huang Kexin Zhao and Bryan Catanzaro DifWave A Versatile Difusion Model for Audio Synthesis In International Conference on Learning Representations 2021 56 Chrysoula Kosma Giannis Nikolentzos and Michalis Vazirgiannis TimeParameterized Convolutional Neu ral Networks for Irregularly Sampled Time Series In arXiv preprint arXiv230803210 2023 57 Alex Krizhevsky Ilya Sutskever and Geofrey E Hinton ImageNet Classifcation with Deep Convolutional Neural Networks In Advances in Neural Information Processing Systems NeurIPS 25 2012 58 Tao Lei When Attention Meets Fast Recurrence Training Language Models with Reduced Compute In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing 2021 pp 76337648 59 Tao Lei Yu Zhang Sida I Wang Hui Dai and Yoav Artzi Simple Recurrent Units for Highly Parallelizable Recurrence In arXiv preprint arXiv170902755 2017 60 Mario LezcanoCasado and David MartnezRubio Cheap Orthogonal Constraints in Neural Networks A Simple Parametrization of the Orthogonal and Unitary Group In The International Conference on Machine Learning ICML 2019 61 Yuhong Li Tianle Cai Yi Zhang Deming Chen and Debadeepta Dey What Makes Convolutional Models Great on Long Sequence Modeling In The International Conference on Learning Representations ICLR 2023 62 Vasileios Lioutas and Yuhong Guo Timeaware Large Kernel Convolutions In The International Conference on Machine Learning ICML PMLR 2020 pp 61726183 63 Chris Lu Yannick Schroecker Albert Gu Emilio Parisotto Jakob Foerster Satinder Singh and Feryal Behba hani Structured State Space Models for InContext Reinforcement Learning In Advances in Neural Informa tion Processing Systems NeurIPS 2023 64 Shahar Lutati Itamar Zimerman and Lior Wolf Focus Your Attention with Adaptive IIR Filters In arXiv preprint arXiv230514952 2023 65 Xuezhe Ma Chunting Zhou Xiang Kong Junxian He Liangke Gui Graham Neubig Jonathan May and Luke Zettlemoyer Mega Moving Average Equipped Gated Attention In The International Conference on Learning Representations ICLR 2023 66 Eric Martin and Chris Cundy Parallelizing Linear Recurrent Neural Nets Over Sequence Length In The International Conference on Learning Representations ICLR 2018 67 Soroush Mehri Kundan Kumar Ishaan Gulrajani Rithesh Kumar Shubham Jain Jose Sotelo Aaron Courville and Yoshua Bengio SampleRNN An Unconditional EndtoEnd Neural Audio Generation Model In The International Conference on Learning Representations ICLR 2017 68 Harsh Mehta Ankit Gupta Ashok Cutkosky and Behnam Neyshabur Long Range Language Modeling via Gated State Spaces In The International Conference on Learning Representations ICLR 2023 69 Zakaria Mhammedi Andrew Hellicar Ashfaqur Rahman and James Bailey Efcient Orthogonal Parametri sation of Recurrent Neural Networks using Householder Refections In International Conference on Machine Learning PMLR 2017 pp 24012409 70 Eric Nguyen Karan Goel Albert Gu Gordon Downs Preey Shah Tri Dao Stephen Baccus and Christopher R S4ND Modeling Images and Videos as Multidimensional Signals with State Spaces In Advances in Neural Information Processing Systems NeurIPS 2022 71 Eric Nguyen Michael Poli Marjan Faizi Armin Thomas Callum BirchSykes Michael Wornow Aman Pa tel Clayton Rabideau Stefano Massaroli Yoshua Bengio et al HyenaDNA Longrange Genomic Sequence Modeling at Single Nucleotide Resolution In Advances in Neural Information Processing Systems NeurIPS 2023 72 Catherine Olsson Nelson Elhage Neel Nanda Nicholas Joseph Nova DasSarma Tom Henighan Ben Mann Amanda Askell Yuntao Bai Anna Chen Tom Conerly Dawn Drain Deep Ganguli Zac HatfeldDodds Danny Hernandez Scott Johnston Andy Jones Jackson Kernion Liane Lovitt Kamal Ndousse Dario Amodei Tom Brown Jack Clark Jared Kaplan Sam McCandlish and Chris Olah Incontext Learning and Induction Heads In Transformer Circuits Thread 2022 httpstransformercircuitspub2022incontextlearningandinduction headsindexhtml 73 Aaron van den Oord Sander Dieleman Heiga Zen Karen Simonyan Oriol Vinyals Alex Graves Nal Kalch brenner Andrew Senior and Koray Kavukcuoglu WaveNet A Generative Model for Raw Audio In arXiv preprint arXiv160903499 2016 2174 Antonio Orvieto Samuel L Smith Albert Gu Anushan Fernando Caglar Gulcehre Razvan Pascanu and So ham De Resurrecting Recurrent Neural Networks for Long Sequences In The International Conference on Machine Learning ICML 2023 75 Denis Paperno Germn Kruszewski Angeliki Lazaridou NgocQuan Pham Rafaella Bernardi Sandro Pezzelle Marco Baroni Gemma Boleda and Raquel Fernndez The LAMBADA Dataset Word Prediction Requiring a Broad Discourse Context In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics 2016 pp 15251534 76 Razvan Pascanu Tomas Mikolov and Yoshua Bengio On the Difculty of Training Recurrent Neural Net works In International Conference on Machine Learning 2013 pp 13101318 77 Bo Peng Eric Alcaide Quentin Anthony Alon Albalak Samuel Arcadinho Huanqi Cao Xin Cheng Michael Chung Matteo Grella Kranthi Kiran GV et al RWKV Reinventing RNNs for the Transformer Era In arXiv preprint arXiv230513048 2023 78 Hao Peng Nikolaos Pappas Dani Yogatama Roy Schwartz Noah A Smith and Lingpeng Kong Random Feature Attention In The International Conference on Learning Representations ICLR 2021 79 Michael Poli Stefano Massaroli Eric Nguyen Daniel Y Fu Tri Dao Stephen Baccus Yoshua Bengio Stefano Ermon and Christopher R Hyena Hierarchy Towards Larger Convolutional Language Models In The International Conference on Machine Learning ICML 2023 80 Zhen Qin Xiaodong Han Weixuan Sun Bowen He Dong Li Dongxu Li Yuchao Dai Lingpeng Kong and Yiran Zhong Toeplitz Neural Network for Sequence Modeling In The International Conference on Learning Representations ICLR 2023 81 Zhen Qin Xiaodong Han Weixuan Sun Dongxu Li Lingpeng Kong Nick Barnes and Yiran Zhong The devil in linear transformer In arXiv preprint arXiv221010340 2022 82 Zhen Qin Weixuan Sun Hui Deng Dongxu Li Yunshen Wei Baohong Lv Junjie Yan Lingpeng Kong and Yiran Zhong CosFormer Rethinking Softmax in Attention In The International Conference on Learning Representations ICLR 2022 83 Ali Rahimi and Benjamin Recht Random features for largescale kernel machines In Advances in neural information processing systems 20 2007 84 Prajit Ramachandran Barret Zoph and Quoc V Le Swish A Selfgated Activation Function In arXiv preprint arXiv171005941 71 2017 p 5 85 David W Romero Anna Kuzina Erik J Bekkers Jakub M Tomczak and Mark Hoogendoorn CKConv Con tinuous Kernel Convolution For Sequential Data In arXiv preprint arXiv210202611 2021 86 Keisuke Sakaguchi Ronan Le Bras Chandra Bhagavatula and Yejin Choi Winogrande An Adversarial Wino grad Schema Challenge at Scale In Communications of the ACM 649 2021 pp 99106 87 George Saon Ankit Gupta and Xiaodong Cui Diagonal State Space Augmented Transformers for Speech Recognition In ICASSP 20232023 IEEE International Conference on Acoustics Speech and Signal Processing ICASSP IEEE 2023 pp 15 88 Imanol Schlag Kazuki Irie and Jrgen Schmidhuber Linear Transformers are Secretly Fast Weight Program mers In The International Conference on Machine Learning ICML PMLR 2021 pp 93559366 89 Noam Shazeer GLU Variants Improve Transformer In arXiv preprint arXiv200205202 2020 90 Freda Shi Xinyun Chen Kanishka Misra Nathan Scales David Dohan Ed H Chi Nathanael Schrli and Denny Zhou Large Language Models can be Easily Distracted by Irrelevant Context In The International Conference on Machine Learning ICML PMLR 2023 pp 3121031227 91 Jiaxin Shi Ke Alexander Wang and Emily Fox Sequence Modeling with Multiresolution Convolutional Mem ory In The International Conference on Machine Learning ICML PMLR 2023 pp 3131231327 92 Jimmy TH Smith Andrew Warrington and Scott W Linderman Simplifed State Space Layers for Sequence Modeling In The International Conference on Learning Representations ICLR 2023 93 Jianlin Su Yu Lu Shengfeng Pan Ahmed Murtadha Bo Wen and Yunfeng Liu Roformer Enhanced Trans former with Rotary Position Embedding In arXiv preprint arXiv210409864 2021 94 Yutao Sun Li Dong Shaohan Huang Shuming Ma Yuqing Xia Jilong Xue Jianyong Wang and Furu Wei Retentive network A successor to transformer for large language models In arXiv preprint arXiv230708621 2023 95 Ilya Sutskever Oriol Vinyals and Quoc V Le Sequence to Sequence Learning with Neural Networks In Advances in Neural Information Processing Systems NeurIPS 27 2014 2296 Corentin Tallec and Yann Ollivier Can Recurrent Neural Networks Warp Time In The International Con ference on Learning Representations ICLR 2018 97 Yi Tay Mostafa Dehghani Samira Abnar Yikang Shen Dara Bahri Philip Pham Jinfeng Rao Liu Yang Se bastian Ruder and Donald Metzler Long Range Arena A Benchmark for Efcient Transformers In Inter national Conference on Learning Representations ICLR 2021 98 Yi Tay Mostafa Dehghani Dara Bahri and Donald Metzler Efcient Transformers A Survey In ACM Com puting Surveys 556 2022 pp 128 99 Hugo Touvron Thibaut Lavril Gautier Izacard Xavier Martinet MarieAnne Lachaux Timothe Lacroix Bap tiste Rozire Naman Goyal Eric Hambro Faisal Azhar et al Llama Open and Efcient Foundation Language Models In arXiv preprint arXiv230213971 2023 100 Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez Lukasz Kaiser and Illia Polosukhin Attention Is All You Need In AdvancesinNeuralInformationProcessingSystemsNeurIPS 2017 101 Eugene Vorontsov Chiheb Trabelsi Samuel Kadoury and Chris Pal On Orthogonality and Learning Recur rent Networks with Long Term Dependencies In International Conference on Machine Learning PMLR 2017 pp 35703578 102 Jue Wang Wentao Zhu Pichao Wang Xiang Yu Linda Liu Mohamed Omar and Rafay Hamid Selective Structured StateSpaces for Longform Video Understanding In Proceedings of the IEEECVF Conference on Computer Vision and Pattern Recognition 2023 pp 63876397 103 Pete Warden Speech Commands A Dataset for LimitedVocabulary Speech Recognition In ArXiv abs180403209 2018 104 Samuel Williams Andrew Waterman and David Patterson Roofine An Insightful Visual Performance Model for Multicore Architectures In Communications of the ACM 524 2009 pp 6576 105 Brandon Yang Gabriel Bender Quoc V Le and Jiquan Ngiam CondConv Conditionally Parameterized Con volutions for Efcient Inference In Advances in Neural Information Processing Systems NeurIPS 32 2019 106 Rowan Zellers Ari Holtzman Yonatan Bisk Ali Farhadi and Yejin Choi HellaSwag Can a Machine Really Finish Your Sentence In Proceedings of the 57th Annual Meeting of the Association for Computational Linguis tics 2019 107 Shuangfei Zhai Walter Talbott Nitish Srivastava Chen Huang Hanlin Goh Ruixiang Zhang and Josh Susskind An Attention Free Transformer In arXiv preprint arXiv210514103 2021 108 Michael Zhang Khaled K Saab Michael Poli Tri Dao Karan Goel and Christopher R Efectively Modeling Time Series with Simple Discrete State Spaces In The International Conference on Learning Representations ICLR 2023 109 Lin Zheng Chong Wang and Lingpeng Kong Linear complexity randomized selfattention mechanism In International Conference on Machine Learning PMLR 2022 pp 2701127041 110 Simiao Zuo Xiaodong Liu Jian Jiao Denis Charles Eren Manavoglu Tuo Zhao and Jianfeng Gao Efcient Long Sequence Modeling via State Space Augmented Transformer In arXiv preprint arXiv221208136 2022 23A Discussion Selection Mechanism Our selection mechanism is inspired by and related to concepts such as gating hypernetworks and datadependence It can also be viewed as related to fast weights J Ba et al 2016 which connects classical RNNs with the mechanism of linear attention Schlag Irie and Schmidhuber 2021 However we believe that it is a distinct concept that is worth clarifying Gating Gating originally referred to the gating mechanisms of RNNs such as the LSTM Hochreiter and Schmidhuber 1997 and GRU J Chung et al 2014 or the gated equation 5n Theorem 1 This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN In particular this aects the propagation of signal through time and causes inputs to interact along the sequence length dimension However the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction often with an activation function For example elementwise multiplicative components of neural network architectures that do not interact along sequence length are now commonly referred to as gated architectures Hua et al 2022 Mehta et al 2023 despite a very dierent meaning than the original RNN sense Thus we believe the original concept of RNN gating versus the popular usage of multiplicative gating actually have a very dierent semantic meaning Hypernetworks Hypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks The original idea Ha Dai and Quoc V Le 2017 used it in a narrow sense to dene a large RNN whose recurrent parameters are generated by a smaller RNN Datadependence Similar to hypernetworks datadependence can refer to any notion where some parameters of the model depend on the data Poli et al 2023 Example GLU Activation To illustrate the issues with these concepts consider a simple diagonal linear layer D where D is a diagonal weight parameter Now suppose that D is itself generated from a linear transformation of with an optional nonlinearity D W Since it is diagonal the multiplication becomes an elementwise product W This is a rather trivial transformation yet it technically satises the common meanings of gating since it has a multiplicative branch hypernetworks since the parameter D is generated by another layer and datadependent since D depends on the data However this in fact simply denes a GLU function which is so simple that it is often considered just an activation function Dauphin et al 2017 Shazeer 2020 instead of a meaningful layer Selection Thus while selection mechanisms could be considered a special case of ideas such as architectural gating hypernetworks or datadependence so can an enormous range of other constructionsessentially anything with a multiplication including standard attention mechanisms Bahdanau Cho and Bengio 2015 Vaswani et al 2017 as welland we nd it uninformative to think of them as such Instead we view it as most closely related to the gating mechanism of traditional RNNs which is a special case Theorem 1 and also has a deeper history of connections to SSMs through variable inputdependent discretization of Funahashi and Nakamura 1993 Gu Dao et al 2020 Tallec and Ollivier 2018 We also eschew the term gating in favor of selection to clarify the overloaded use of former More narrowly we use selection to refer to the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence length Section 31 Beyond selective SSMs and gated RNNs other examples may include inputdependent convolutions Kosma Nikolentzos and Vazirgiannis 2023 Lioutas and Guo 2020 Lutati Zimerman and Wolf 2023 Yang et al 2019 and even attention 24B Related Work We overview several prior works related to our methods We mention that some of the most closely related models include recurrent layers such as S4 S5 and quasiRNNs as well as endtoend architectures such as H3 RetNet and RWKV B1 S4 Variants and Derivatives We describe a brief overview of some structured SSMs from past work particularly those that have a relation to our method S4 Gu Goel and R 2022 Gu Johnson Goel et al 2021 introduced the rst structured SSM describing diagonal structure and diagonal plus lowrank DPLR It focused on ecient convolutional algorithms for DPLR SSMs due to a connection to continuoustime online memorization HIPPO Gu Dao et al 2020 DSS Gupta Gu and Berant 2022 rst discovered the empirical eectiveness of diagonal structured SSMs by approximating the HIPPO initialization This was expanded on theoretically in S4D Gu Gupta et al 2022 S5 Smith Warrington and Linderman 2023 independently discovered the diagonal SSM approximation and is the rst S4 model to be computed recurrently with the parallel scan However this required lowering the eective state dimension which they accomplished by switching the SSM dimensions from a SISO singleinput singleoutput to MIMO multiinput multioutput formulation Our proposed S6 shares the scan but diers by i keeping the SISO dimensions which provides a larger eective recurrent state ii using a hardwareaware algorithm to overcome the computation issue iii adding the selection mechanism Lu et al 2023 applied S5 to metaRL in order to handle resetting the SSM state between episode trajectories Their mechanism can be viewed as a particular hardcoded instance of a selection mechanism where A is manually set to 0 instead of our learnable mechanism that depends on the input It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries Mega Ma et al 2023 introduced a simplication of S4 to be real instead of complex valued giving it an interpretation of being an exponential moving average EMA They additionally make an interesting connection of the discretization step of SSMs to an EMA damping term Contrary to ndings in the original S4 papers this was the rst model to show that realvalued SSMs are empirically eective in certain settings or when combined with dierent architectural components Liquid S4 Hasani et al 2023 is also motivated by augmenting S4 with an inputdependent state transition From this perspective it shares similarity to selection mechanisms although in a limited form which is still computed convolutionally and close to LTI SGConv Y Li et al 2023 Hyena Poli et al 2023 LongConv Fu et al 2023 MultiresConv J Shi K A Wang and Fox 2023 and Toeplitz Neural Network Qin Han W Sun He et al 2023 all focus on the convolutional representation of S4 and create global or long convolution kernels with dierent parameterizations However these methods cannot do fast autoregressive inference directly Notably all of these methods and all other structured SSMs that we are aware of have been nonselective and usually strictly LTI linear time invariant B2 SSM Architectures We use SSM architectures or state space neural networks SSNN to refer to deep neural network architectures incorporating one of the previous SSMs as a black box layer GSS Mehta et al 2023 was the rst gated neural network architecture incorporating SSMs It is motivated by the gated attention unit GAU of Hua et al 2022 and looks quite similar to our block except with additional projections Most importantly its projection contracts the model dimension to reduce the state size of the SSM while ours expands the model dimension in order to increase the state size based on the motivation in Section 31 25Mega Ma et al 2023 combined the EMA simplication of S4 described above into a hybrid architecture using an ecient attention approximation H3 Dao Fu Saab et al 2023 is motivated by combining S4 with linear attention Katharopoulos et al 2020 It is the rst to generalize this formulation of linear attention to more general recurrences which is also the basis of later architectures Selective S4 J Wang et al 2023 incorporates S4 as a black box to generate a binary mask which is multiplied on the input While sharing the selection name we consider this an architectural modication that is closer to architectural gating than a selection mechanism Appendix A For example we hypothesize that it would not solve the Selective Copying task because simply masking out the irrelevant inputs does not aect the spacing between the relevant ones indeed the Selective Copying task can even be viewed as coming premasked if the noise tokens are embedded to 0 RetNet Y Sun et al 2023 is also based on Linear Attention and very similar to H3 but reduces the inner S4 layer to a special case where the state dimension is 1 Although not framed as such its recurrence can be viewed as a special case of a linear SSM Its primary source of improvement is using a linear attention with large head dimension which can be viewed as another method to perform inputdependent state expansion Using a larger head dimension in the context of linear attention variants was rst done by H3 but not extensively used since this requires a proportional amount of extra computation RetNet avoids this with an alternate way to parallelize the computation with a variant of standard multihead attention instead of convolutions made feasible by their particular special case of SSMs which acts as a simple EMA RWKV B Peng et al 2023 is another recent RNN designed for language modeling It is based on AFT attentionfree Transformer S Zhai et al 2021 another variant of linear attention Its main WKV mechanism involves LTI recurrences and can be seen as the ratio of two SSMs We also highlight the gated attention unit GAU from Hua et al 2022 which was motivated by combining the Transformers MHA and MLP blocks together and was an inspiration for our architecture Section 34 combining the H3 and MLP blocks B3 Relationship to RNNs RNNs and SSMs are broadly related as they both involve the concepts of recurrence on a latent state Several older RNNs such as the strongly typed RNN Balduzzi and Ghifary 2016 quasiRNN QRNN Bradbury et al 2016 and simple recurrent unit SRU Lei 2021 Lei et al 2017 involve forms of gated RNNs without timewise nonlinearities Because of the connections of gating mechanisms and selection mechanisms these can be viewed as cases of selective SSMs and are thus more powerful in a sense than the family of LTI structured SSMs above The main dierences are They do not use state expansion 1 or selective B C parameters both of which are important for performance Section 46 They use a heuristic gating mechanism which we generalize as a consequence of the selection mechanism discretization Theorem 1 The connections to principled SSM theory provides better parameterizations and initializations Section 36 Additionally older RNNs famously suered from eciency issues and the vanishing gradients problem Pascanu Mikolov and Bengio 2013 both caused by their sequential nature The latter could be solved for some of the above RNNs by leveraging the parallel scan Martin and Cundy 2018 but the former was dicult without theory later developed for SSMs For example modern structured SSMs dier in more careful parameterization of the recurrent dynamics inspired by classical SSM theory eg through discretization Gu Johnson Goel et al 2021 Gu Johnson Timalsina et al 2023 or direct analysis Orvieto et al 2023 We also note that there is a long line of work on orthogonal RNNs Arjovsky Shah and Bengio 2016 Hena Szlam and LeCun 2016 LezcanoCasado and MartnezRubio 2019 Mhammedi et al 2017 Vorontsov et al 2017 26which are motivated by constraining the A transition matrix to be orthogonal or unitary in order to control its eigenvalues and prevent the vanishing gradient problem However these had other limitations we believe that these stem from the fact that orthogonalunitary RNNs are also LTI For example they are almost always evaluated on the Copying task which they can solve perfectly but observed to struggle on the Selective Copying task Jing et al 2019 B4 Linear Attention The Linear Attention LA Katharopoulos et al 2020 framework is an important result popularizing kernel attention and showing how it relates to recurrent autoregressive models Many variants have proposed alternative kernels and other modications Random Feature Attention RFA H Peng et al 2021 chooses the kernel feature map to approximate softmax attention ie the exp feature map using the random Fourier feature approximation of Gaussian kernels Rahimi and Recht 2007 Performer Choromanski et al 2021 nds an approximation to the exponential kernel involving only positive features which also allows the softmax normalization term TransNormer Qin Han W Sun D Li et al 2022 showed that the LA denominator term can be unstable and proposed replacing it with a LayerNorm cosFormer Qin W Sun et al 2022 augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality Linear Randomized Attention Zheng C Wang and L Kong 2022 generalize RFA from the perspective of importance sampling and generalize it to provide better estimates of the full softmax kernel rather than just the exptransformed numerator Aside from kernel attention many other variants of ecient attention exist the survey Tay Dehghani Bahri et al 2022 oers an extensive categorization of many of these B5 Long Context Models Long context has become a popular subject and several recent models have claimed to scale to longer and longer sequences However these are often from a computational standpoint and have not been extensively validated These include Recurrent Memory Transformer Bulatov Kuratov and Burtsev 2023 a lightweight wrapper around a Transformer backbone It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks their main result is similar to our Induction Heads extrapolation experiment Table 2 LongNet Ding et al 2023 which claimed to scale to 1B length but only evaluated on length 100 for actual tasks Hyena and HyenaDNA Nguyen Poli et al 2023 Poli et al 2023 which claimed to leverage up to 1M context However their experiments trained on proportionally more data at longer contexts making it hard to conclude if quality improvements at 1M context are due to context length or due to more data and computation Sparse Transformer Child et al 2019 showed a proofofconcept of using a strided sparse attention Transformer to model audio waveforms of length 220 1048576 although did not discuss performance tradeos when controlling for computation and model size In contrast we believe this work presents one of the rst approaches to meaningfully demonstrate increasing performance with longer context C Mechanics of Selective SSMs Proof of Theorem 1 Consider a selective SSM Algorithm 2 with 1 A 1 B 1 The corresponding continuoustime SSM 1 is which is also called a leaky integrator 27The discretization step size is where we observe that the parameter can be viewed as a learnable bias and folded into the linear projection Now applying the zeroorder hold ZOH discretization formulas A expA 1 1 exp 1 B A1expA I B expA I 1 A Thus the fnal discrete recurrence 2a is 1 1 as desired D Hardwareaware Algorithm For Selective SSMs Without inputdependent selectivity SSMs can be eciently implemented as a convolution Dao Fu Saab et al 2023 Gu Goel and R 2022 which leverages the fast Fourier transform FFT as primitive With selectivity SSMs are nolonger equivalent to convolution but we leverage the parallel associative scan While SSM scans are theoretically ecient FLOPs scaling linear in training foundation models with selective SSMs requires them to be ecient on modern hardware GPUs as well We describe how we use kernel fusion and recomputation to make SSM scan fast and memoryecient We evaluate the speed of our scan implementation compared to convolution and attention in Section 45 showing that it is up to 7 times faster than attention at sequence length 32K and is as memoryecient as the best attention implementation FlashAttention Speed On modern hardware accelerators GPUs most operations except matrix multiply are bounded by memorybandwidth Dao Fu Ermon et al 2022 Ivanov et al 2021 Williams Waterman and Patterson 2009 This the case with our scan operation and we use kernel fusion to reduce the amount of memory IOs leading to signicant speedup compared to a standard implementation The standard way to implement the scan algorithm in Section 32 is to prepare the scan input A B of size in GPU HBM highbandwidth memory commonly referred to as GPU memory call a parallel associative scan implementation to write the scan output of size to GPU HBM then multiply that scan output with C to produce an output of size However this requires the number of memory readswrites on the order of We can instead fuse the discretization step the scan and the multiplication with C into one kernel 1 We read in bytes of memory A B C from slow HBM to fast SRAM 2 We discretize to produce A B of size in SRAM 3 We perform a parallel associative scan yielding intermediate states of size in SRAM 4 We multiply and sum with C producing outputs of size and write it to HBM This way we reduce IOs by a factor of the state dimension which in practice speeds up the operation by 2040 times Section 45 28Table 11 Induction heads Models are trained on sequence length 28 256 and tested on various sequence lengths of 26 64 up to 220 1048576 denotes perfect generalization accuracy while denotes out of memory Model Params Test Accuracy at Sequence Length 26 27 28 29 210 211 212 213 214 215 216 217 218 219 220 MHAAbs 137K 996 1000 586 266 188 98 109 78 MHARoPE 137K 1000 836 313 184 86 90 55 MHAxPos 137K 1000 996 676 254 70 90 78 H3 153K 1000 809 395 238 148 82 59 66 82 47 82 63 74 Hyena 69M 977 1000 441 125 66 51 70 59 66 66 59 63 98 Mamba 74K 1000 Most of the parameters are in learnable positional encodings For sequence length too long where we cannot t the sequence in SRAM which is much smaller than HBM we split the sequences into chunks and perform the fused scan on each chunk As long as we have the intermediate scan states we can continue the scan with the next chunk Memory We describe how we use the classical technique of recomputation to reduce the total amount of memory required to train selective SSM layers From the way we fuse the forward pass we do not save the intermediate states of size to avoid memory blowup However these intermediate states are necessary for the backward pass to compute gradients We instead recompute those intermediate states in the backward pass Since the inputs A B C and output gradient read from HBM to SRAM are of size and the input gradients are also of size recomputation avoids the cost of reading elements from HBM This means that recomputation of the SSM states in the backward pass speeds up the computation compared to storing them and reading them from HBM Beyond optimizing for the memory requirement of just the scan operation we also use recomputation to optimize the memory requirement of the entire selective SSM block input projection convolution activation scan output projection In particular we do not save intermediate activations that take a lot of memory but are fast to recompute eg output of activation function or short convolution As a result the selective SSM layer has the same memory requirement as an optimized Transformer implementation with FlashAttention In particular each attention layer FlashAttention stores around 12 bytes of activations per token an each MLP layer stores around 20 bytes of activations per token for a total of 32 bytes assuming mixedprecision training in FP16 or BF16 Each selective SSM stores around 16 bytes of activations per token Hence two layers of selective SSMs have around the same activation memory as an attention layer and an MLP layer E Experimental Details and Additional Results E1 Synthetic Tasks Selective Copying Our setting is on sequences of length 4096 with a vocab size of 16 possible tokens including the white noise token from Figure 2 and requiring models to memorize 16 data tokens We use 2 layer models with a model dimension of 64 Models are trained for 400K steps at a constant learning rate of 00001 with a batch size of 64 Induction Heads Training consists of randomly generating data every step with a batch size of 8 We choose an epoch size of 8192 steps and track the accuracy on xed validation sets also randomly generated of each target sequence length For the MHAAbs and Mamba models results are reported after the 25th epoch 8192 25 204800 steps For the MHARoPE and MHAxPos models results are reported after the 50th epoch 8192 50 409600 steps For the LTI H3 and Hyena models results are reported after the 10th epoch 81920 steps because they had converged by then and failed to improve further 29Table 12 Scaling Law Model Sizes Our model sizes and hyperparameters for scaling experiments Model dimension and number of heads applies only to Transformer models Params Training steps Learning Rate Batch Size Tokens 125M 12 768 12 64 4800 6e4 05M tokens 25B 350M 24 1024 16 64 13500 3e4 05M tokens 7B 760M 24 1536 16 96 29000 25e4 05M tokens 15B 13B 24 2048 32 64 50000 2e4 05M tokens 26B We use the Adam optimizer with no weight decay All models are trained at constant learning rates 2 4 and 1 3 and the better results are reported for each model 2 4 for all models except Mamba The attention and Hyena models did not learn at LR 1 3 H3 learned at both LRs but interestingly generalized better to shorter sequences at the smaller LR of 2 4 Mamba learned at both LRs but extrapolated better at the larger LR of 1 3 E2 Language Modeling E21 Scaling Law Details All models were trained on the Pile Model Sizes Table 12 species the model sizes we use for scaling laws This is taken directly from the GPT3 specications Brown et al 2020 with very minor modications First we changed the batch size of the 13B model from 1M tokens to 05M tokens since we did not use enough parallelization to require the larger batch size Second we changed the number of training steps and total tokens to roughly match Chinchilla scaling laws Homann et al 2022 which specify that training tokens should increase proportionally to model size Training Recipes All models used the AdamW optimizer with gradient clip value 10 weight decay 01 no dropout linear learning rate warmup with cosine decay By default the peak learning rate is the GPT3 specication We give several models an improved recipe inspired by changes adopted by popular large language models such as PaLM Chowdhery et al 2023 and LLaMa Touvron et al 2023 These include linear learning rate warmup with cosine decay to 1 5 with a peak value of 5 the GPT3 value no linear bias terms RMSNorm instead of LayerNorm AdamW hyperparameter 9 95 the GPT3 value instead of the PyTorch default of 9 999 Architecture and Training Details Our models are Transformer The standard Transformer based on GPT3 Table 12 Transformer A Transformer with an improved architecture namely rotary positional encodings Su et al 2021 and SwiGLU MLP Shazeer 2020 and the improved training recipe above Hyena Interleaving a Hyena block the H3 block with S4 replaced by a global convolution parameterized by an MLP with standard MLP blocks The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly increased by 15 to preserve parameter count 30H3 The H3 architecture with a few modications including i using the same thin Hyena dimensions above ii the improved training recipe above iii a linear attention head dimension of 8 RWKV The default RWKV model from B Peng et al 2023 including its modied MLP block We also used as much of its specied training recipe as possible such as increasing the learning rates by 2 or 3 on certain parameters RetNet The default RetNet model from Y Sun et al 2023 We also gave it the improved training recipe above Mamba The standard Mamba architecture with the improved training recipe E22 Additional Scaling Law Ablations We perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws in Figure 4 Left Mamba Architecture Interleaving Blocks We test the eect of dierent architectural blocks combined with the Mamba block We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra path added This leads to two natural ablations What if the Mamba block is interleaved with a standard MLP block instead of stacked homogenously This can also be interpreted as taking Mamba and removing half of the SSMs What if the Mamba block is interleaved with MHA multihead attention blocks This can also be interpreted as taking a Transformer with SwiGLU MLPs ie what we call Transformer and simply adding SSMs to the MLP blocks Figure 9 Right shows these variants compared to the original homogenous Mamba architecture Interestingly neither change matters too much The MambaMLP architecture is only slightly worse and still better than all models except Transformer The MambaMHA architecture is only slightly better which is somewhat surprising in light of the fact that many recent works have found that combining LTI SSMs with Attention can lead to substantial improvements Dao Fu Saab et al 2023 Fathi et al 2023 Fathullah et al 2023 Saon Gupta and Cui 2023 Zuo et al 2022 H3 Architecture Training Recipes Next we ablate dierences between the Hyena and H3 models our weakest and strongest models outside of Transformer and Mamba particularly to isolate the eect of training recipes Hyena The Hyena block with its original architecture and GPT3 training recipe same as Figure 4 Hyena The same architecture but with the improved training recipe described above H3 The same architecture as Hyena but with the Hyena convolution kernel swapped out for S4D convolution kernel H3 The same as H3 but with a linear attention head dimension of 8 This increases computation inside the SSM recurrence but does not increase parameters Our general convention is that Model represents the base model with the improved training recipe and Model also allows for architectural changes Figure 9 Right shows that A large improvement is achieved by the improved training recipe which was used for many of the models in the main Figure 4 RetNet H3 Transformer Mamba The choice of the inner LTI SSM does not matter eg Hyena vs S4 consistent with ndings throughout this paper The head dimension expansion improves performance consistent with one of our main themes that expanded state dimension improves performance for SSMs Section 3 311019 1020 FLOPs log scale 101 7 100 8 100 9 100 Perplexity log scale Scaling Laws on The Pile Sequence Length 2048 Mamba MambaMLP MambaMHA 1019 1020 FLOPs log scale 101 Perplexity log scale Scaling Laws on The Pile Sequence Length 2048 Hyena Hyena H3 H3 Figure 9 Scaling laws extra ablations Left Instead of Right Instead of E23 Downstream Evaluation Details This pretraining procedure is the same as the scaling law protocol but extended to 300B tokens For the 13B model we use a batch size of 1M tokens to be consistent with the GPT3 specications We report the perplexity on the Pile validation set and for this metric only compare to models trained on the same dataset and with the same tokenizer in particular Pythia and RWKV For downstream evaluation we use the LM evaluation harness from EleutherAI L Gao Tow et al 2021 as done by most work in this area We evaluate on the following tasksdatasets that measure common sense reasoning LAMBADA Paperno et al 2016 HellaSwag Zellers et al 2019 PIQA Bisk et al 2020 ARCchallenge P Clark et al 2018 ARCeasy an easy subset of ARCchallenge WinoGrande Sakaguchi et al 2021 We report accuracy for LAMBADA WinoGrande PIQA and ARCeasy and accuracy normalized by sequence length for HellaSwag and ARCchallenge since normalized accuracy is higher for almost all models for these task E3 DNA Modeling E31 Pretraining Details We describe the dataset and training procedure of the HG38 pretraining task in more detail The dataset follows the splits from the prior Enformer work on genomics Avsec et al 2021 the training split contains a total of 34021 segments of length 217 131072 that cover the genome for a total of approximately 45 billion tokens DNA base pairs These segments are pairs of chromosome number starting index ending index and can be extended if necessary eg to get longer segments We deviate from HyenaDNA when the training sequence length is not 217 HyenaDNA always takes a xed subsegment eg the beginning or middle of the prescribed segment and thus for any training sequence length each epoch is xed to 34021 samples and doesnt necessarily go through the whole genome On the other hand we use the entire training data When the context length is less than or equal to 217 we divide up each segment into nonoverlapping subsegments of length so that there are 217 total samples and 217 45 tokens per epoch When the context length is greater than 217 we turn each segment into two samples one that begins with the prescribed segment and one that ends with the prescribed segment Thus each epoch has 2 items and 2 32tokens per epoch For example at sequence length 218 262144 there are 4 as many tokens as the default and at sequence length 220 there are 16 as many tokens Other training details generally follow the same protocol as our language modeling experiments Appendix E2 For example we use the AdamW with 1 2 09 095 no dropout weight decay 01 We use a cosine learning rate scheduler with linear warmup for 10 of total steps E32 Scaling Model Size Details Models The models we consider are Transformer a Transformer with improved architecture notably the usage of RoPE positional encodings Su et al 2021 Informally we found these to be noticeably better than vanilla positional encodings from Vaswani et al 2017 HyenaDNA the Hyena model from Nguyen Poli et al 2023 and Poli et al 2023 which is roughly a Transformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP Mamba the standard Mamba architecture Model Sizes We use the following model sizes Blocks 4 5 6 7 8 10 12 Model Dimension 64 96 128 192 256 384 512 Params Approx 250K 700K 14M 35M 70M 193M 407M Note that the number of blocks for Mamba is doubled because one Transformer layer includes both the MHA and MLP blocks and similarly for Hyena which requires two Mamba blocks to match parameters Section 34 Training For each model Transformer HyenaDNA Mamba we swept the learning rate across 1 3 2 3 4 3 8 3 The optimal Transformer and HyenaDNA learning rates were 2e3 across all sizes The optimal Mamba learning rate was 8e3 note that Mamba performed better than baselines with matched learning rates 2e3 but was more stable and improved even more at higher learning rates Furthermore as this LR is on the upper range of the sweep it is possible that our results are still suboptimal Note that in contrast to standard LM scaling laws Table 12 our LR held constant across model sizes for simplicity The optimal LR should go down for larger models but we didnt nd a noticeable eect at the small model sizes at most a few million parameters we considered E33 Scaling Context Length Details We use a total batch size of 224 16 tokens per training step for every sequence length eg at length 220 there are 16 segments per batch and at length 210 there are 16384 segments per batch This is a large batch size relative to the model size by usual LM standards but note that a batch size of 223 is the minimum possible on a machine with 8 GPUs and sequence length of 220 and that HyenaDNA used much larger batches of 228 The learning rate used was 0008 for Mamba and 0001 for HyenaDNA we initially attempted to use the same learning rate of 0002 from the previous section for HyenaDNA but found that it was unstable at the longest context length Sequence Length Warmup Following Nguyen Poli et al 2023 we use sequence length warmup SLW during pretraining We choose a simple schedule of 2 epochs at each poweroftwo sequence length starting from 210 1024 Note that because of how data is curated at the longest sequence lengths more steps and tokens are spent proportionally In particular each stage up to length 217 processes the same number of tokens but 4 as many tokens are processed at length 218 8 as many at length 219 and 16 as many at length 220 Unlike HyenaDNA we always control for the number of tokens per gradient update so the batch size is successively halved as the sequence lengths are doubled in each stage 33Table 13 Great Apes DNA Classifcation Accuracy after fnetuning on sequences of length 210 1024 up to 220 1048576 using pretrained models of the same context length Random guessing is 20 Model Params Accuracy at Sequence Length 210 212 214 216 218 220 HyenaDNA 14M 2804 2843 4117 4222 3110 5487 Mamba 14M 3147 2750 2766 4072 4241 7167 Mamba 7M 3000 2901 3148 4373 5660 8131 Remark E1 We also note that the schedule was not tuned and we never experimented with turning of sequence length warmup for these pretraining experiments We later found that SLW did not help noticeably for audio pretraining at similar lengths Section 44 and it is possible that it is not necessary for DNA pretraining either E34 Species Great Apes Classifcation Models are causal and therefore only the last element across the sequence length of the models output is used for the classication head Note that we control for the total number of elements in the loss function per gradient step The pretraining objective includes all positions across the sequence length so that is held constant in other words the batch size decreases as the sequence length increases However for a classication task since only the last position enters the loss the batch size itself is held constant Note that this also means that netuning models with longer sequence lengths is more computationally expensive Training consists of 10 epochs each of which has 1024 gradient steps Each gradient step uses batch size 64 which are all independently randomly drawn by uniformly picking a species uniformly picking a chromosome and then uniformly picking a contiguous segment of DNA Following Nguyen Poli et al 2023 models with a maximum context length greater than 214 16384 use sequence length warmup with 1 epoch at length 214 16384 1 epoch at length 215 32768 1 epoch at length 216 65536 and so on up to the maximum sequence length For example the model with 220 1048576 context undergoes 6 epochs of sequence length warmup before 4 more epochs at its maximum sequence length The learning rate for all Hyena models is while the learning rate for all Mamba models is These were found by performing learning rate sweeps for each model among 1 5 2 5 4 5 1 4 2 4 for the smaller sequence lengths 210 212 214 216 and these values were consistently found to be the best for each model An abridged learning rate sweep was done at length 218 which agreed with these values and a single run at length 220 was performed as described above the computational cost of these experiments is proportional to the sequence length The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear warmup to the maximum learning rate and 5 epochs of cosine decay down to 1 6 The unusually long learning rate warmup schedule was chosen because the sequence length warmup was also long eg comprising 6 out of 10 epochs for the model with context length 220 we did not experiment with this choice Results for the Species classication task are in Table 13 E4 Audio Details E41 YouTubeMix Audio Pretraining Model We use a model with 3 blocks per stage 3 5 15 total Mamba blocks pooling factor 16 and outer dimension 64 for about 35M parameters Dataset The data is mulaw encoded at 8 bits so the model is modeling discrete tokens with a vocab size of 256 The dataset consists of clips of up to 1 minute long or length 960000 which is subsampled and divided into segments of any desired sequence length Since the architecture involves two stages of pooling by a factor of 16 34Table 14 YouTubeMix length scaling sequence lengths and batch sizes Sequence length Batch size Tokens batch 468 2048 958464 1 958464 234 2048 479232 2 958464 117 2048 239616 4 958464 59 2048 120832 8 966656 30 2048 61440 16 983040 15 2048 30720 32 983040 8 2048 16384 64 1048576 4 2048 8192 128 1048576 104 105 Sequence Length 125 130 135 140 145 150 Bits Per Byte Audio Waveforms SSM Parameterization S4MLP Mamba S6 complex selective BC selective MambaS4 104 105 Sequence Length 125 130 135 140 145 Bits Per Byte Audio Waveforms SSM Parameterization Mamba S6 complex selective BC selective MambaS4 Figure 10 Audio Pretraining YouTubeMix Ablations As a uniformlysampled continuous signal modality audio wave forms actually beneft from LTI models which have matching inductive bias Left Homogenous models all blocks have the same parameterization Right Only the center UNet blocks are ablated the outer blocks are MambaS4 Purple line is same as fgure on left and we want the resulting sequence length to be a a multiple of 8 for hardware eciency the longest possible sequence is 468 2048 958464 The rest of our sequence lengths are dened by successively halving this and rounding up to the nearest multiple of 2048 Table 14 lists the specications used in Figure 7 Beyond the varying batch sizes the number of valid segments in the training set varied between dierent sequence lengths eg the number of training steps per epoch was not constant for dierent points in the graph which may have contributed to kinks in the scaling curves Training Models were trained for 200 training steps with a maximum learning rate of 0002 20 10 warmup steps and weight decay 01 similar to our general pretraining recipe across domains Additional Ablations SSM Parameterizations We investigate SSM parameterizations on longform audio waveform pretraining in the setting of Figure 7 The setting is modied slightly to use larger models 8 layers and 64 for 6M params the SaShiMi default shorter sequences 211 2048 to 218 262144 instead of 213 to 220 lower LR 0001 from 0002 and shorter training cycles 100K instead of 200K steps Figure 10 shows that the change from S4 S6 ie the selection mechanism is not always benecial On longform audio waveforms it in fact signicantly hampers performance which may be intuitive from the point of view that audio is uniformly sampled and very smooth and therefore benets from continuous linear timeinvariant LTI methods After ablating away the selection mechanism note that the resulting model is the S4 layer inside the Mamba block To disambiguate we call this MambaS4 as opposed the default Mamba architecture MambaS6 However on the right side we keep the outer layers of the UNet MambaS4 and ablate only the inner layers The performance dierences shrink dramatically this reinforces the hypothesis that layers closer to the raw audio signal should be LTI but once they are tokenized and compressed by the outer layers the inner layers no longer need to be LTI In this setting however the realvalued SSM still underperforms the complexvalued one 35E42 SC09 Speech Generation Autoregressive training largely followed the autoregressive language modeling protocol such as Weight decay 01 Learning rate warmup for 10 of total steps AdamW optimizer with 09 095 Gradient clip value 01 We used a learning rate of 0002 and 200000 training steps at a batch size of 16 The large Mamba model in Table 4 has 15 layers per stage with an outer dimension of 96 and pooling factor 4 We note that this dataset is small training went through 100 epochs and for this large model there was signicant overtting of the BPB or NLL However automated metrics of generated samples continually improving throughout training The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of 64 and pooling factor 4 The S4MLP block has roughly 22 42 parameters expansion factor 2 in the MLP The Transformer block has 42 22 parameters expansion factor 1 in the MLP The Mamba block has the usual 62 parameters All models have roughly 6M total parameters E5 Efciency Benchmark Scan Operation We compare the core operation of selective SSMs which is the parallel scan Section 33 against convolution and attention measured on an A100 80GB PCIe GPU Note that these do not include the cost of other operations outside of this core operation such as computing the convolutional kernel in globalconvolution models or computing the QKV projections in attention As a baseline we implement a standard parallel scan in PyTorch with no kernel fusion This requires materializing the parameters A B C in HBM Our scan implementation fuses the discretization step and the parallel scan avoiding the cost of materializing all the large parameters in HBM For convolution we use the standard implementation in PyTorch which separately performs FFTs on the inputs and the lters multiply them in frequency domain then performs an inverse FFT to obtain the result The theoretical complexity is log for sequence length For attention we compare against the fastest implementation that we are aware of FlashAttention2 Dao 2023 with causal mask Note that FlashAttention2 with causal mask is about 17 faster than without causal mask since approximately only half of the attention entries are computed We use batch size of 1 and increase the sequence length from 29 512 210 1 211 2 up to 219 500 some of the baselines run out of memory before reaching 500K We use a model dimension of 1024 and state dimension 16 We measure with BF16 inputs which is the data type most commonly used for large scale training Endtoend Inference We measure the inference throughput of a Mamba 14B model and an untrained Mamba 69B model against a standard Transformer GPT3 architecture at 13B and 67B size We use the standard Transformer implementation in the Huggingface transformers library We set the prompt length to be 2048 and the generation length to be 128 We vary the batch size from 1 2 4 8 16 32 64 to 128 and measure time time taken to generate 128 tokens We then calculate the throughput tokenss as batch size 128time taken We repeat the measurements 3 times and take the average Measurements are done on an A100 80GB PCIe GPU Memory Benchmark The memory usage simply scales proportionally to the size of the activation tensors as with most deep sequence models We report measurements of the training memory requirements of 125M models 36Table 15 Memory benchmark Mambas memory footprint is comparable to the most optimized Transformer Results for 125M models Batch size Transformer w FlashAttention2 Mamba 1 46GB 48GB 2 52GB 58GB 4 69GB 73GB 8 115GB 123GB 16 207GB 231GB 32 345GB 382GB on 1 A100 80GB GPU Each batch consists of sequences of length 2048 We compare to the most memoryecient Transformer implementation we are aware of with kernel fusion from torchcompile and with FlashAttention2 Table 15 shows that Mambas memory requirement is comparable to a similarsized Transformer with an extremely optimized implementation and we expect further improvement in Mambas memory footprint in the future 37