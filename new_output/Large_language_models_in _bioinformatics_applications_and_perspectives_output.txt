Text: Large language models in bioinformatics: applications and perspectives , Font: TimesNewRomanPS-BoldMT, Size: 14.039999961853027
Text: Jiajia Liu, Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 1,‚Ä†, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: , Mengyuan Yang, Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2,‚Ä†, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: , Yankai Yu, Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 3,‚Ä†, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: , Haixia Xu, Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 4,5,‚Ä†, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: ,  Kang Li, Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 5, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: , Xiaobo Zhou, Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 1,6,7,*, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 1, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: Center for Computational Systems Medicine, McWilliams School of Biomedical Informatics, The , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: University of Texas Health Science Center at Houston, Houston, Texas, 77030, USA , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: School of Life Sciences, Zhengzhou University, Zhengzhou, Henan 450001, China , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 3, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, Sichuan , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 611756, China , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 4, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: The Center of Gerontology and Geriatrics, West China Hospital, Sichuan University, Chengdu, Sichuan , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 610041, China  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 5, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: West China Biomedical Big Data Center, West China Hospital, Sichuan University, Chengdu, Sichuan , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 610041, China , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 6, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: McGovern Medical School, The University of Texas Health Science Center at Houston, Houston, TX , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 77030, USA , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 7, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: School of Dentistry, The University of Texas Health Science Center at Houston, Houston, TX 77030, USA , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: ‚Ä†, Font: TimesNewRomanPSMT, Size: 6.960000038146973
Text: These authors have contributed equally to this work. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: * , Font: TimesNewRomanPSMT, Size: 8.039999961853027
Text: Address correspondence to: , Font: TimesNewRomanPSMT, Size: 12.0
Text: Xiaobo Zhou, Ph.D.  , Font: TimesNewRomanPSMT, Size: 12.0
Text: McWilliams School of Biomedical Informatics , Font: TimesNewRomanPSMT, Size: 12.0
Text: The University of Texas Health Science Center at Houston , Font: TimesNewRomanPSMT, Size: 12.0
Text: 7000 Fannin St., Houston, TX 77030 , Font: TimesNewRomanPSMT, Size: 12.0
Text: Phone: 713-500-3923 , Font: TimesNewRomanPSMT, Size: 12.0
Text: Email: Xiaobo.Zhou@uth.tmc.edu  , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027

Text: Abstract , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Large language models (LLMs) are a class of artificial intelligence models based on deep learning, , Font: TimesNewRomanPSMT, Size: 12.0
Text: which have great performance in various tasks, especially in natural language processing (NLP). , Font: TimesNewRomanPSMT, Size: 12.0
Text: Large language models typically consist of artificial neural networks with numerous parameters, , Font: TimesNewRomanPSMT, Size: 12.0
Text: trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. , Font: TimesNewRomanPSMT, Size: 12.0
Text: However, their potential for solving bioinformatics problems may even exceed their proficiency , Font: TimesNewRomanPSMT, Size: 12.0
Text: in modeling human language. In this review, we will present a summary of the prominent large , Font: TimesNewRomanPSMT, Size: 12.0
Text: language models used in natural language processing, such as BERT and GPT, and focus on , Font: TimesNewRomanPSMT, Size: 12.0
Text: exploring the applications of large language models at different omics levels in bioinformatics, , Font: TimesNewRomanPSMT, Size: 12.0
Text: mainly including applications of large language models in genomics, transcriptomics, proteomics, , Font: TimesNewRomanPSMT, Size: 12.0
Text: drug discovery and single cell analysis. Finally, this review summarizes the potential and prospects , Font: TimesNewRomanPSMT, Size: 12.0
Text: of large language models in solving bioinformatic problems. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 1., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Arial-BoldMT, Size: 12.0
Text: Introduction , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Significant progress has been made in the field of natural language processing with the advent of , Font: TimesNewRomanPSMT, Size: 12.0
Text: large language models. Examples of these models include OpenAI‚Äôs GPT-X [1] and Google‚Äôs , Font: TimesNewRomanPSMT, Size: 12.0
Text: BERT [2] models. These models are transformative because they can understand, generate, and , Font: TimesNewRomanPSMT, Size: 12.0
Text: manipulate human language at an unprecedented scale. Vast Large language models are typically , Font: TimesNewRomanPSMT, Size: 12.0
Text: trained on datasets that encompass a significant portion of the internet‚Äôs text, enabling them to , Font: TimesNewRomanPSMT, Size: 12.0
Text: learn the complexities of language and context. These models are built upon a neural network , Font: TimesNewRomanPSMT, Size: 12.0
Text: architecture called transformers [3]. The transformer architecture revolutionized NLP due to its , Font: TimesNewRomanPSMT, Size: 12.0
Text: parallelization, scalability, and ability to capture long-range dependencies in text. Instead of , Font: TimesNewRomanPSMT, Size: 12.0
Text: relying on recurrent or convolutional layers, transformers use self-attention mechanisms, as , Font: TimesNewRomanPSMT, Size: 12.0
Text: previously described, which allow them to assess the importance of every word in a sentence when , Font: TimesNewRomanPSMT, Size: 12.0
Text: understanding context. This innovation is key to their remarkable performance. , Font: TimesNewRomanPSMT, Size: 12.0
Text: The training regimen for large language models comprises two phases: pre-training and fine-tuning. , Font: TimesNewRomanPSMT, Size: 12.0
Text: During pre-training, the model is trained on an extensive corpus of text data to acquire proficiency , Font: TimesNewRomanPSMT, Size: 12.0
Text: in grammar, factual knowledge, reasoning abilities, and word understanding. Fine-tuning tailors , Font: TimesNewRomanPSMT, Size: 12.0
Text: these models for specific tasks like translation, summarization, or question-answering. The , Font: TimesNewRomanPSMT, Size: 12.0
Text: adaptability of large language models is a major advantage; they can excel at various NLP tasks , Font: TimesNewRomanPSMT, Size: 12.0
Text: without task-specific architectures. However, they have found applications in diverse fields , Font: TimesNewRomanPSMT, Size: 12.0

Text: beyond NLP, including biology, healthcare, education, finance, customer service, and more. In , Font: TimesNewRomanPSMT, Size: 12.0
Text: particular, there have been many successful applications of large language models in the field of , Font: TimesNewRomanPSMT, Size: 12.0
Text: bioinformatics. In this manuscript, we focus on the applications of large language models to , Font: TimesNewRomanPSMT, Size: 12.0
Text: several bioinformatic tasks through five areas: DNA level, RNA level, protein level, drug , Font: TimesNewRomanPSMT, Size: 12.0
Text: discovery and single cell analysis, respectively corresponding to sections: applications of large , Font: TimesNewRomanPSMT, Size: 12.0
Text: language models in genomics, transcriptomics, proteomics, drug discovery and single cell analysis. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Applications of LLMs in genomics focus on LLMs using DNA sequence; applications of LLMs , Font: TimesNewRomanPSMT, Size: 12.0
Text: in transcriptomics using RNA sequence; applications of LLMs in proteomics focus on LLMs using , Font: TimesNewRomanPSMT, Size: 12.0
Text: protein sequence; applications of LLMs in drug discovery focus on LLMs using Molecular , Font: TimesNewRomanPSMT, Size: 12.0
Text: SMILES (seq) and applications of LLMs in single-cell analysis focus on LLMs using gene , Font: TimesNewRomanPSMT, Size: 12.0
Text: expression data from scRNA-seq or scMulti-omics data (, Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 1, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: ). , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 2., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Arial-BoldMT, Size: 12.0
Text: Large language models in natural language processing , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The emergence of large language models has brought milestone progress to natural language , Font: TimesNewRomanPSMT, Size: 12.0
Text: processing. These large language models, particularly exemplified by BERT [2] and GPT [1], , Font: TimesNewRomanPSMT, Size: 12.0
Text: usually use the ‚Äúpre-training + fine-tuning‚Äù approach (, Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 2, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: ). Pre-training is mainly to train a , Font: TimesNewRomanPSMT, Size: 12.0
Text: general/foundation model with strong generalization ability on a large-scale text corpus. Fine-, Font: TimesNewRomanPSMT, Size: 12.0
Text: tuning relies on the pre-trained model, undergoing additional training for a specific task. This , Font: TimesNewRomanPSMT, Size: 12.0
Text: process allows the model to adjust to the unique data and demands of the task at hand. The goal of , Font: TimesNewRomanPSMT, Size: 12.0
Text: fine-tuning is to improve the performance of the model on specific tasks, such as sentiment analysis, , Font: TimesNewRomanPSMT, Size: 12.0
Text: question answering systems, abstract text summarization, machine translation, creative text , Font: TimesNewRomanPSMT, Size: 12.0
Text: generation, etc. in natural language processing.  , Font: TimesNewRomanPSMT, Size: 12.0
Text: Self-attention. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: At the heart of the transformer is self-attention [3]. In this mechanism, every , Font: TimesNewRomanPSMT, Size: 12.0
Text: word/token in the input sequence is linked to three vectors: the Key vector (K), the Query vector , Font: TimesNewRomanPSMT, Size: 12.0
Text: (Q), and the Value vector (V). These vectors are learned during the training of the model. For each , Font: TimesNewRomanPSMT, Size: 12.0
Text: token, the model calculates attention scores for its relationship with other tokens. The attention , Font: TimesNewRomanPSMT, Size: 12.0
Text: score between a Query (Q) and Key (K) pair is calculated through the dot product, which is then , Font: TimesNewRomanPSMT, Size: 12.0
Text: adjusted by scaling it down with the square root of the dimension of the key vectors (to prevent , Font: TimesNewRomanPSMT, Size: 12.0
Text: exceedingly large values). Subsequently, a softmax function is frequently applied to attain , Font: TimesNewRomanPSMT, Size: 12.0
Text: normalized scores. The attention score is then multiplied by the Value (V) vector to derive the , Font: TimesNewRomanPSMT, Size: 12.0

Text: representation through the weighted sum of the attention mechanism. The self-attention process is , Font: TimesNewRomanPSMT, Size: 12.0
Text: represented as following [3]: , Font: TimesNewRomanPSMT, Size: 12.0
Text: ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëÑ, ùêæ, ùëâ) = ùë†ùëúùëìùë°ùëöùëéùë•(, Font: CambriaMath, Size: 12.0
Text: ùëÑùêæ, Font: CambriaMath, Size: 8.520000457763672
Text: ùëá, Font: CambriaMath, Size: 6.960000038146973
Text: ‚àöùëë, Font: CambriaMath, Size: 8.520000457763672
Text: ùëò, Font: CambriaMath, Size: 6.960000038146973
Text: )ùëâ, Font: CambriaMath, Size: 12.0
Text:    , Font: TimesNewRomanPS-ItalicMT, Size: 12.0
Text:                                         (1) , Font: TimesNewRomanPSMT, Size: 12.0
Text: BERT (Bidirectional Encoder Representations from Transformers). , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: BERT stands as a , Font: TimesNewRomanPSMT, Size: 12.0
Text: revolutionary model in the realm of natural language processing, reshaping the landscape of deep , Font: TimesNewRomanPSMT, Size: 12.0
Text: learning. Introduced by Google in 2018, BERT is engineered to comprehend the context and , Font: TimesNewRomanPSMT, Size: 12.0
Text: subtleties of human language in unprecedented ways [2]. At its core, BERT employs a Transformer , Font: TimesNewRomanPSMT, Size: 12.0
Text: architecture, which enables it to capture the relationships between words in both directions of a , Font: TimesNewRomanPSMT, Size: 12.0
Text: sentence, making it ‚Äúbidirectional‚Äù. This implies that BERT considers not only the words , Font: TimesNewRomanPSMT, Size: 12.0
Text: preceding a given word but also those following it, allowing it to grasp the full meaning of a , Font: TimesNewRomanPSMT, Size: 12.0
Text: sentence. One of BERT‚Äôs most remarkable features is its pretraining process. It learns from an , Font: TimesNewRomanPSMT, Size: 12.0
Text: extensive corpus of text from the internet, effectively absorbing vast amounts of knowledge. The , Font: TimesNewRomanPSMT, Size: 12.0
Text: pre-trained model can subsequently undergo fine-tuning for diverse natural language , Font: TimesNewRomanPSMT, Size: 12.0
Text: understanding tasks, including text classification, question answering, and language translation. , Font: TimesNewRomanPSMT, Size: 12.0
Text: BERT has consistently demonstrated state-of-the-art performance across a broad spectrum of NLP , Font: TimesNewRomanPSMT, Size: 12.0
Text: tasks and benchmarks, thanks to its ability to handle context, polysemy, and long-range , Font: TimesNewRomanPSMT, Size: 12.0
Text: dependencies effectively. Its versatility and accuracy have made it a pivotal tool in various , Font: TimesNewRomanPSMT, Size: 12.0
Text: applications, from chatbots and virtual assistants to sentiment analysis and content , Font: TimesNewRomanPSMT, Size: 12.0
Text: recommendation systems. , Font: TimesNewRomanPSMT, Size: 12.0
Text: GPT (Generative Pretrained Transformer). , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: GPT stands as a remarkable achievement in the , Font: TimesNewRomanPSMT, Size: 12.0
Text: realm of natural language processing. Introduced by OpenAI, GPT is a sophisticated language , Font: TimesNewRomanPSMT, Size: 12.0
Text: model designed to generate human-like text, making it a pivotal component in a wide array of , Font: TimesNewRomanPSMT, Size: 12.0
Text: applications [1]., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: At its core, GPT is built upon the transformer architecture, renowned for its , Font: TimesNewRomanPSMT, Size: 12.0
Text: attention mechanisms that allow it to comprehend the nuances of context and language structure. , Font: TimesNewRomanPSMT, Size: 12.0
Text: What sets GPT apart is its ‚Äúpretraining‚Äù phase, where it‚Äôs exposed to an extensive corpus of text , Font: TimesNewRomanPSMT, Size: 12.0
Text: from the internet, enabling it to absorb an immense amount of linguistic knowledge. The real , Font: TimesNewRomanPSMT, Size: 12.0
Text: brilliance of GPT lies in its capacity to produce text that is both coherent and contextually relevant. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Given a prompt, it can produce everything from creative stories and natural-sounding dialogue to , Font: TimesNewRomanPSMT, Size: 12.0
Text: summaries and translations. This versatility has found applications in chatbots, content generation, , Font: TimesNewRomanPSMT, Size: 12.0
Text: language translation, and even code generation., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: GPT‚Äôs knack for understanding context, handling , Font: TimesNewRomanPSMT, Size: 12.0
Text: ambiguity, and generating human-like text has earned it a place of honor in various domains, from , Font: TimesNewRomanPSMT, Size: 12.0

Text: creative writing and customer support to academic research and data analysis. Its adaptability, , Font: TimesNewRomanPSMT, Size: 12.0
Text: powered by fine-tuning, makes it a potent tool for diverse language-related tasks. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Comparison between BERT and GPT. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: BERT and GPT stand as two exceptional language , Font: TimesNewRomanPSMT, Size: 12.0
Text: models that have transformed the landscape of Natural Language Processing (NLP). While they , Font: TimesNewRomanPSMT, Size: 12.0
Text: share some common traits, they also exhibit significant differences in their design and applications. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Both BERT and GPT leverage the transformer architecture, employing attention mechanisms to , Font: TimesNewRomanPSMT, Size: 12.0
Text: grasp contextual information and dependencies within text data. This architecture has , Font: TimesNewRomanPSMT, Size: 12.0
Text: demonstrated significant effectiveness in comprehending and generating text that resembles , Font: TimesNewRomanPSMT, Size: 12.0
Text: human language. Furthermore, both models go through a pretraining phase, during which they are , Font: TimesNewRomanPSMT, Size: 12.0
Text: exposed to extensive amounts of text data sourced from the internet. This unsupervised learning , Font: TimesNewRomanPSMT, Size: 12.0
Text: process helps them learn language structure, grammar, and a broad range of linguistic patterns. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Furthermore, both BERT and GPT excel at transfer learning. These models undergo pretraining , Font: TimesNewRomanPSMT, Size: 12.0
Text: on an extensive text corpus and can be fine-tuned for particular downstream tasks, including text , Font: TimesNewRomanPSMT, Size: 12.0
Text: classification, sentiment analysis, or language generation. This makes them versatile and , Font: TimesNewRomanPSMT, Size: 12.0
Text: applicable across various NLP tasks. , Font: TimesNewRomanPSMT, Size: 12.0
Text: However, there are notable differences between these models. BERT employs a bidirectional , Font: TimesNewRomanPSMT, Size: 12.0
Text: approach, considering both left and right contexts when learning language representations. It aims , Font: TimesNewRomanPSMT, Size: 12.0
Text: to create deep contextual embeddings for words. In contrast, GPT uses a left-to-right approach, , Font: TimesNewRomanPSMT, Size: 12.0
Text: generating text autoregressively by predicting the next word in a sequence based on the preceding , Font: TimesNewRomanPSMT, Size: 12.0
Text: words. This design makes GPT more suitable for generation tasks. Additionally, their architectures , Font: TimesNewRomanPSMT, Size: 12.0
Text: differ, with BERT using a bidirectional Transformer encoder and a masked language model (MLM) , Font: TimesNewRomanPSMT, Size: 12.0
Text: objective during pretraining, while GPT uses a unidirectional transformer decoder and a causal , Font: TimesNewRomanPSMT, Size: 12.0
Text: language model objective. BERT undergoes training through a masked language model task and , Font: TimesNewRomanPSMT, Size: 12.0
Text: next sentence prediction, whereas GPT (GPT-1) is trained to predict the succeeding word in a , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequence. In practice, BERT is often preferred for tasks requiring deep contextual understanding, , Font: TimesNewRomanPSMT, Size: 12.0
Text: such as question-answering, sentiment analysis, and text classification. It is widely used for tasks , Font: TimesNewRomanPSMT, Size: 12.0
Text: where understanding the context of words is crucial. On the other hand, GPT shines in text , Font: TimesNewRomanPSMT, Size: 12.0
Text: generation tasks, including creative writing, language translation, and dialogue generation. It is an , Font: TimesNewRomanPSMT, Size: 12.0
Text: excellent choice for tasks that prioritize generating human-like text.  , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3. Applications of large language models in bioinformatics , Font: TimesNewRomanPS-BoldMT, Size: 12.0

Text: 3.1 Applications of large language models in genomics , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Decoding the language embedded in DNA to unveil its concealed instructions has long stood as a , Font: TimesNewRomanPSMT, Size: 12.0
Text: primary goal in biological research [4]. The universal genetic code, elucidating the translation of , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNA into proteins, has recently garnered attention for deciphering biological functions through , Font: TimesNewRomanPSMT, Size: 12.0
Text: models based on BERT or GPT architectures. DNABERT [5] for instance, employs a , Font: TimesNewRomanPSMT, Size: 12.0
Text: transformer‚Äîa robust, attention-based architecture renowned for its prowess in diverse natural , Font: TimesNewRomanPSMT, Size: 12.0
Text: language processing tasks. An evolution of this, DNABERT-2[6] introduces enhancements such , Font: TimesNewRomanPSMT, Size: 12.0
Text: as an efficient tokenizer and strategies to address input length constraints, thereby optimizing time , Font: TimesNewRomanPSMT, Size: 12.0
Text: and memory usage while bolstering model capabilities. Notably, DNABERT-2 introduces the , Font: TimesNewRomanPSMT, Size: 12.0
Text: Genome Understanding Evaluation (GUE), a comprehensive dataset for multi-species genome , Font: TimesNewRomanPSMT, Size: 12.0
Text: classification. With a threefold increase in efficiency compared to DNABERT, DNABERT-2 , Font: TimesNewRomanPSMT, Size: 12.0
Text: outperforms its predecessor on 23 out of 28 datasets. GROVER [7], a DNA language model , Font: TimesNewRomanPSMT, Size: 12.0
Text: leveraging byte-pair tokenization to scrutinize the human genome. GROVER's core objective lies , Font: TimesNewRomanPSMT, Size: 12.0
Text: in discerning contextual relationships between tokens, facilitating the identification of genomic , Font: TimesNewRomanPSMT, Size: 12.0
Text: region structures associated with functional genomics annotation. This unique approach positions , Font: TimesNewRomanPSMT, Size: 12.0
Text: GROVER as an invaluable tool for researchers delving into the intricacies of the human genome. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Building on the success of the GPT series in extracting general information from DNA sequences, , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNAGPT emerges as a GPT-based model for DNA, pre-trained on a vast dataset exceeding 10 , Font: TimesNewRomanPSMT, Size: 12.0
Text: billion base pairs. This model can be finely tuned for various DNA sequence analysis tasks. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Additionally, there is the Nucleotide Transformer [8], as another foundational model for DNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequences, has developed four distinct language models of varying sizes. These models have been , Font: TimesNewRomanPSMT, Size: 12.0
Text: pre-trained on three different datasets spanning multiple species. DNABERT, DNABERT-2, , Font: TimesNewRomanPSMT, Size: 12.0
Text: GROVER, DNAGPT, and the Nucleotide Transformer, all being pre-trained models, find , Font: TimesNewRomanPSMT, Size: 12.0
Text: application in sequence prediction tasks, including predicting promoter regions, enhancer regions, , Font: TimesNewRomanPSMT, Size: 12.0
Text: cis-regulatory elements, splice sites, and transcription factor binding sites. Detailed information is , Font: TimesNewRomanPSMT, Size: 12.0
Text: available in , Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 3, Table 1, and Supplementary Table 1, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: . , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.1.1 DNA sequence language models used to predict the genome-wide variant effects , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The significance of DNA sequence mutations in fostering diversity both within and between , Font: TimesNewRomanPSMT, Size: 12.0
Text: species cannot be overstated. Genome-wide association studies (GWAS) have played a pivotal , Font: TimesNewRomanPSMT, Size: 12.0
Text: role in furnishing essential biological insights across various species. However, a persistent , Font: TimesNewRomanPSMT, Size: 12.0
Text: challenge lies in pinpointing the specific causal variants accountable for the associations , Font: TimesNewRomanPSMT, Size: 12.0

Text: uncovered in these studies [4, 9]. The Genomic Pre-trained Network (GPN) [10], Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: is focused on , Font: TimesNewRomanPSMT, Size: 12.0
Text: acquiring knowledge about genome-wide variant effects by undergoing unsupervised pre-training , Font: TimesNewRomanPSMT, Size: 12.0
Text: on genomic DNA sequences. In this process, the model is presented with a 512-bp DNA sequence , Font: TimesNewRomanPSMT, Size: 12.0
Text: wherein certain positions are intentionally masked. Its primary objective is to predict the , Font: TimesNewRomanPSMT, Size: 12.0
Text: nucleotides at these masked positions. Notably, GPN excels in predicting the effects of rare , Font: TimesNewRomanPSMT, Size: 12.0
Text: variants that conventional GWAS methods may overlook. Leveraging the DNA sequence of any , Font: TimesNewRomanPSMT, Size: 12.0
Text: species, GPN demonstrates its capacity to predict variant effects across the entire genome., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: Furthermore, foundational models within the DNA sequence language model realm, such as , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNABERT, DNABERT-2, and the Nucleotide Transformer, also exhibit the capability to predict , Font: TimesNewRomanPSMT, Size: 12.0
Text: variants from DNA sequences. This collective advancement underscores the ongoing efforts to , Font: TimesNewRomanPSMT, Size: 12.0
Text: enhance our understanding of the intricate relationship between DNA sequence mutations and the , Font: TimesNewRomanPSMT, Size: 12.0
Text: resultant diversity in biological landscapes. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.1.2 DNA sequence language models used to predict the cis-regulatory regions , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Cis-regulatory sequences, including enhancers and promoters, represent pivotal elements in the , Font: TimesNewRomanPSMT, Size: 12.0
Text: regulation of gene expression, exerting a significant impact on the development and physiology , Font: TimesNewRomanPSMT, Size: 12.0
Text: [11]. However, identifying these sequences in DNA represents a major challenge, which is , Font: TimesNewRomanPSMT, Size: 12.0
Text: essential to comprehend their functions and their direct or indirect association with various , Font: TimesNewRomanPSMT, Size: 12.0
Text: diseases [12]. To address this issue, pre-trained models such as DNABERT, DNABERT-2, , Font: TimesNewRomanPSMT, Size: 12.0
Text: GROVER, and DNAGPT have been developed to accurately predict the promoter regions and , Font: TimesNewRomanPSMT, Size: 12.0
Text: their activities. These models have demonstrated remarkable accuracy and have emerged as , Font: TimesNewRomanPSMT, Size: 12.0
Text: valuable tools in the field of molecular biology for identifying cis-regulatory regions in DNA, and , Font: TimesNewRomanPSMT, Size: 12.0
Text: thus providing useful information on their functions and related diseases. BERT-Promoter [13], Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: was proposed to identify promoters and their activity. It uses a pre-trained BERT model for , Font: TimesNewRomanPSMT, Size: 12.0
Text: efficient feature representation and SHAP analysis for filtering. Different machine-learning , Font: TimesNewRomanPSMT, Size: 12.0
Text: algorithms are subsequently applied to construct the final prediction model. Notably, BERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: features in BERT-Promoter demonstrate significant performance improvement and enhanced , Font: TimesNewRomanPSMT, Size: 12.0
Text: model generalization compared to traditional feature representations. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Enhancers, small DNA segments binding to transcription factor proteins, play a crucial role in , Font: TimesNewRomanPSMT, Size: 12.0
Text: fortifying gene transcription and influencing gene expression [14, 15]. iEnhancer-BERT [16] , Font: TimesNewRomanPSMT, Size: 12.0
Text: presents an innovative transfer learning approach leveraging DNABERT to facilitate enhancer , Font: TimesNewRomanPSMT, Size: 12.0
Text: prediction. Departing from conventional fine-tuning methods, iEnhancer-BERT utilizes the output , Font: TimesNewRomanPSMT, Size: 12.0

Text: of all transformer encoder layers to generate feature vectors. These vectors undergo further , Font: TimesNewRomanPSMT, Size: 12.0
Text: classification through a Convolutional Neural Network (CNN) layer. Recognizing biological , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequences as the natural language for computational modeling signals an emerging trend in the , Font: TimesNewRomanPSMT, Size: 12.0
Text: field. In conclusion, iEnhancer-BERT presents a promising avenue for identifying new DNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: enhancers. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.1.3 DNA sequence language models used to predict the DNA-protein interaction , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The accurate identification of DNA-protein interactions is vital for regulating gene expression and , Font: TimesNewRomanPSMT, Size: 12.0
Text: understanding evolutionary processes [17]. Several DNA language models have been developed , Font: TimesNewRomanPSMT, Size: 12.0
Text: to predict these interactions, with downstream tasks for DNABERT, DNABERT-2, and GROVER , Font: TimesNewRomanPSMT, Size: 12.0
Text: including the prediction of protein-DNA binding based on ChIP-seq data. Besides, TFBert [18] is , Font: TimesNewRomanPSMT, Size: 12.0
Text: a pre-training DNA-protein binding model that has the ability to provide satisfactory results with , Font: TimesNewRomanPSMT, Size: 12.0
Text: minimal fine-tuning on a single dataset. This model treats DNA sequences as natural sentences , Font: TimesNewRomanPSMT, Size: 12.0
Text: and k-mer nucleotides as words, allowing for the effective extraction of context information from , Font: TimesNewRomanPSMT, Size: 12.0
Text: upstream and downstream nucleotides. Through pre-training on the 690 ChIP-seq datasets, TFBert , Font: TimesNewRomanPSMT, Size: 12.0
Text: can efficiently accomplish this task. MoDNA [19] framework is an innovative approach that , Font: TimesNewRomanPSMT, Size: 12.0
Text: incorporates common DNA functional motifs as domain knowledge. In the initial stage of self-, Font: TimesNewRomanPSMT, Size: 12.0
Text: supervised pre-training, the MoDNA framework establishes two prediction tasks including k-mer , Font: TimesNewRomanPSMT, Size: 12.0
Text: tokens prediction and motif prediction. Through pre-training on vast amounts of unlabeled genome , Font: TimesNewRomanPSMT, Size: 12.0
Text: data, the MoDNA framework successfully acquires semantic-level genome representations, which , Font: TimesNewRomanPSMT, Size: 12.0
Text: prove useful for promoter prediction and transcription factor binding site prediction. In essence, , Font: TimesNewRomanPSMT, Size: 12.0
Text: the MoDNA framework can be regarded as a biological language model that predicts DNA-protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: binding. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.1.4 DNA sequence language models used to predict the DNA methylation. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: DNA methylation stands as a fundamental biological process, playing a pivotal role in the , Font: TimesNewRomanPSMT, Size: 12.0
Text: epigenetic regulation of gene expression [20]. This process is linked to various medical conditions , Font: TimesNewRomanPSMT, Size: 12.0
Text: and finds diverse applications, such as serving as a marker for metagenomic binning. The types of , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNA methylation vary based on the specific nucleotide to which the methyl group attaches in the , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequence [21]. Several models have been developed to predict DNA methylation, with varying , Font: TimesNewRomanPSMT, Size: 12.0
Text: degrees of accuracy and specificity. The advancement of these models has led to a better , Font: TimesNewRomanPSMT, Size: 12.0
Text: understanding of the mechanisms underlying DNA methylation and its implications in various , Font: TimesNewRomanPSMT, Size: 12.0
Text: biological processes. Among them, the BERT6mA [22] is specifically designed for predicting 6-, Font: TimesNewRomanPSMT, Size: 12.0

Text: methyadenine (6mA) sites. iDNA-ABT [23], iDNA-ABF [24], and MuLan-Methyl [25] are , Font: TimesNewRomanPSMT, Size: 12.0
Text: versatile predictors that can be used for various methylation predictions, including 6mA, 5-, Font: TimesNewRomanPSMT, Size: 12.0
Text: hydroxymethylcytosine (5hmC), and 4-methylcytosine (4mC). iDNA-ABT, an advanced deep , Font: TimesNewRomanPSMT, Size: 12.0
Text: learning model, incorporates adaptive embedding based on BERT along with transductive , Font: TimesNewRomanPSMT, Size: 12.0
Text: information maximization (TIM). While demonstrating potential in identifying species, iDNA-, Font: TimesNewRomanPSMT, Size: 12.0
Text: ABT has yet to fully explore feature representation learning's potential, especially in uncovering , Font: TimesNewRomanPSMT, Size: 12.0
Text: key sequential patterns critical for understanding DNA methylation mechanisms. The iDNA-ABF , Font: TimesNewRomanPSMT, Size: 12.0
Text: approach adopts a multi-scale architecture, using multiple tokenizers instead of a single one. This , Font: TimesNewRomanPSMT, Size: 12.0
Text: enables BERT encoders to extract diverse embeddings based on tokenization, which are then , Font: TimesNewRomanPSMT, Size: 12.0
Text: combined to generate the final evolutionary output feature.  , Font: TimesNewRomanPSMT, Size: 12.0
Text: On the other hand, MuLan-Methyl introduces a novel deep-learning framework employing five , Font: TimesNewRomanPSMT, Size: 12.0
Text: transformer-based language models‚ÄîBERT[2], DistilBERT[26], ALBERT[27], XLNet[28], and , Font: TimesNewRomanPSMT, Size: 12.0
Text: ELECTRA[29], Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: ‚Äîto predict three types of methylation sites from DNA sequences and taxonomic , Font: TimesNewRomanPSMT, Size: 12.0
Text: information. The model generates its output by averaging the prediction probabilities from these , Font: TimesNewRomanPSMT, Size: 12.0
Text: language models. It's noteworthy that language models find successful application in biological , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequence analysis, and the joint utilization of different models significantly enhances performance. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.1.5 DNA sequence language models used to identify the splice site. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The precise splicing of pre-mRNA is crucial for ensuring accurate protein translation. This , Font: TimesNewRomanPSMT, Size: 12.0
Text: intricate process is governed by the selection of splice sites during splicing reactions, leading to , Font: TimesNewRomanPSMT, Size: 12.0
Text: the creation of diverse isoforms and splicing events. However, identifying splice sites poses a , Font: TimesNewRomanPSMT, Size: 12.0
Text: challenge, especially considering the prevalent GT-AG sequences in the DNA [30]. In response to , Font: TimesNewRomanPSMT, Size: 12.0
Text: this challenge, DNABERT and DNABERT-2 were developed and trained using 10,000 donor, , Font: TimesNewRomanPSMT, Size: 12.0
Text: acceptor, and non-splice site sequences from the human reference genome. The objective was to , Font: TimesNewRomanPSMT, Size: 12.0
Text: predict splice sites from DNA sequences. Notably, DNABERT consistently exhibited high , Font: TimesNewRomanPSMT, Size: 12.0
Text: attention to intronic regions. This observation suggests the presence and functional significance of , Font: TimesNewRomanPSMT, Size: 12.0
Text: various intronic splicing enhancers and silencers, acting as cis-regulatory elements for splicing. , Font: TimesNewRomanPSMT, Size: 12.0
Text: The focus on intronic regions underscores the importance of these elements in modulating splicing , Font: TimesNewRomanPSMT, Size: 12.0
Text: outcomes and highlights the potential of DNABERT models in unraveling the complexities of , Font: TimesNewRomanPSMT, Size: 12.0
Text: splicing regulation. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.2 Applications of large language models in transcriptomics, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0

Text: Efforts to develop BERT-based language models for DNA have faced challenges in accurately , Font: TimesNewRomanPSMT, Size: 12.0
Text: capturing evolutionary information from homologous sequences. Unlike proteins, RNA sequences , Font: TimesNewRomanPSMT, Size: 12.0
Text: are less conserved. In response to this, two notable RNA foundation models have been introduced: , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA-FM [31] and RNA-MSM [32]. RNA-FM employs self-supervised learning to predict , Font: TimesNewRomanPSMT, Size: 12.0
Text: secondary/3D structures, leveraging the vast dataset of all 23 million non-coding RNA sequences. , Font: TimesNewRomanPSMT, Size: 12.0
Text: This approach allows RNA-FM to effectively capture diverse structural information, providing a , Font: TimesNewRomanPSMT, Size: 12.0
Text: comprehensive understanding of RNA sequence features. On the other hand, RNA-MSM utilizes , Font: TimesNewRomanPSMT, Size: 12.0
Text: homologous sequences sourced from RNAcmap through an automated pipeline. This model excels , Font: TimesNewRomanPSMT, Size: 12.0
Text: in accurately mapping to 2D base pairing probabilities and 1D solvent accessibilities. The pre-, Font: TimesNewRomanPSMT, Size: 12.0
Text: trained model can be fine-tuned for various downstream tasks related to RNA structure and , Font: TimesNewRomanPSMT, Size: 12.0
Text: function, as shown in , Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 4, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: , , Font: TimesNewRomanPSMT, Size: 12.0
Text: Table 1, and Supplementary Table 1, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: . , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.2.1 RNA sequence language models used to predict the RNA family classification and , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: secondary structure , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: RNA secondary structure prediction poses a substantial challenge for RNA structural biologists, , Font: TimesNewRomanPSMT, Size: 12.0
Text: requiring focused efforts to better understand RNA folding rules and enhance the accuracy of , Font: TimesNewRomanPSMT, Size: 12.0
Text: structure prediction models. Such models hold significant potential for facilitating downstream , Font: TimesNewRomanPSMT, Size: 12.0
Text: tasks, including the development of RNA-targeting drugs [33]. RNABERT [34] is designed with , Font: TimesNewRomanPSMT, Size: 12.0
Text: three key components: tokens and position embedding, a transformer model, and pre-training tasks. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Leveraging the pre-training BERT algorithm for non-coding RNA (ncRNA), RNABERT is , Font: TimesNewRomanPSMT, Size: 12.0
Text: specifically tailored for secondary structure prediction and RNA family classification. The model's , Font: TimesNewRomanPSMT, Size: 12.0
Text: architecture and training tasks are strategically crafted to capture the intricate rules governing RNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: folding, enabling more accurate predictions. One notable application of RNABERT lies in , Font: TimesNewRomanPSMT, Size: 12.0
Text: addressing the practical need for rapid and precise structural alignment of unknown sequences to , Font: TimesNewRomanPSMT, Size: 12.0
Text: existing RNA families. This capability makes RNABERT a valuable tool for annotating novel , Font: TimesNewRomanPSMT, Size: 12.0
Text: transcripts, assisting researchers in understanding the structural characteristics of previously , Font: TimesNewRomanPSMT, Size: 12.0
Text: uncharacterized RNA molecules. The potential contributions of RNABERT extend beyond , Font: TimesNewRomanPSMT, Size: 12.0
Text: structure prediction, offering practical solutions for tasks critical to advancing our knowledge of , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA biology and its applications in therapeutic development. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.2.2 RNA sequence language models used to predict the RNA splicing , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: RNA splicing is a vital process in the post-transcriptional gene expression of eukaryotic organisms. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Researchers have made strides in enhancing the sequence-based modeling of RNA splicing , Font: TimesNewRomanPSMT, Size: 12.0

Text: through the development of a pre-trained model known as SpliceBERT [35]. This model is trained , Font: TimesNewRomanPSMT, Size: 12.0
Text: on precursor messenger RNA sequences derived from 72 vertebrates, enabling it to generate , Font: TimesNewRomanPSMT, Size: 12.0
Text: embeddings that preserve both the evolutionary information of nucleotides and the functional , Font: TimesNewRomanPSMT, Size: 12.0
Text: characteristics of splice sites. SpliceBERT serves a dual purpose by not only capturing the nuances , Font: TimesNewRomanPSMT, Size: 12.0
Text: of RNA splicing but also facilitating the identification of potential splice-disrupting variants. The , Font: TimesNewRomanPSMT, Size: 12.0
Text: pre-trained model allows for the unsupervised prioritization of such variants based on their impact , Font: TimesNewRomanPSMT, Size: 12.0
Text: on the output of SpliceBERT within the sequence context. This capability offers a valuable tool , Font: TimesNewRomanPSMT, Size: 12.0
Text: for researchers seeking to understand the influence of genetic variations on RNA splicing, , Font: TimesNewRomanPSMT, Size: 12.0
Text: providing insights that can aid in the identification and prioritization of potentially significant , Font: TimesNewRomanPSMT, Size: 12.0
Text: variants in an efficient and data-driven manner. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.2.3 RNA sequence language models used to identify the lncRNAs and predict the lncRNAs‚Äô , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: coding potential , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Long non-coding RNA (lncRNA) is a crucial transcript form that plays a substantial regulatory , Font: TimesNewRomanPSMT, Size: 12.0
Text: role in the development of cancers and diseases without encoding proteins [36]. Initially, small , Font: TimesNewRomanPSMT, Size: 12.0
Text: Open Reading Frames (sORFs) in lncRNA were thought to be weak in protein translation. , Font: TimesNewRomanPSMT, Size: 12.0
Text: However, recent studies have revealed that they can indeed encode peptides. This discovery adds , Font: TimesNewRomanPSMT, Size: 12.0
Text: complexity to the identification of lncRNA, particularly those containing sORFs, which is crucial , Font: TimesNewRomanPSMT, Size: 12.0
Text: for uncovering novel regulatory factors. Addressing this challenge, LncCat [37] utilizes category , Font: TimesNewRomanPSMT, Size: 12.0
Text: boosting and ORF-attention features for improved performance on both long ORF and sORF , Font: TimesNewRomanPSMT, Size: 12.0
Text: datasets. The ORF-attention feature positively influences the prediction of lncRNA. LncCat , Font: TimesNewRomanPSMT, Size: 12.0
Text: employs the BERT model to represent peptide sequences encoded by ORFs as part of the ORF-, Font: TimesNewRomanPSMT, Size: 12.0
Text: attention features. CatBoost[38] is utilized to build the prediction model, incorporating the , Font: TimesNewRomanPSMT, Size: 12.0
Text: aforementioned features, to identify lncRNA. The effectiveness of LncCat is demonstrated across , Font: TimesNewRomanPSMT, Size: 12.0
Text: five species datasets and the Ribo-seq dataset, showcasing its utility in accurately identifying , Font: TimesNewRomanPSMT, Size: 12.0
Text: lncRNA with sORFs and contributing to the discovery of novel regulatory elements. , Font: TimesNewRomanPSMT, Size: 12.0
Text: The prediction of translatable sORFs within lncRNAs, referred to as lncRNA-sORFs, is crucial f, Font: TimesNewRomanPSMT, Size: 12.0
Text: or expediting the discovery of peptides encoded by these RNAs. Computational prediction metho, Font: TimesNewRomanPSMT, Size: 12.0
Text: ds play a vital role in this task. In this context, LSCPP-BERT (https://github.com/Sakuraxia/LSC, Font: TimesNewRomanPSMT, Size: 12.0
Text: PP-BERT) emerges as a novel method designed for predicting the coding potential of lncRNA-s, Font: TimesNewRomanPSMT, Size: 12.0
Text: ORFs in plants. Leveraging pre-trained bidirectional encoder representations from transformer m, Font: TimesNewRomanPSMT, Size: 12.0
Text: odels, LSCPP-BERT offers a reliable tool for predicting coding lncRNA-sORFs and holds the po, Font: TimesNewRomanPSMT, Size: 12.0

Text: tential to significantly contribute to drug development and agricultural applications by enhancing, Font: TimesNewRomanPSMT, Size: 12.0
Text:  our understanding of the coding potential within lncRNAs. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.2.4 RNA sequence language models used to predict the RNA‚ÄìRBP interactions , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: RNA sequences differ from DNA sequences by a single base (thymine to uracil), resulting in a , Font: TimesNewRomanPSMT, Size: 12.0
Text: singular variance where the syntax and semantics largely remain congruent. The versatility of , Font: TimesNewRomanPSMT, Size: 12.0
Text: BERT extends beyond DNA to encompass Cross-linking and Immunoprecipitation (CLIP-seq) , Font: TimesNewRomanPSMT, Size: 12.0
Text: data, providing a valuable tool for predicting the binding preferences of RNA-binding proteins , Font: TimesNewRomanPSMT, Size: 12.0
Text: (RBPs). BERT-RBP [39] is a model specifically designed for forecasting RNA-RBP interactions. , Font: TimesNewRomanPSMT, Size: 12.0
Text: It adapts the BERT architecture and is pre-trained on a human reference genome. BERT-RBP , Font: TimesNewRomanPSMT, Size: 12.0
Text: outperforms contemporary prediction models when assessed against eCLIP-seq data from 154 , Font: TimesNewRomanPSMT, Size: 12.0
Text: RBPs. Additionally, the model exhibits the ability to discern both transcript region types and RNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: secondary structures based solely on sequence information. In essence, BERT-RBP not only , Font: TimesNewRomanPSMT, Size: 12.0
Text: contributes insights into the fine-tuning mechanisms of BERT in biological contexts but also , Font: TimesNewRomanPSMT, Size: 12.0
Text: provides compelling evidence of the model's versatility in addressing various challenges related to , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA, showcasing its potential in advancing our understanding of RNA-protein interactions. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.2.5 RNA sequence language models used to predict the RNA modification , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Post-transcriptional modifications of RNA play a crucial role in a diverse array of biological , Font: TimesNewRomanPSMT, Size: 12.0
Text: processes. Among these modifications, N7-methylguanosine (m7G) stands out as one of the most , Font: TimesNewRomanPSMT, Size: 12.0
Text: prevalent, playing an essential role in gene expression regulation [36]. The precise identification , Font: TimesNewRomanPSMT, Size: 12.0
Text: of m7G sites within the transcriptome is of immense value for a comprehensive understanding of , Font: TimesNewRomanPSMT, Size: 12.0
Text: their potential functional mechanisms. While high-throughput experimental methods offer precise , Font: TimesNewRomanPSMT, Size: 12.0
Text: localization of m7G sites, their costliness and time-consuming nature present challenges. In , Font: TimesNewRomanPSMT, Size: 12.0
Text: response to this, BERT-m7G [40] emerges as a transformative computational tool. Grounded in , Font: TimesNewRomanPSMT, Size: 12.0
Text: the transformer architecture of BERT and utilizing stacking ensemble techniques, BERT-m7G , Font: TimesNewRomanPSMT, Size: 12.0
Text: excels at identifying RNA N7-methylguanosine sites solely from RNA sequence information. This , Font: TimesNewRomanPSMT, Size: 12.0
Text: computational method proves imperative in accurately pinpointing m7G sites, providing a more , Font: TimesNewRomanPSMT, Size: 12.0
Text: efficient alternative to labor-intensive experimental approaches. BERT-m7G showcases the power , Font: TimesNewRomanPSMT, Size: 12.0
Text: of computational approaches in unraveling post-transcriptional modifications, offering a valuable , Font: TimesNewRomanPSMT, Size: 12.0
Text: tool for researchers seeking to understand the functional implications of m7G in gene expression , Font: TimesNewRomanPSMT, Size: 12.0
Text: regulation. , Font: TimesNewRomanPSMT, Size: 12.0

Text: The post-transcriptional 2‚Äô-O-methylation (Nm) RNA modification plays a significant role in , Font: TimesNewRomanPSMT, Size: 12.0
Text: diverse cellular processes and is linked to several diseases [41]. To gain profound insights into the , Font: TimesNewRomanPSMT, Size: 12.0
Text: underlying biological mechanisms, the Bert2Ome [42] method proves to be an efficient tool for , Font: TimesNewRomanPSMT, Size: 12.0
Text: inferring 2‚Äô-O-methylation RNA modification sites directly from RNA sequences. Bert2Ome , Font: TimesNewRomanPSMT, Size: 12.0
Text: integrates a BERT-based model with Convolutional Neural Networks (CNN) to discern the , Font: TimesNewRomanPSMT, Size: 12.0
Text: intricate relationship between modification sites and the content of RNA sequences. This , Font: TimesNewRomanPSMT, Size: 12.0
Text: innovative approach not only reduces the time required for labor-intensive biological experiments , Font: TimesNewRomanPSMT, Size: 12.0
Text: but also surpasses existing methodologies across various datasets and species, demonstrating , Font: TimesNewRomanPSMT, Size: 12.0
Text: superior performance across multiple metrics. Bert2Ome showcases the power of computational , Font: TimesNewRomanPSMT, Size: 12.0
Text: methods in advancing our understanding of post-transcriptional RNA modifications, providing a , Font: TimesNewRomanPSMT, Size: 12.0
Text: valuable tool for researchers exploring the role of 2‚Äô-O-methylation in diverse cellular contexts , Font: TimesNewRomanPSMT, Size: 12.0
Text: and diseases. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.2.6 RNA sequence language models used to predict protein expression and mRNA , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: degradation , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: mRNA vaccines have emerged as a cost-effective, swift, and secure alternative to traditional , Font: TimesNewRomanPSMT, Size: 12.0
Text: vaccines, displaying high potency[43]. The mechanism of action for mRNA vaccines involves , Font: TimesNewRomanPSMT, Size: 12.0
Text: introducing a segment of mRNA corresponding to a viral protein, typically derived from the virus's , Font: TimesNewRomanPSMT, Size: 12.0
Text: outer membrane. Thus, CodonBERT[44] has been specifically designed for mRNA sequences to , Font: TimesNewRomanPSMT, Size: 12.0
Text: predict protein expression. Utilizing a multi-head attention transformer architecture framework, , Font: TimesNewRomanPSMT, Size: 12.0
Text: CodonBERT underwent pre-training on a vast dataset comprising 10 million mRNA coding , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequences from diverse organisms. This extensive pre-training equips CodonBERT to excel in , Font: TimesNewRomanPSMT, Size: 12.0
Text: various mRNA prediction tasks, including protein expression and mRNA degradation prediction. , Font: TimesNewRomanPSMT, Size: 12.0
Text: One of CodonBERT's notable strengths lies in its capacity to assimilate new biological information, , Font: TimesNewRomanPSMT, Size: 12.0
Text: positioning it as an asset for advancing mRNA vaccine design. By surpassing existing state-of-, Font: TimesNewRomanPSMT, Size: 12.0
Text: the-art methods, CodonBERT contributes to optimizing mRNA-based vaccine development, , Font: TimesNewRomanPSMT, Size: 12.0
Text: promising improved efficacy and broader applicability in the realm of immunization. The model's , Font: TimesNewRomanPSMT, Size: 12.0
Text: proficiency in predicting protein expression levels enhances its utility in designing mRNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequences for vaccines, ultimately impacting the efficiency and effectiveness of the vaccine , Font: TimesNewRomanPSMT, Size: 12.0
Text: development process. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3 Applications of large language models in proteomics, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0

Text: Protein is an indispensable molecule in life, assuming a pivotal role in the construction and , Font: TimesNewRomanPSMT, Size: 12.0
Text: sustenance of vital processes. As the field of protein research advances, there has been a substantial , Font: TimesNewRomanPSMT, Size: 12.0
Text: surge in the accumulation of protein data [45]. In this context, the utilization of large language , Font: TimesNewRomanPSMT, Size: 12.0
Text: models emerges as a viable approach to extract pertinent and valuable information from these vast , Font: TimesNewRomanPSMT, Size: 12.0
Text: reservoirs of data. Several pre-trained protein language models (PPLMs) have been proposed to , Font: TimesNewRomanPSMT, Size: 12.0
Text: learn the characteristic representations of proteins data (e.g., protein sequences, gene ontology , Font: TimesNewRomanPSMT, Size: 12.0
Text: annotations, property descriptions), then applied to different tasks by fine-tuning, adding or , Font: TimesNewRomanPSMT, Size: 12.0
Text: altering downstream networks, such as protein structure, post-translational modifications (PTMs), , Font: TimesNewRomanPSMT, Size: 12.0
Text: and biophysical properties, which align with corresponding downstream tasks like secondary , Font: TimesNewRomanPSMT, Size: 12.0
Text: structure prediction, major PTMs prediction, and stability prediction [46, 47].  , Font: TimesNewRomanPSMT, Size: 12.0
Text: Even though antibodies are classified as proteins, the datasets of antibodies and subsequent tasks , Font: TimesNewRomanPSMT, Size: 12.0
Text: differ significantly from those of proteins. Through the establishment and continuous updates of , Font: TimesNewRomanPSMT, Size: 12.0
Text: the, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Observed Antibody Space (OAS) database [48], a substantial amount of antibody sequence , Font: TimesNewRomanPSMT, Size: 12.0
Text: data has become available, which can be utilized to facilitate the development of pre-trained , Font: TimesNewRomanPSMT, Size: 12.0
Text: antibody large language models (PALMs). PALMs primarily delve into downstream topics , Font: TimesNewRomanPSMT, Size: 12.0
Text: encompassing therapeutic antibody binding mechanisms, immune evolution, and antibody , Font: TimesNewRomanPSMT, Size: 12.0
Text: discovery, which correspond to tasks like paratope prediction, B cell maturation analysis, and , Font: TimesNewRomanPSMT, Size: 12.0
Text: antibody sequence classification (, Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 5, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: ).  , Font: TimesNewRomanPSMT, Size: 12.0
Text: In this section, some of the popular protein-related large language models of recent years are , Font: TimesNewRomanPSMT, Size: 12.0
Text: introduced, as well as corresponding important downstream tasks. It is important to emphasize that , Font: TimesNewRomanPSMT, Size: 12.0
Text: the capabilities of both PPLMs and PALMs extend beyond the specific downstream tasks outlined , Font: TimesNewRomanPSMT, Size: 12.0
Text: in this section. For further details, additional information can be referenced within , Font: TimesNewRomanPSMT, Size: 12.0
Text: Table 2, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  and , Font: TimesNewRomanPSMT, Size: 12.0
Text: Supplementary Table 2, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: . , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3.1 Protein language models for secondary structure and contact prediction , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The structure of proteins plays a crucial and decisive role in their function and interactions [49]. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Nonetheless, the conventional laboratory-based techniques employed for protein structure analysis , Font: TimesNewRomanPSMT, Size: 12.0
Text: are frequently characterized by their time-consuming, labor-intensive nature. In addition to , Font: TimesNewRomanPSMT, Size: 12.0
Text: traditional template-based and physics-based methods, with the development of deep learning, the , Font: TimesNewRomanPSMT, Size: 12.0
Text: use of large language models to predict protein structures has gradually shown advantages in , Font: TimesNewRomanPSMT, Size: 12.0
Text: computational speed and prediction accuracy [50]. , Font: TimesNewRomanPSMT, Size: 12.0

Text: MSA Transformer [51] presents a protein language model that takes a set of sequences as input in , Font: TimesNewRomanPSMT, Size: 12.0
Text: the form of a multiple sequence alignment (MSA). This model employs a unique mechanism of , Font: TimesNewRomanPSMT, Size: 12.0
Text: interleaved row and column attention across the input sequences. After model trained with a , Font: TimesNewRomanPSMT, Size: 12.0
Text: variant of MLM objective across many protein families, it outperformed other unsupervised , Font: TimesNewRomanPSMT, Size: 12.0
Text: approaches at the time. Furthermore, it exhibits superior parameter efficiency compared to prior , Font: TimesNewRomanPSMT, Size: 12.0
Text: state-of-the-art protein language models. When using PPLMs to predict secondary structure or , Font: TimesNewRomanPSMT, Size: 12.0
Text: contact, based on the experience brought by BERT, it seems that using a language model with a , Font: TimesNewRomanPSMT, Size: 12.0
Text: larger number of parameters is easier to achieve better performance. Few models seem to have , Font: TimesNewRomanPSMT, Size: 12.0
Text: more parameters than the largest models in ProtTrans [52]. ProtTrans trains a series of large , Font: TimesNewRomanPSMT, Size: 12.0
Text: language models based on two autoregressive models (Transformer-XL [53], XLNet [28]) and , Font: TimesNewRomanPSMT, Size: 12.0
Text: four automatic encoder models (BERT [2], Albert [27], Electra [29], T5 [54]) on data from UniRef , Font: TimesNewRomanPSMT, Size: 12.0
Text: [55] and BFD [56] [57] containing up to 393 billion amino acids. The parameters of models range , Font: TimesNewRomanPSMT, Size: 12.0
Text: from millions to billions. In addition to predicting secondary structure, it is worth highlighting that , Font: TimesNewRomanPSMT, Size: 12.0
Text: ProtTrans achieved a significant breakthrough in per-residue predictions. For the first time, the , Font: TimesNewRomanPSMT, Size: 12.0
Text: transfer of the most informative embeddings (ProtT5) outperformed the state-of-the-art methods , Font: TimesNewRomanPSMT, Size: 12.0
Text: without relying on evolutionary information, thus bypassing the need for costly database searches. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3.2 Protein language models for protein sequence generation , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Generation of protein has broad application prospects in fields such as drug design and protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: engineering [58]. By using methods such as machine learning or deep learning, protein sequences , Font: TimesNewRomanPSMT, Size: 12.0
Text: can be generated. The generated sequences are hoped to have good foldability so that they can , Font: TimesNewRomanPSMT, Size: 12.0
Text: form stable three-dimensional structures. Moreover, the desired proteins are expected to exhibit , Font: TimesNewRomanPSMT, Size: 12.0
Text: specific functional properties, including enzyme activity and antibody binding capability. The , Font: TimesNewRomanPSMT, Size: 12.0
Text: advancement of large language models and the integration of conditional models have significantly , Font: TimesNewRomanPSMT, Size: 12.0
Text: propelled the progress in the field of protein generation [59]. , Font: TimesNewRomanPSMT, Size: 12.0
Text: The model, referred to as ProGen [60], incorporates UniprotKB Keywords as conditional tags in , Font: TimesNewRomanPSMT, Size: 12.0
Text: 2020. These tags encompass a vocabulary consisting of various categories, including 'biological , Font: TimesNewRomanPSMT, Size: 12.0
Text: process', 'cellular component', and 'molecular function'. In total, the conditional tags encompass , Font: TimesNewRomanPSMT, Size: 12.0
Text: over 1,100 distinct terms. When assessing protein sequences generated by ProGen using metrics , Font: TimesNewRomanPSMT, Size: 12.0
Text: for sequence similarity, secondary structure accuracy, and conformational energy, they exhibit , Font: TimesNewRomanPSMT, Size: 12.0
Text: desired structural properties. In 2022, inspired by the remarkable achievements of generative , Font: TimesNewRomanPSMT, Size: 12.0
Text: Transformer-based language models like the GPT-x series, the development of ProtGPT2 [61] , Font: TimesNewRomanPSMT, Size: 12.0

Text: emerged. Notably, the proteins generated by ProtGPT2 exhibit amino acid propensities t following , Font: TimesNewRomanPSMT, Size: 12.0
Text: the principles of natural ones. Assessments involving disorder and secondary structure prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: reveal that a substantial majority (88%) of ProtGPT2-generated proteins possess globular , Font: TimesNewRomanPSMT, Size: 12.0
Text: characteristics, aligning with the attributes found in natural sequences. Employing AlphaFold [62, , Font: TimesNewRomanPSMT, Size: 12.0
Text: 63] on ProtGPT2 sequences yields well-folded non-idealized structures, encompassing the , Font: TimesNewRomanPSMT, Size: 12.0
Text: presence of extensive loops, and the emergence of previously unseen topologies that are absent , Font: TimesNewRomanPSMT, Size: 12.0
Text: from current structure databases. It appears that ProtGPT2 has acquired the language specific to , Font: TimesNewRomanPSMT, Size: 12.0
Text: proteins. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3.3 Protein language models for protein function prediction , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Proteins are molecules that play a crucial role in various aspects of cellular metabolism, signal , Font: TimesNewRomanPSMT, Size: 12.0
Text: transduction, and structural support in living organisms. A deep understanding of the function of , Font: TimesNewRomanPSMT, Size: 12.0
Text: proteins in living organisms is of great significance for drug development and disease mechanism , Font: TimesNewRomanPSMT, Size: 12.0
Text: analysis. The diversity and complexity of proteins make it difficult to accurately predict and , Font: TimesNewRomanPSMT, Size: 12.0
Text: annotate their functions. Fortunately, PPLMs can effectively address these challenges [64, 65]. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Taking into account the substantial presence of local semantics within protein sequences, a novel , Font: TimesNewRomanPSMT, Size: 12.0
Text: approach to pre-training modeling, referred to as SPRoBERTa [66], was introduced in 2022. This , Font: TimesNewRomanPSMT, Size: 12.0
Text: method takes protein sequences as inputs and offers the flexibility of straightforward fine-tuning , Font: TimesNewRomanPSMT, Size: 12.0
Text: for diverse protein-related tasks, encompassing prediction tasks at the protein-level tasks (such as , Font: TimesNewRomanPSMT, Size: 12.0
Text: remote homology prediction and protein function prediction), as well as amino acid level (e.g., , Font: TimesNewRomanPSMT, Size: 12.0
Text: secondary structure prediction) and amino acid pair-level (e.g., contact prediction). In the next , Font: TimesNewRomanPSMT, Size: 12.0
Text: year, ProtST [67] introduced a multimodal training framework for proteins, which involves the , Font: TimesNewRomanPSMT, Size: 12.0
Text: integration of a protein language model (PLM) whose input is protein sequences and a biomedical , Font: TimesNewRomanPSMT, Size: 12.0
Text: language model (BLM) whose input is protein property descriptions into a large multimodal model. , Font: TimesNewRomanPSMT, Size: 12.0
Text: This integration is achieved through three pre-training tasks: unimodal mask prediction, , Font: TimesNewRomanPSMT, Size: 12.0
Text: multimodal representation alignment, and multimodal mask prediction. The proposed model , Font: TimesNewRomanPSMT, Size: 12.0
Text: demonstrates exceptional performance in diverse downstream tasks related to protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: representation. In addition to finishing the task of protein function annotation, it shows the , Font: TimesNewRomanPSMT, Size: 12.0
Text: effectiveness on zero-shot protein classification. Furthermore, the model possesses the capability , Font: TimesNewRomanPSMT, Size: 12.0
Text: to facilitate the retrieval of functional proteins from a vast-scale database, even in the absence of , Font: TimesNewRomanPSMT, Size: 12.0
Text: any functional annotation. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3.4 Protein language models for major post-translational modification prediction , Font: TimesNewRomanPS-BoldMT, Size: 12.0

Text: Post-translational modification (PTM) refers to the process in which the structure and function of , Font: TimesNewRomanPSMT, Size: 12.0
Text: proteins are changed through a series of chemical modification reactions after translation is , Font: TimesNewRomanPSMT, Size: 12.0
Text: completed, including various chemical changes such as phosphorylation, methylation, acetylation, , Font: TimesNewRomanPSMT, Size: 12.0
Text: and glycosylation of proteins. These modifications can significantly impact protein stability, , Font: TimesNewRomanPSMT, Size: 12.0
Text: subcellular localization, interactions, and functional expression. In-depth investigation of PTMs , Font: TimesNewRomanPSMT, Size: 12.0
Text: yields valuable insights for disease diagnosis and therapeutic interventions [68, 69]. Language , Font: TimesNewRomanPSMT, Size: 12.0
Text: models can perform tasks such as signal peptide prediction and major PTMs prediction effectively. , Font: TimesNewRomanPSMT, Size: 12.0
Text: ProteinBERT [70] is not really a large language model in terms of parameters (only ~ 16M), but , Font: TimesNewRomanPSMT, Size: 12.0
Text: thanks to the introduction of the GO annotation prediction task and the interaction of GO with , Font: TimesNewRomanPSMT, Size: 12.0
Text: protein sequences, compared with other deep learning models with larger parameters, this model , Font: TimesNewRomanPSMT, Size: 12.0
Text: has achieved considerable or even better performance on multiple benchmarks covering diverse , Font: TimesNewRomanPSMT, Size: 12.0
Text: protein properties including major PTMs prediction. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3.5 Protein language models for evolution and mutation prediction , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: During the process of biological evolution, the sequence and structure of proteins undergo changes. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Evolution and mutation serve as vital mechanisms that generate functional diversity in proteins , Font: TimesNewRomanPSMT, Size: 12.0
Text: [71]. Gaining insights into the process of protein evolution and mutation can reveal strategies for , Font: TimesNewRomanPSMT, Size: 12.0
Text: organisms to adapt to environmental changes and survival competition, as well as the origin and , Font: TimesNewRomanPSMT, Size: 12.0
Text: evolution of protein function, and provide new ideas for drug development and disease treatment , Font: TimesNewRomanPSMT, Size: 12.0
Text: [72]. , Font: TimesNewRomanPSMT, Size: 12.0
Text: In the context of protein sequence inputs, early protein language models treated an entire sequence , Font: TimesNewRomanPSMT, Size: 12.0
Text: as either a paragraph or a sentence, with individual amino acids representing individual words [73, , Font: TimesNewRomanPSMT, Size: 12.0
Text: 74]. In 2019, a model known as UniRep [75], built upon the Long Short-Term Memory (LSTM) , Font: TimesNewRomanPSMT, Size: 12.0
Text: architecture emerged. This model underwent training using the UniRef50 [55] dataset and , Font: TimesNewRomanPSMT, Size: 12.0
Text: exhibited a remarkable improvement in efficiency, surpassing other models in several tasks , Font: TimesNewRomanPSMT, Size: 12.0
Text: including remote homology detection and mutational effect prediction. Since 2020, more and more , Font: TimesNewRomanPSMT, Size: 12.0
Text: large language models of protein have been proposed to perform prediction in evolution and , Font: TimesNewRomanPSMT, Size: 12.0
Text: mutation. In 2020, a deep transformer model known as ESM-1b [76] underwent training on a vast , Font: TimesNewRomanPSMT, Size: 12.0
Text: and diverse dataset comprising 250 million sequences. This training enabled the model to acquire , Font: TimesNewRomanPSMT, Size: 12.0
Text: protein sequence representations encompassing essential characteristics. The architecture of the , Font: TimesNewRomanPSMT, Size: 12.0
Text: model comprised 33 layers, housing approximately 650 million parameters. To facilitate its , Font: TimesNewRomanPSMT, Size: 12.0
Text: training, ESM-1b utilized self-supervised strategy, masking language modeling objective. This , Font: TimesNewRomanPSMT, Size: 12.0

Text: approach allowed the model to learn and capture crucial patterns and dependencies within the , Font: TimesNewRomanPSMT, Size: 12.0
Text: protein sequences, thereby enhancing its overall performance and representation capabilities. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3.6 Protein language models for biophysical properties prediction , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The biophysical properties of proteins include fluorescence landscapes, stability landscapes, and , Font: TimesNewRomanPSMT, Size: 12.0
Text: so on [77]. Accurate prediction of these properties plays a crucial role in advancing our , Font: TimesNewRomanPSMT, Size: 12.0
Text: understanding of protein folding mechanisms, stability, conformational alterations, and so on. This , Font: TimesNewRomanPSMT, Size: 12.0
Text: is of great significance for the development of drug design, protein engineering, enzyme , Font: TimesNewRomanPSMT, Size: 12.0
Text: engineering, and other relevant fields. The progressive advancements in deep learning have , Font: TimesNewRomanPSMT, Size: 12.0
Text: enabled the efficient utilization of the continuously evolving PPLMs for the precise prediction of , Font: TimesNewRomanPSMT, Size: 12.0
Text: biophysical properties associated with proteins. , Font: TimesNewRomanPSMT, Size: 12.0
Text: A significant development known as Tasks Assessing Protein Embeddings (TAPE) [78] emerged , Font: TimesNewRomanPSMT, Size: 12.0
Text: in 2019, which introduced a comprehensive benchmark of protein bioinformatics tasks. This paper , Font: TimesNewRomanPSMT, Size: 12.0
Text: aimed to establish a standardized evaluation system for protein transfer learning by providing well-, Font: TimesNewRomanPSMT, Size: 12.0
Text: defined tasks, curated datasets, and rigorous assessment metrics. The task set encompassed five , Font: TimesNewRomanPSMT, Size: 12.0
Text: distinct problems including fluorescence landscape prediction and stability landscape prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: and spanning three major aspects of protein analysis: protein structure prediction, remote protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: homolog detection, and protein design. This systematic approach facilitated the rigorous , Font: TimesNewRomanPSMT, Size: 12.0
Text: evaluation and comparison of different methodologies and models within the field of protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: transfer learning. In 2022, PromptProtein [79] (taking protein sequences as inputs) stands as the , Font: TimesNewRomanPSMT, Size: 12.0
Text: pioneering prompt-based pre-trained protein model, aiming to address various levels of protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: structures through prompt-guided multi-task pre-training. Furthermore, it incorporates a prompt , Font: TimesNewRomanPSMT, Size: 12.0
Text: fine-tuning module, enabling downstream tasks to effectively leverage specific levels of structural , Font: TimesNewRomanPSMT, Size: 12.0
Text: information as required. Through extensive experimentation in the domains of function prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: and biophysical properties prediction, PromptProtein demonstrates significant superiority over , Font: TimesNewRomanPSMT, Size: 12.0
Text: existing methods, showcasing substantial performance gains. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3.7 Protein language models for protein-protein interaction and binding affinity prediction , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Protein-protein interaction (PPI) constitutes a fundamental molecular-level process in biological , Font: TimesNewRomanPSMT, Size: 12.0
Text: activities, and its prediction holds profound significance in the realm of drug discovery and design. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Investigating the interaction between proteins can help us discover new drug targets and design , Font: TimesNewRomanPSMT, Size: 12.0
Text: drugs with high efficiency and selectivity. PPLMs can help us efficiently and relatively accurately , Font: TimesNewRomanPSMT, Size: 12.0
Text: obtain protein-protein interaction types and binding affinities between proteins [80, 81]. , Font: TimesNewRomanPSMT, Size: 12.0

Text: The underlying motivation behind the KeAP [82] model aligns with that of ProtST, as both aim to , Font: TimesNewRomanPSMT, Size: 12.0
Text: incorporate more fine-grained information compared to OntoProtein [83]. KeAP adopts a triplet , Font: TimesNewRomanPSMT, Size: 12.0
Text: format consisting of (Protein, Relation, Attribute) as input, which is subsequently processed by , Font: TimesNewRomanPSMT, Size: 12.0
Text: distinct encoders and a specially designed cascaded decoder based on the Transformer architecture. , Font: TimesNewRomanPSMT, Size: 12.0
Text: The model employs Masked Language Modeling (MLM) as the primary pre-training task, , Font: TimesNewRomanPSMT, Size: 12.0
Text: facilitating efficient training. Leveraging its unique cross-attention fusion mechanism, the model , Font: TimesNewRomanPSMT, Size: 12.0
Text: excels in capturing intricate protein information at a finer granularity. As a result, KeAP exhibits , Font: TimesNewRomanPSMT, Size: 12.0
Text: exceptional performance across nine diverse downstream tasks including protein-protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: interaction identification and protein-protein binding affinity estimation. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.3.8 Antibody large language models for antigen-receptor binding and antigen-antibody , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: binding prediction , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Antigen proteins break down in the cytoplasm, forming neoantigen peptides. These peptides bind , Font: TimesNewRomanPSMT, Size: 12.0
Text: to the Major Histocompatibility Complex (MHC), creating pMHC complexes. After a series of , Font: TimesNewRomanPSMT, Size: 12.0
Text: steps, these complexes reach the cell membrane for presentation. Subsequently, the T-cell , Font: TimesNewRomanPSMT, Size: 12.0
Text: Receptor (TCR) recognizes the pMHC complex, stimulating B cells to produce antibodies, , Font: TimesNewRomanPSMT, Size: 12.0
Text: triggering an immune response [84] . , Font: TimesNewRomanPSMT, Size: 12.0
Text: The application of language models in this process aims at accurately predicting the binding of , Font: TimesNewRomanPSMT, Size: 12.0
Text: peptides to HLA molecules as a key objective[85, 86]. Peptides serve as a life's language, with , Font: TimesNewRomanPSMT, Size: 12.0
Text: large language models excelling in extracting context, particularly in pMHC binding and , Font: TimesNewRomanPSMT, Size: 12.0
Text: presentation prediction. For instance, MHCRoBERTa [87] utilizes the pretrained BERT to model , Font: TimesNewRomanPSMT, Size: 12.0
Text: the input amino acid sequences. Through effectively learning the biological meanings of each , Font: TimesNewRomanPSMT, Size: 12.0
Text: token, the MHCRoBERTa model is able to distinguish between different alleles. However, , Font: TimesNewRomanPSMT, Size: 12.0
Text: MHCRoBERTa primarily focused on pMHC-I prediction. BERTMHC [88] is a specific pMHC-, Font: TimesNewRomanPSMT, Size: 12.0
Text: II binding predicting method that incorporated 2,413 MHC‚Äìpeptide pairs, encompassing 47 MHC , Font: TimesNewRomanPSMT, Size: 12.0
Text: class II alleles, into the training data., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: This effectively fills a gap in the field of pMHC-II binding , Font: TimesNewRomanPSMT, Size: 12.0
Text: prediction. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Another key goal is predicting the binding specificity of adaptive immune receptors (AIRs) for , Font: TimesNewRomanPSMT, Size: 12.0
Text: antigens. This variability in specificity mainly arises from the flexibility of three complementarity-, Font: TimesNewRomanPSMT, Size: 12.0
Text: determining region (CDR) loops (CDR1-3), with CDR3 being crucial for binding to antigenic , Font: TimesNewRomanPSMT, Size: 12.0
Text: peptides[89]. TCR-BERT [90] leverages unlabeled TCR CDR3 sequences to learn a general , Font: TimesNewRomanPSMT, Size: 12.0
Text: representation of TCR sequences. This enables downstream applications for predicting the antigen , Font: TimesNewRomanPSMT, Size: 12.0

Text: specificity in TCR recognition. However, TCR-BERT trains each individual AIR chain separately , Font: TimesNewRomanPSMT, Size: 12.0
Text: and fails to understand the paired interaction between the two chains of AIR. Jianhua Yao et al., , Font: TimesNewRomanPSMT, Size: 12.0
Text: effectively addressed this issue by pre-training a specially designed BERT model, SC-AIR-BERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: [91], which outperforms other state-of-the-art methods in both TCR and BCR antigen-binding , Font: TimesNewRomanPSMT, Size: 12.0
Text: specificity prediction tasks. , Font: TimesNewRomanPSMT, Size: 12.0
Text: After the antigen recognition process by T cells concludes, it stimulates B cells to produce specific , Font: TimesNewRomanPSMT, Size: 12.0
Text: antibodies that bind to the corresponding antigen[92]. In antibody language models section, three , Font: TimesNewRomanPSMT, Size: 12.0
Text: recent studies from PALMs will be introduced.  PALMs are presented independently mainly , Font: TimesNewRomanPSMT, Size: 12.0
Text: because of the differences between their downstream tasks and those of PPLMs. It is also worth , Font: TimesNewRomanPSMT, Size: 12.0
Text: mentioning that the research on the large language model of antibodies is also a hot topic of recent , Font: TimesNewRomanPSMT, Size: 12.0
Text: research. , Font: TimesNewRomanPSMT, Size: 12.0
Text: AbLang [93], an antibody language model built upon RoBERTa [94], was developed with the , Font: TimesNewRomanPSMT, Size: 12.0
Text: hypothesis that models trained on antibody databases would exhibit superior performance in , Font: TimesNewRomanPSMT, Size: 12.0
Text: addressing antibody-related challenges. One of the specific problems AbLang aims to solve is the , Font: TimesNewRomanPSMT, Size: 12.0
Text: restoration of residues that are lost during the sequencing process due to errors. Comparative , Font: TimesNewRomanPSMT, Size: 12.0
Text: evaluations have demonstrated that AbLang outperforms both IMGT germlines [95] and the , Font: TimesNewRomanPSMT, Size: 12.0
Text: general protein language model ESM-1b [76] in terms of accurately restoring the missing residues , Font: TimesNewRomanPSMT, Size: 12.0
Text: in antibody sequences. Furthermore, AbLang achieves this with increased efficiency, , Font: TimesNewRomanPSMT, Size: 12.0
Text: demonstrating a higher processing speed compared to the aforementioned models. , Font: TimesNewRomanPSMT, Size: 12.0
Text: AntiBERTa [96] leverages the latent vectors derived from protein sequences, the model exhibits a , Font: TimesNewRomanPSMT, Size: 12.0
Text: discernible comprehension of the antibody "language" to a certain degree, as evidenced by the , Font: TimesNewRomanPSMT, Size: 12.0
Text: visual representations generated. The model's ability is exemplified through a diverse array of tasks, , Font: TimesNewRomanPSMT, Size: 12.0
Text: including tracing the B cell origin of the antibody, quantifying immunogenicity, and predicting the , Font: TimesNewRomanPSMT, Size: 12.0
Text: antibody's binding site. , Font: TimesNewRomanPSMT, Size: 12.0
Text: The EATLM [97] is a straightforward architecture composed of a series of stacked transformer , Font: TimesNewRomanPSMT, Size: 12.0
Text: layers. Its uniqueness lies in the pre-training tasks it employs. Alongside the conventional Masked , Font: TimesNewRomanPSMT, Size: 12.0
Text: Language Modeling (MLM), the model introduces additional pre-training tasks, namely Ancestor , Font: TimesNewRomanPSMT, Size: 12.0
Text: Germline Prediction (AGP) and, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: Mutation Position Prediction (MPP). These tasks aim to , Font: TimesNewRomanPSMT, Size: 12.0
Text: incorporate specific biological mechanisms into the pre-training phase. The most important , Font: TimesNewRomanPSMT, Size: 12.0
Text: contribution of the paper that proposed this model was to propose a reliable antibody-specific , Font: TimesNewRomanPSMT, Size: 12.0
Text: benchmark to evaluate different pre-protein language models and antibody language models. , Font: TimesNewRomanPSMT, Size: 12.0

Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: 3.4 Applications of large language models in drug discovery , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Drug discovery is an expensive and long-term process that exhibits a low success rate. During the , Font: TimesNewRomanPSMT, Size: 12.0
Text: early stages of drug discovery, computer-aided drug discovery, employing empirical or expert , Font: TimesNewRomanPSMT, Size: 12.0
Text: knowledge algorithms, machine learning algorithms, and deep learning algorithms, serve to , Font: TimesNewRomanPSMT, Size: 12.0
Text: accelerate the generation and screening of drug molecules and their lead compounds [98-100]., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: It , Font: TimesNewRomanPSMT, Size: 12.0
Text: speeds up the entire drug discovery process, especially the development of small molecule drugs. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Among commonly used medications, small molecule drugs can account for up to 98% of the total , Font: TimesNewRomanPSMT, Size: 12.0
Text: [101]. The structure of small molecule drugs exhibits excellent spatial dispersibility, and their , Font: TimesNewRomanPSMT, Size: 12.0
Text: chemical properties determine their good drug-like properties and pharmacokinetic properties , Font: TimesNewRomanPSMT, Size: 12.0
Text: [102]. With the development of deep learning and the proposal of large language models, it has , Font: TimesNewRomanPSMT, Size: 12.0
Text: become easy to apply these methods to discover hidden patterns of molecules and interactions , Font: TimesNewRomanPSMT, Size: 12.0
Text: between molecules for drugs (such as small molecules) and targets (such as proteins and RNA) , Font: TimesNewRomanPSMT, Size: 12.0
Text: that can be easily represented as sequence data. The Simplified Molecular-Input Line-Entry , Font: TimesNewRomanPSMT, Size: 12.0
Text: System (SMILES) string and chemical fingerprint are commonly used to represent molecules., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: Additionally, through the pooling process of graph neural networks(GNN), small molecules can , Font: TimesNewRomanPSMT, Size: 12.0
Text: be transformed into sequential representations [103]. With the protein sequence, large language , Font: TimesNewRomanPSMT, Size: 12.0
Text: models can engage in drug discovery through various inputs., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Within this section, key tasks within , Font: TimesNewRomanPSMT, Size: 12.0
Text: the early drug discovery process that have effectively leveraged large language models will be , Font: TimesNewRomanPSMT, Size: 12.0
Text: introduced (, Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 6, Table 3, Supplementary Table 3, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: )., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: 3.4.1, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Arial-BoldMT, Size: 12.0
Text: Large language models for drug-like molecular properties prediction, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri-Bold, Size: 8.039999961853027
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: During the drug discovery process, significant attention is devoted to specific properties associated , Font: TimesNewRomanPSMT, Size: 12.0
Text: with candidate molecules, such as Absorption, Distribution, Metabolism, Excretion and , Font: TimesNewRomanPSMT, Size: 12.0
Text: Toxicology (ADMET) and Pharmacokinetics (PK). The objective is to facilitate the development , Font: TimesNewRomanPSMT, Size: 12.0
Text: of more efficacious, accessible, and safe drugs [104, 105]. The utilization of large language models , Font: TimesNewRomanPSMT, Size: 12.0
Text: in molecular property prediction encompasses downstream tasks that involve predicting these , Font: TimesNewRomanPSMT, Size: 12.0
Text: properties. Since the input of molecular SMILES representation is consistent, it is easy to improve , Font: TimesNewRomanPSMT, Size: 12.0
Text: and fine-tune the model based on specific datasets according to the requirements (properties of , Font: TimesNewRomanPSMT, Size: 12.0
Text: interest to researchers). , Font: TimesNewRomanPSMT, Size: 12.0
Text: In contrast to its predecessors, SMILES-BERT [106] departed from the usage of knowledge-based , Font: TimesNewRomanPSMT, Size: 12.0
Text: molecular fingerprints as input. Instead, it adopted a representation method where molecules were , Font: TimesNewRomanPSMT, Size: 12.0

Text: encoded as SMILES sequences and employed as input for both pre-training and fine-tuning within , Font: TimesNewRomanPSMT, Size: 12.0
Text: a BERT-based model. This novel approach yielded superior outcomes across various downstream , Font: TimesNewRomanPSMT, Size: 12.0
Text: molecular property prediction tasks, surpassing the performance of previous models reliant on , Font: TimesNewRomanPSMT, Size: 12.0
Text: molecular fingerprints. ChemBERTa [107] is also a network based on BERT, which is essentially , Font: TimesNewRomanPSMT, Size: 12.0
Text: not much different from previous methods, and even not the most advanced in terms of , Font: TimesNewRomanPSMT, Size: 12.0
Text: performance at that time. However, it emphasizes the scalability of models based on large language , Font: TimesNewRomanPSMT, Size: 12.0
Text: models, and explores the impact of pre-training dataset size, tokenizer, and string representation, , Font: TimesNewRomanPSMT, Size: 12.0
Text: which provides several directions for discussions on molecular property prediction methods based , Font: TimesNewRomanPSMT, Size: 12.0
Text: on large language models. K-BERT [108], similarly based on the BERT architecture, distinguishes , Font: TimesNewRomanPSMT, Size: 12.0
Text: itself through the adoption of three distinct pre-training tasks in its pre-training phase: atom feature , Font: TimesNewRomanPSMT, Size: 12.0
Text: prediction, molecular feature prediction, and contrastive learning. This unique approach empowers , Font: TimesNewRomanPSMT, Size: 12.0
Text: the model to transcend the mere discovery of the SMILES paradigm and ‚Äúcomprehend‚Äù the , Font: TimesNewRomanPSMT, Size: 12.0
Text: underlying essence of SMILES representations. As a result, K-BERT exhibits remarkable , Font: TimesNewRomanPSMT, Size: 12.0
Text: performance across 15 drug datasets, showcasing its competence in the field of drug discovery. , Font: TimesNewRomanPSMT, Size: 12.0
Text: It is worth noting that with the growth of data, more and more molecules can be easily represented , Font: TimesNewRomanPSMT, Size: 12.0
Text: as graphs and processed through graph neural networks. Given the importance of graph neural , Font: TimesNewRomanPSMT, Size: 12.0
Text: networks in the development of molecular pre-training models, A BERT-based molecular pre-, Font: TimesNewRomanPSMT, Size: 12.0
Text: training network will be briefly introduced to demonstrate the use of large language model variants. , Font: TimesNewRomanPSMT, Size: 12.0
Text: The design of pre-training tasks greatly affects the performance of large language models. Drawing , Font: TimesNewRomanPSMT, Size: 12.0
Text: inspiration from the MLM task in BERT, Mole-BERT [109], a BERT-based graph-based pre-, Font: TimesNewRomanPSMT, Size: 12.0
Text: training neural network, introduces atom-level Masked Atoms Modeling (MAM) task and graph-, Font: TimesNewRomanPSMT, Size: 12.0
Text: level Triplet Masked Contrastive Learning (TMCL) task. These tasks enable the network to , Font: TimesNewRomanPSMT, Size: 12.0
Text: acquire a comprehensive understanding of the ‚Äúlanguage‚Äù embedded within molecular graphs. By , Font: TimesNewRomanPSMT, Size: 12.0
Text: adopting this approach, the network achieves exceptional performance across eight downstream , Font: TimesNewRomanPSMT, Size: 12.0
Text: data task datasets. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.4.2, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Arial-BoldMT, Size: 12.0
Text: Large language models for drug-like molecules generation  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: It is very difficult to chase the full coverage of the enormous drug-like chemical space (estimated , Font: TimesNewRomanPSMT, Size: 12.0
Text: at more than 10, Font: TimesNewRomanPSMT, Size: 12.0
Text: 63, Font: TimesNewRomanPSMT, Size: 8.039999961853027
Text:  compounds), and traditional virtual screening libraries usually contain less than , Font: TimesNewRomanPSMT, Size: 12.0
Text: 10, Font: TimesNewRomanPSMT, Size: 12.0
Text: 7, Font: TimesNewRomanPSMT, Size: 8.039999961853027
Text:  compounds and are sometimes not available. In such circumstances, the utilization of deep , Font: TimesNewRomanPSMT, Size: 12.0
Text: learning methods to generate molecules exhibiting drug-like properties emerges as a viable , Font: TimesNewRomanPSMT, Size: 12.0
Text: approach [110, 111]. Inspired by the generative pre-training model GPT, MolGPT [112] model , Font: TimesNewRomanPSMT, Size: 12.0

Text: was introduced. In addition to performing the next token prediction task, MolGPT incorporates an , Font: TimesNewRomanPSMT, Size: 12.0
Text: extra training task for conditional prediction, facilitating the capability of conditional generation. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Beyond its capacity to generate innovative and efficacious molecules, the model has demonstrated , Font: TimesNewRomanPSMT, Size: 12.0
Text: an enhanced ability to capture the statistical characteristics within the dataset. , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.4.3, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Arial-BoldMT, Size: 12.0
Text: Large language models for drug-target interaction predictions , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The investigation of Drug-Target Interaction (DTI) holds paramount significance in the realm of , Font: TimesNewRomanPSMT, Size: 12.0
Text: drug development and the optimization of drug therapy. By attaining a profound comprehension , Font: TimesNewRomanPSMT, Size: 12.0
Text: of the interaction between drugs and their target proteins, it offers valuable guidance for the design , Font: TimesNewRomanPSMT, Size: 12.0
Text: and development of pharmaceutical agents. This expedites the drug development process and , Font: TimesNewRomanPSMT, Size: 12.0
Text: mitigates the expenditure of time and resources entailed in laboratory experimentation and trial-, Font: TimesNewRomanPSMT, Size: 12.0
Text: and-error methodologies [113, 114]. , Font: TimesNewRomanPSMT, Size: 12.0
Text: During the exploration of DTI, diligent focus is placed on the prediction of drug-target binding , Font: TimesNewRomanPSMT, Size: 12.0
Text: affinity. A proficiently trained DTI language model possesses the capability to conduct high-, Font: TimesNewRomanPSMT, Size: 12.0
Text: throughput drug screening, thereby expediting the drug discovery process. DTI-BERT employs a , Font: TimesNewRomanPSMT, Size: 12.0
Text: fine-tuned ProtBERT [115] model to process protein sequences, while employing discrete wavelet , Font: TimesNewRomanPSMT, Size: 12.0
Text: transform for the processing of molecular fingerprints of drug molecules. Subsequently, by , Font: TimesNewRomanPSMT, Size: 12.0
Text: acquiring the hidden states of the corresponding pairs, the ultimate outcome is achieved through , Font: TimesNewRomanPSMT, Size: 12.0
Text: concatenation and subsequent neural network processing. This approach is simple and effective. , Font: TimesNewRomanPSMT, Size: 12.0
Text: TransDTI [116] is a multi-class classification and regression workflow. In contrast to DTI-BERT, , Font: TimesNewRomanPSMT, Size: 12.0
Text: this model not only uses fine-tuned SMILES-BERT to extract drug features, but also expands the , Font: TimesNewRomanPSMT, Size: 12.0
Text: selection of fine-tuned large protein models. After acquiring potential representations of drug-, Font: TimesNewRomanPSMT, Size: 12.0
Text: target pairs, the authors subject the representations to downstream neural networks for the , Font: TimesNewRomanPSMT, Size: 12.0
Text: completion of a multi-classification task. Additionally, the paper employs molecular docking and , Font: TimesNewRomanPSMT, Size: 12.0
Text: dynamic analysis as means of verifying the model‚Äôs predictions. Hyeunseok Kang et al. did similar , Font: TimesNewRomanPSMT, Size: 12.0
Text: work using different pre-trained models in 2022 [117]. The Chemical-Chemical Protein-Protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: Transferred DTA (C2P2) method uses pre-trained protein and molecular large language models to , Font: TimesNewRomanPSMT, Size: 12.0
Text: capture the interaction information within molecules, similar to previous methods. Given the , Font: TimesNewRomanPSMT, Size: 12.0
Text: relatively limited scale of the DTI dataset, C2P2 leverages protein-protein interaction (PPI) and , Font: TimesNewRomanPSMT, Size: 12.0
Text: chemical-chemical interaction (CCI) tasks to acquire knowledge of intermolecular interactions and , Font: TimesNewRomanPSMT, Size: 12.0
Text: subsequently transfer this knowledge to affinity prediction tasks. The incorporation of this training , Font: TimesNewRomanPSMT, Size: 12.0

Text: framework undeniably enhances the network‚Äôs ability to predict the binding affinity between the , Font: TimesNewRomanPSMT, Size: 12.0
Text: two molecules, as evidenced by the experimental outcomes [118]. , Font: TimesNewRomanPSMT, Size: 12.0
Text: It is worth highlighting that in scenarios involving the docking or when emphasizing the spatial , Font: TimesNewRomanPSMT, Size: 12.0
Text: structure of a complex, methodologies incorporating 3D convolution networks, point clouds-based , Font: TimesNewRomanPSMT, Size: 12.0
Text: networks, and graph networks are often employed [119-122]. Although these methods can better , Font: TimesNewRomanPSMT, Size: 12.0
Text: capture the inter-molecular interactions, they inevitably consume more computing resources. In , Font: TimesNewRomanPSMT, Size: 12.0
Text: situations where the molecular structure is unknown, but the sequence is available, the prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: of DTI using large-scale models still holds significant promise., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: 3.4.4, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Arial-BoldMT, Size: 12.0
Text: Large language models for drug synergistic effects predictions , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Combination therapy is common for complex diseases like cancer, infections, and neurological , Font: TimesNewRomanPSMT, Size: 12.0
Text: disorders, often surpassing single-drug treatments. Predicting drug pair synergy, where combining , Font: TimesNewRomanPSMT, Size: 12.0
Text: drugs boosts therapeutic effects, is vital in drug development. However, it‚Äôs challenging due to , Font: TimesNewRomanPSMT, Size: 12.0
Text: many drug combinations and complex biology [123, 124]. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Various computational methods, including machine learning, help predict drug pair synergy. Wei , Font: TimesNewRomanPSMT, Size: 12.0
Text: Zhang et al. [125] introduced DCE-DForest, a model for predicting drug combination synergies. , Font: TimesNewRomanPSMT, Size: 12.0
Text: It uses a pretrained drug BERT model to encode the drug SMILES and then predicts synergistic , Font: TimesNewRomanPSMT, Size: 12.0
Text: effects based on the embedding vectors of drugs and cell lines using the deep forest method. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Mengdie Xua et al. [126] utilized a fine-tuned pre-trained large language model and a dual feature , Font: TimesNewRomanPSMT, Size: 12.0
Text: fusion mechanism to predict synergistic drug combinations. Its input includes hashed atom pair , Font: TimesNewRomanPSMT, Size: 12.0
Text: molecular fingerprints of drugs, SMILES string encodings, and cell line gene expressions. They , Font: TimesNewRomanPSMT, Size: 12.0
Text: conducted ablation analyses on the dual feature fusion network for drug-drug synergy prediction, , Font: TimesNewRomanPSMT, Size: 12.0
Text: highlighting the significant role of fingerprint inputs in ensuring high-quality drug synergy , Font: TimesNewRomanPSMT, Size: 12.0
Text: predictions. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.5 Applications of large language models in single cell analysis , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The emergence of single-cell RNA sequencing (scRNA-seq) has marked the onset of a , Font: TimesNewRomanPSMT, Size: 12.0
Text: revolutionary era in genomics and biomedical research. Unlike traditional bulk RNA sequencing , Font: TimesNewRomanPSMT, Size: 12.0
Text: methods, scRNA-seq allows us to delve into the intricacies of gene expression at single-cell , Font: TimesNewRomanPSMT, Size: 12.0
Text: resolution, offering unprecedented insights and paving the way for numerous groundbreaking , Font: TimesNewRomanPSMT, Size: 12.0
Text: advancements , Font: TimesNewRomanPSMT, Size: 12.0
Text: [127-130], Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: . One of the most significant changes brought about by scRNA-seq is its , Font: TimesNewRomanPSMT, Size: 12.0

Text: ability to uncover cellular heterogeneity within tissues and organisms. It facilitates the , Font: TimesNewRomanPSMT, Size: 12.0
Text: identification and recognition of diverse cell types, subpopulations, and rare cell states that were , Font: TimesNewRomanPSMT, Size: 12.0
Text: previously concealed in bulk measurements. As we discussed in the previous sections, large , Font: TimesNewRomanPSMT, Size: 12.0
Text: language models have found successful applications in the domains of genomics, transcriptomics, , Font: TimesNewRomanPSMT, Size: 12.0
Text: proteomics and drug discovery. In this section, our attention turns to single-cell language models , Font: TimesNewRomanPSMT, Size: 12.0
Text: that can be employed for various downstream tasks in single-cell analysis. These tasks include , Font: TimesNewRomanPSMT, Size: 12.0
Text: identifying cell types and states, discovering novel cell populations, inferring gene regulation , Font: TimesNewRomanPSMT, Size: 12.0
Text: networks, and integrating single-cell multi-omics (scMulti-omics) data, among others. (, Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 7, , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Table 4, Supplementary Table 4, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: ). , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.5.1 Single-cell language models for cell clustering based on scRNA-seq data , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Cell clustering for single-cell RNA sequencing (scRNA-seq) is crucial steps in deciphering the , Font: TimesNewRomanPSMT, Size: 12.0
Text: complex landscape of cellular heterogeneity within biological samples [131-136]. Single-cell , Font: TimesNewRomanPSMT, Size: 12.0
Text: clustering aims to group individual cells into clusters according to their gene expression profiles. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Traditional single cell clustering methods mainly use clustering algorithms such as hierarchical , Font: TimesNewRomanPSMT, Size: 12.0
Text: clustering, k-means algorithm, neural network-based methods, etc. to divide cells into clusters. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Large language models enable cell clustering on large amount of scRNA-seq data from different , Font: TimesNewRomanPSMT, Size: 12.0
Text: tissues, species, organs, sequencing technologies, platforms, etc. For example, tGPT [137] is a , Font: TimesNewRomanPSMT, Size: 12.0
Text: generative pretraining model from transcriptomes, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: that requires input genes ranked in descending , Font: TimesNewRomanPSMT, Size: 12.0
Text: order of their expression. tGPT can be trained to learn the feature representation of scRNA-seq , Font: TimesNewRomanPSMT, Size: 12.0
Text: based on high-expressed genes. The learned feature representation is applied to cell clustering on , Font: TimesNewRomanPSMT, Size: 12.0
Text: atlas-scale data including Human Cell Atlas (HCA) [138], Human Cell Landscape (HCL) [139], , Font: TimesNewRomanPSMT, Size: 12.0
Text: Tabula Muris [136] and Macaque Retina [140] datasets using Leiden algorithm [141]. , Font: TimesNewRomanPSMT, Size: 12.0
Text: scFoundation [142] is the currently largest large language model in single-cell field, featuring 100 , Font: TimesNewRomanPSMT, Size: 12.0
Text: million parameters across 50 million gene expression profiles. scFoundation introduces a novel , Font: TimesNewRomanPSMT, Size: 12.0
Text: pre-training task known as read-depth-aware (RDA) modeling based on Bayesian down sampling, , Font: TimesNewRomanPSMT, Size: 12.0
Text: which selects genes of a low read-depth variant in the same cell instead of directly using neighbor , Font: TimesNewRomanPSMT, Size: 12.0
Text: genes to predict masked genes. scFoundation has a transformer-based encoder-decoder structure. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Only non-zero and non-masked gene were fed into the encoder to learn cell embedding for cell , Font: TimesNewRomanPSMT, Size: 12.0
Text: clustering.  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.5.2 Single-cell language models for cell type annotation based on scRNA-seq data, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0

Text: Single-cell annotation focuses on assigning biological labels, typically cell type or cell state, to , Font: TimesNewRomanPSMT, Size: 12.0
Text: individual cells or clusters. It plays a pivotal role in understanding cell function, identifying , Font: TimesNewRomanPSMT, Size: 12.0
Text: disease-specific cell populations, and unraveling the intricacies of tissue development and , Font: TimesNewRomanPSMT, Size: 12.0
Text: homeostasis. However, annotating cell types in single-cell RNA sequencing data is very , Font: TimesNewRomanPSMT, Size: 12.0
Text: challenging due to the high levels of noise, dropout events, and batch effects inherent in scRNA-, Font: TimesNewRomanPSMT, Size: 12.0
Text: seq data. The remarkable success of large language models in natural language processing and , Font: TimesNewRomanPSMT, Size: 12.0
Text: computer vision opens new avenues for addressing cell type annotation in single-cell RNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequencing data. Currently, there have been emerging some computational tools utilizing large , Font: TimesNewRomanPSMT, Size: 12.0
Text: language models for cell type annotation using scRNA-seq data, such as CIForm [143], TOSICA , Font: TimesNewRomanPSMT, Size: 12.0
Text: [144], scTransSort [145], TransCluster  [146], scBERT [147] and scGPT [148]. CIForm [143] is , Font: TimesNewRomanPSMT, Size: 12.0
Text: designed for cell type annotation on large-scale datasets with multiple reference data based on , Font: TimesNewRomanPSMT, Size: 12.0
Text: transformer. After a transformer encoder to learn the cell embeddings, a multi-layer neural , Font: TimesNewRomanPSMT, Size: 12.0
Text: network-based classifier is employed in CIForm to predict the cell types of single cells. CIForm , Font: TimesNewRomanPSMT, Size: 12.0
Text: was evaluated on both intra-datasets and inter-datasets, considering the annotation on different , Font: TimesNewRomanPSMT, Size: 12.0
Text: species, organs, tissues and technologies, reference and query data from different sequencing , Font: TimesNewRomanPSMT, Size: 12.0
Text: platforms or studies (which we refer to batch size effect in single cell analysis), and even multi , Font: TimesNewRomanPSMT, Size: 12.0
Text: reference data from different sources. TOSICA [144] developed an interpretable cell type , Font: TimesNewRomanPSMT, Size: 12.0
Text: annotation method that converts gene tokens into pathway/regulons tokens by adding a , Font: TimesNewRomanPSMT, Size: 12.0
Text: knowledge-based mask matrix from GSEA to the fully connected weight matrix in the gene , Font: TimesNewRomanPSMT, Size: 12.0
Text: embedding step. Only highly variable genes are used as input in TOSICA and the class token (CLS) , Font: TimesNewRomanPSMT, Size: 12.0
Text: in the output of the transformer is utilized to predict the cell type probabilities using the whole , Font: TimesNewRomanPSMT, Size: 12.0
Text: conjunction neural network cell type classifier. In addition to interpretable cell type annotation, , Font: TimesNewRomanPSMT, Size: 12.0
Text: TOSICA can also discover new cell types, perform interpretable trajectory analysis and be immune , Font: TimesNewRomanPSMT, Size: 12.0
Text: to batch effects in scRNA-seq data. scTransSort [145] and TransCluster [146] were designed by , Font: TimesNewRomanPSMT, Size: 12.0
Text: the same author team. scTransSort [145] proposed a gene patch embedding that uses CNN to , Font: TimesNewRomanPSMT, Size: 12.0
Text: generate a sequence of flattened 2D gene embedding patches to alleviate the problem of scRNA-, Font: TimesNewRomanPSMT, Size: 12.0
Text: seq data sparsity and avoid using HVGs. Positional embedding representing the relative positions , Font: TimesNewRomanPSMT, Size: 12.0
Text: between genes is added to each patch, and then passed through a transformer consisting of a multi-, Font: TimesNewRomanPSMT, Size: 12.0
Text: head self-attention mechanism and a fully connected feedforward to obtain the learned cell , Font: TimesNewRomanPSMT, Size: 12.0
Text: embedding, and then a linear classifier is used for supervised cell type classification training. The , Font: TimesNewRomanPSMT, Size: 12.0
Text: transformer structure in TransCluster [146] included both self-attention-based encoder and , Font: TimesNewRomanPSMT, Size: 12.0

Text: decoder, combined with a one-dimensional CNN to extract features from input. The linear , Font: TimesNewRomanPSMT, Size: 12.0
Text: classifier is the final step to conduct cell type annotation. scBERT [147] acquires a broad , Font: TimesNewRomanPSMT, Size: 12.0
Text: understanding of the syntax of gene-gene interactions during the pre-training phase, aiming to , Font: TimesNewRomanPSMT, Size: 12.0
Text: eliminate batch effects across datasets and enhance generalizability. In the fine-tuning step, , Font: TimesNewRomanPSMT, Size: 12.0
Text: model‚Äôs parameters guided by reference datasets are retained since a classifier is added to the pre-, Font: TimesNewRomanPSMT, Size: 12.0
Text: trained performer. The excellent design of scBERT lies in giving up the utilization of HVGs and , Font: TimesNewRomanPSMT, Size: 12.0
Text: dimensionality reduction. Instead, scBERT replaces the transformer encoder employed in BERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: with Performer [149] to enhance the model's scalability. Consequently, scBERT enables the , Font: TimesNewRomanPSMT, Size: 12.0
Text: unbiased, data-driven discovery of gene expression patterns and longer-range dependencies for , Font: TimesNewRomanPSMT, Size: 12.0
Text: cell type annotation. scGPT [148] was developed based on a generative pre-trained foundation , Font: TimesNewRomanPSMT, Size: 12.0
Text: model to learn cell and gene representations from a variety of single-cell data after HVG selection. , Font: TimesNewRomanPSMT, Size: 12.0
Text: In the pre-training step, scGPT employs stacked transformer layers with multi-head attention, , Font: TimesNewRomanPSMT, Size: 12.0
Text: enabling the simultaneous learning of cell and gene embeddings. scGPT was trained in an , Font: TimesNewRomanPSMT, Size: 12.0
Text: autoregressive manner via zero-shot learning, initially generating gene expression values from cell , Font: TimesNewRomanPSMT, Size: 12.0
Text: embeddings, and then gradually learning to generate gene expression of cells by leveraging , Font: TimesNewRomanPSMT, Size: 12.0
Text: existing knowledge of gene expressions. During the fine-tuning step. scGPT used a supervised , Font: TimesNewRomanPSMT, Size: 12.0
Text: model to annotate unknown cells from their cell representation by introducing a multilayer , Font: TimesNewRomanPSMT, Size: 12.0
Text: perceptron-based (MLP-based) classifier to the generative pre-trained foundation model. Notably, , Font: TimesNewRomanPSMT, Size: 12.0
Text: scGPT employed special tokens, batch tokens and modality tokens, to eliminate batch effects and , Font: TimesNewRomanPSMT, Size: 12.0
Text: modality differences in the pre-training single-cell reference datasets to improve the annotation , Font: TimesNewRomanPSMT, Size: 12.0
Text: accuracy.  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.5.3 Single-cell language models for gene function analysis based on scRNA-seq data , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: In addition to cell-level tasks, such as cell clustering and cell type annotation, the attention , Font: TimesNewRomanPSMT, Size: 12.0
Text: mechanism in the transformer can learn the relationship between genes, and the transformer can , Font: TimesNewRomanPSMT, Size: 12.0
Text: output the learned gene embedding after pre-training and finetuning, which can be used for gene , Font: TimesNewRomanPSMT, Size: 12.0
Text: function analysis for scRNA-seq data. As mentioned in section 3.5.2, scGPT [148] is a , Font: TimesNewRomanPSMT, Size: 12.0
Text: generalizable feature extractor based on zero-shot learning that enables scGPT to be applied to , Font: TimesNewRomanPSMT, Size: 12.0
Text: gene expression prediction and genetic perturbation prediction. The attention matrix learned by , Font: TimesNewRomanPSMT, Size: 12.0
Text: scGPT is used to infer gene regulation network. Similar to scGPT, scFoundation [142] is a , Font: TimesNewRomanPSMT, Size: 12.0
Text: foundation model that can learn both cell representation and gene representation. Zero-expressed , Font: TimesNewRomanPSMT, Size: 12.0
Text: genes and masked genes are combined with the the output from the transformer-based encoder. , Font: TimesNewRomanPSMT, Size: 12.0

Text: This combined information is then input into the decoder and projected to gene expression values , Font: TimesNewRomanPSMT, Size: 12.0
Text: through a multilayer perceptron (MLP). The gene context expression is employed to formulate a , Font: TimesNewRomanPSMT, Size: 12.0
Text: cell-specific gene graph, facilitating the prediction of perturbations using the GEARS [150] model. , Font: TimesNewRomanPSMT, Size: 12.0
Text: It is worth mentioning that another large language model Geneformer [151] is pre-trained on a vast , Font: TimesNewRomanPSMT, Size: 12.0
Text: scale of single-cell transcriptomes. All genes of each cell are reordered according to their gene , Font: TimesNewRomanPSMT, Size: 12.0
Text: expression and input into the transformer for training. Subsequently, it undergoes fine-tuning for , Font: TimesNewRomanPSMT, Size: 12.0
Text: diverse downstream tasks, encompassing the prediction of dosage-sensitive disease genes and , Font: TimesNewRomanPSMT, Size: 12.0
Text: downstream targets, forecasting chromatin dynamics, and anticipating network dynamics. This , Font: TimesNewRomanPSMT, Size: 12.0
Text: fine-tuning process leverages the pre-trained weights transferred to the task-specific models with , Font: TimesNewRomanPSMT, Size: 12.0
Text: limited data.  , Font: TimesNewRomanPSMT, Size: 12.0
Text: 3.5.4 Single-cell language models for single-cell multi-omics data , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Studying single-cell multi-omics data involves integrating information from different omics , Font: TimesNewRomanPSMT, Size: 12.0
Text: technologies (e.g., genomics, transcriptomics, epigenomics and proteomics) at the single-cell level, , Font: TimesNewRomanPSMT, Size: 12.0
Text: which has multiple advantages compared to studying single-omics data types. The adaptability, , Font: TimesNewRomanPSMT, Size: 12.0
Text: generalization capabilities, and feature extraction abilities of large language models make them , Font: TimesNewRomanPSMT, Size: 12.0
Text: valuable tools to find solutions for feature-variance, sparsity and cell heterogeneity that scMuti-, Font: TimesNewRomanPSMT, Size: 12.0
Text: omics data suffers from. A crucial phase in the analysis of single-cell multi-omics data involves , Font: TimesNewRomanPSMT, Size: 12.0
Text: the integration of such diverse datasets. scGPT [148] utilizes supplementary sets of tokens to , Font: TimesNewRomanPSMT, Size: 12.0
Text: signify distinct sequencing modalities in the context of scMulti-omics integration tasks. The , Font: TimesNewRomanPSMT, Size: 12.0
Text: modality tokens are associated with input features, such as genes, regions, and proteins. They are , Font: TimesNewRomanPSMT, Size: 12.0
Text: added to the transformer output, either at the feature or cell level, before proceeding with specific , Font: TimesNewRomanPSMT, Size: 12.0
Text: fine-tuning objectives. This deliberate inclusion helps prevent the transformer from amplifying , Font: TimesNewRomanPSMT, Size: 12.0
Text: attention within features of the same modalities, while simultaneously downplaying the , Font: TimesNewRomanPSMT, Size: 12.0
Text: significance of those associated with different modalities. scMVP [152] is designed specific for , Font: TimesNewRomanPSMT, Size: 12.0
Text: integration of paired single-cell RNA-seq and ATAC-seq data, where gene expression and , Font: TimesNewRomanPSMT, Size: 12.0
Text: chromatin accessibility are in the same cell [153-156]. scMVP uses mask attention-based scRNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: encoders and transformer multi-head self-attention-based scATAC encoders to project scRNA-seq , Font: TimesNewRomanPSMT, Size: 12.0
Text: and scATAC-seq into latent space. The distribution of latent embedding representing the joint , Font: TimesNewRomanPSMT, Size: 12.0
Text: profiling of scRNA and scATAC is the GMM distribution. A cell type-guided attention module , Font: TimesNewRomanPSMT, Size: 12.0
Text: calculates correlations between scRNA and scATAC in the same cell. Subsequently, scRNA-seq , Font: TimesNewRomanPSMT, Size: 12.0
Text: and scATAC-seq data are reconstructed and imputed by learning parameters of the negative , Font: TimesNewRomanPSMT, Size: 12.0

Text: binomial (NB) and zero-inflated Poisson (ZIP) distributions using a two-channel decoder network , Font: TimesNewRomanPSMT, Size: 12.0
Text: with a structure similar to the encoder network. DeepMAPS [157] is a graph transformer-based , Font: TimesNewRomanPSMT, Size: 12.0
Text: method, but it is designed for data integration and inference of biological networks from scMulti-, Font: TimesNewRomanPSMT, Size: 12.0
Text: omics data, encompassing scRNA-seq, scATAC-seq, and CITE-seq data. The graph constructed , Font: TimesNewRomanPSMT, Size: 12.0
Text: by DeepMAPS consists of nodes for genes and cells, so all other modalities should map their , Font: TimesNewRomanPSMT, Size: 12.0
Text: features to genes. The transformer in DeepMAPS aims to learn both local and global features to , Font: TimesNewRomanPSMT, Size: 12.0
Text: build cell-cell and gene-gene relations combined with RNA velocity. The cell-cell relation can , Font: TimesNewRomanPSMT, Size: 12.0
Text: further be used to infer cell-cell communications. , Font: TimesNewRomanPSMT, Size: 12.0
Text: In recent years, advancements in technology have enabled the concurrent characterization of , Font: TimesNewRomanPSMT, Size: 12.0
Text: different modalities in the same cell. This progress has given rise to computational tools capable , Font: TimesNewRomanPSMT, Size: 12.0
Text: of predicting one modality from another. One such tool is scTranslator [158], which translates , Font: TimesNewRomanPSMT, Size: 12.0
Text: single-cell transcriptome to proteome. scTranslator is pre-trained on both paired bulk data and , Font: TimesNewRomanPSMT, Size: 12.0
Text: paired single-cell data, then it is fine-tuned to infer protein abundance from scRNA-seq data by , Font: TimesNewRomanPSMT, Size: 12.0
Text: minimizing the mean squared error (MSE) loss between predicted and actual proteins. The learned , Font: TimesNewRomanPSMT, Size: 12.0
Text: attention matrix is applied to infer integrative gene-gene, protein-protein, and gene-protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: regulatory. Another method called scMoFormer [159] can not only translate gene expression to , Font: TimesNewRomanPSMT, Size: 12.0
Text: protein abundance, but is also applicable to multi-omics predictions, including protein abundance , Font: TimesNewRomanPSMT, Size: 12.0
Text: to gene expression, chromatin accessibility to gene expression, gene expression to chromatin , Font: TimesNewRomanPSMT, Size: 12.0
Text: accessibility using graph transformers. Taking protein prediction task as an example, scMoFormer , Font: TimesNewRomanPSMT, Size: 12.0
Text: constructs cell-gene graph, gene-gene graph, protein-protein graph, and gene-protein graph based , Font: TimesNewRomanPSMT, Size: 12.0
Text: on gene expression profiles and prior knowledge from STRING database [160]. Each modality has , Font: TimesNewRomanPSMT, Size: 12.0
Text: a separate transformer to learn the global information that may not be included in prior knowledge. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Message-passing graph neural networks (GNNs) link nodes across various graphs, while , Font: TimesNewRomanPSMT, Size: 12.0
Text: transformers are employed to precisely map gene expression to protein abundance. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: 4. Conclusion , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Pre-trained large language models have been used in multiple biological tasks. In this review, we , Font: TimesNewRomanPSMT, Size: 12.0
Text: discuss the applications of LLMs in genomics, transcriptomics, proteomics, single-cell analysis, , Font: TimesNewRomanPSMT, Size: 12.0
Text: and drug discovery. As discussed above, LLMs can learn the DNA and RNA sequencing pattern , Font: TimesNewRomanPSMT, Size: 12.0
Text: to predict DNA and RNA-based modification and regulation. LLMs can be pre-trained using , Font: TimesNewRomanPSMT, Size: 12.0
Text: proteins to achieve protein structure prediction, protein generation, protein function annotation, , Font: TimesNewRomanPSMT, Size: 12.0

Text: and protein interaction prediction. LLMs can learn cell and gene embeddings from scRNA-seq , Font: TimesNewRomanPSMT, Size: 12.0
Text: and scMulti-omics data to annotate cell types, integrate datasets and predict gene-related functional , Font: TimesNewRomanPSMT, Size: 12.0
Text: analysis. LLMs can be trained to predict molecular properties or generate molecules based on , Font: TimesNewRomanPSMT, Size: 12.0
Text: molecular scaffolds and specified molecular properties, predict interactions between drugs and , Font: TimesNewRomanPSMT, Size: 12.0
Text: targets as well as drug synergies. , Font: TimesNewRomanPSMT, Size: 12.0
Text: In genomics and transcriptomics, large language models have been used in plenty of biological , Font: TimesNewRomanPSMT, Size: 12.0
Text: tasks. Currently, DNA and RNA sequences are recognized as similar languages, existing works , Font: TimesNewRomanPSMT, Size: 12.0
Text: have largely hinged on k-mer, fixed-length permutations of A, G, C, and T/U, as the token of the , Font: TimesNewRomanPSMT, Size: 12.0
Text: genome language due to its simplicity. LLMs were asked to learn the complex statistical properties , Font: TimesNewRomanPSMT, Size: 12.0
Text: of existing biological systems, the pre-trained foundational LLM models have made significant , Font: TimesNewRomanPSMT, Size: 12.0
Text: strides in this area for effectively addressing diverse categories of downstream tasks. DNABERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: is a DNA pre-trained on DNA languages, however, some RNA languages use DNABERT instead , Font: TimesNewRomanPSMT, Size: 12.0
Text: of RNA foundation models to train the RNA sequences for specific biological tasks. For example, , Font: TimesNewRomanPSMT, Size: 12.0
Text: M6A-BERT-Stacking serves as a tissue-specific predictor designed to identify RNA N6-, Font: TimesNewRomanPSMT, Size: 12.0
Text: methyladenosine sites, utilizing DNABERT and a Stacking Strategy. In its approach, M6A-BERT-, Font: TimesNewRomanPSMT, Size: 12.0
Text: Stacking utilizes pre-trained DNABERT and fine-tuned DNABERT attention models. Notably, it , Font: TimesNewRomanPSMT, Size: 12.0
Text: has been observed that DNABERT effectively directs attention to critical regions within known , Font: TimesNewRomanPSMT, Size: 12.0
Text: m6A sites and captures informative feature representations from input sequences. What‚Äôs more, , Font: TimesNewRomanPSMT, Size: 12.0
Text: some biological tasks can be predicted from DNA or RNA sequences. Like DNABERT and , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNABERT-2 can predict the splice site from the reference genome, however, SpliceBERT was , Font: TimesNewRomanPSMT, Size: 12.0
Text: trained in the RNA sequence of RNA-seq and predicted the splice site of specific pre-mRNA. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Since DNA and RNA sequences both consist of four letters, their foundational sequences language , Font: TimesNewRomanPSMT, Size: 12.0
Text: model might be able to be used in both DNA and RNA-related tasks. In summary, today‚Äôs LLMs , Font: TimesNewRomanPSMT, Size: 12.0
Text: are sufficiently advanced to model molecular biology. However, we expect it to learn one-step , Font: TimesNewRomanPSMT, Size: 12.0
Text: causality relationships which can be learned from the correlations across modalities such as DNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: variation, and mRNA abundance. , Font: TimesNewRomanPSMT, Size: 12.0
Text: In proteomics, the protein language models take sequence information as input and produce fine-, Font: TimesNewRomanPSMT, Size: 12.0
Text: tuned output results tailored to different downstream tasks associated with proteins. In terms of , Font: TimesNewRomanPSMT, Size: 12.0
Text: the model‚Äôs parameter magnitude, the large parameter quantity of the language model establishes , Font: TimesNewRomanPSMT, Size: 12.0
Text: a solid foundation for effectively addressing diverse categories of downstream tasks. Nonetheless, , Font: TimesNewRomanPSMT, Size: 12.0
Text: the excessive parameters often pose deployment challenges for ordinary researchers. One approach , Font: TimesNewRomanPSMT, Size: 12.0

Text: involves utilizing large models online, akin to our daily use of GPT, albeit at the expense of , Font: TimesNewRomanPSMT, Size: 12.0
Text: additional deployment costs. Another method entails employing knowledge distillation or other , Font: TimesNewRomanPSMT, Size: 12.0
Text: algorithms to capture the refined knowledge that researchers deem valuable from the large model. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Considering the model‚Äôs input perspective, the most basic protein language model is limited to , Font: TimesNewRomanPSMT, Size: 12.0
Text: handling inputs such as MSA and protein sequences. However, incorporating other modalities of , Font: TimesNewRomanPSMT, Size: 12.0
Text: information, such as 3D structural information of proteins, introduces challenges in training and , Font: TimesNewRomanPSMT, Size: 12.0
Text: utilizing large language models. One approach involves transforming the additional modalities of , Font: TimesNewRomanPSMT, Size: 12.0
Text: information into a sequence-based format, necessitating careful consideration of the effectiveness , Font: TimesNewRomanPSMT, Size: 12.0
Text: and rationality of such information conversion across modalities. Another method involves , Font: TimesNewRomanPSMT, Size: 12.0
Text: integrating other large models to collectively capture the multi-modal information pertaining to , Font: TimesNewRomanPSMT, Size: 12.0
Text: proteins. This approach requires consideration of multi-modal fusion techniques, fusion timing, , Font: TimesNewRomanPSMT, Size: 12.0
Text: and related concerns. The increase of diverse types of protein-related data has driven the protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: language models to evolve from an independent model to an integral component of larger protein , Font: TimesNewRomanPSMT, Size: 12.0
Text: models.  , Font: TimesNewRomanPSMT, Size: 12.0
Text: In computer-aided drug discovery, the key steps include processes such as docking, scoring, and , Font: TimesNewRomanPSMT, Size: 12.0
Text: screening. When employing large language models, it becomes feasible to solely consider the , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequence information of molecules, leading to heightened prediction efficiency. However, the , Font: TimesNewRomanPSMT, Size: 12.0
Text: absence of spatial structural information significantly impacts prediction accuracy. Therefore, the , Font: TimesNewRomanPSMT, Size: 12.0
Text: use of deep learning for drug discovery is more based on structure, which means that the sequence , Font: TimesNewRomanPSMT, Size: 12.0
Text: features extracted by large language models are usually input as prior knowledge into the network. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Drawing inspiration from the construction of other large language models in the field of , Font: TimesNewRomanPSMT, Size: 12.0
Text: bioinformatics, it is also plausible to construct large models based on molecular structure, such as , Font: TimesNewRomanPSMT, Size: 12.0
Text: large graph models, to predict binding affinity, based on the CrossDocked2020 dataset. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Additionally, the experience gained from the prediction of PPI and CCI can be transferred to the , Font: TimesNewRomanPSMT, Size: 12.0
Text: prediction of DTI using techniques like transfer learning. Computer-aided drug discovery also , Font: TimesNewRomanPSMT, Size: 12.0
Text: involves molecular generation. The generation of drug molecules needs to consider properties such , Font: TimesNewRomanPSMT, Size: 12.0
Text: as effectiveness, novelty, and drug-likeness. While existing methods do incorporate these , Font: TimesNewRomanPSMT, Size: 12.0
Text: properties to some extent, there remains a dearth of extensive research on the generated molecules, , Font: TimesNewRomanPSMT, Size: 12.0
Text: as well as a scarcity of practical chemical or biological experimental verification to support their , Font: TimesNewRomanPSMT, Size: 12.0
Text: viability. , Font: TimesNewRomanPSMT, Size: 12.0

Text: In single-cell analysis, large language models are pre-trained on a large scale of gene expression , Font: TimesNewRomanPSMT, Size: 12.0
Text: to apply to cell-level and gene-level downstream tasks. There are many differences from NLP in , Font: TimesNewRomanPSMT, Size: 12.0
Text: single-cell applications of LLMs. First, scRNA-seq data suffers from high sparsity, which is also , Font: TimesNewRomanPSMT, Size: 12.0
Text: one of the difficulties to be solved in large language models applied to single-cell RNA-seq data. , Font: TimesNewRomanPSMT, Size: 12.0
Text: CIForm, TOSICA, TransCluster and scGPT select highly variable genes as input to alleviate the , Font: TimesNewRomanPSMT, Size: 12.0
Text: problem of scRNA-seq data sparsity and reduce the training burden of large language models. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Second, gene expression differs from human language and sequence data, it is continuous values , Font: TimesNewRomanPSMT, Size: 12.0
Text: and does not have orders between gene tokens. Thus, how to define the genes‚Äô position is a critical , Font: TimesNewRomanPSMT, Size: 12.0
Text: problem of LLMs in single-cell analysis. tGPT and Geneformer sort genes from high to low , Font: TimesNewRomanPSMT, Size: 12.0
Text: according to their gene expression. CIForm and TransCluster apply sine and cosine functions to , Font: TimesNewRomanPSMT, Size: 12.0
Text: determine the position information. scBERT uses gene2vec to transfer gene expression to discrete , Font: TimesNewRomanPSMT, Size: 12.0
Text: values. Third, batch effects can pose a significant challenge in cell type clustering, annotation, and , Font: TimesNewRomanPSMT, Size: 12.0
Text: integration, especially when the input comprises multiple datasets from distinct sequencing , Font: TimesNewRomanPSMT, Size: 12.0
Text: batches or technologies. The generalizability of large language models can adapt to different , Font: TimesNewRomanPSMT, Size: 12.0
Text: biological datasets and applications, making them versatile tools to be applied to various single-, Font: TimesNewRomanPSMT, Size: 12.0
Text: cell datasets. It is worth mentioning that the combination of Graph Neural Networks (GNNs) and , Font: TimesNewRomanPSMT, Size: 12.0
Text: transformers has brought about significant advancements and meaningful contributions to the field , Font: TimesNewRomanPSMT, Size: 12.0
Text: of single-cell analysis, such as scMoFormer uses graph transformers to construct cell-gene graph, , Font: TimesNewRomanPSMT, Size: 12.0
Text: gene-gene graph, protein-protein graph, and gene-protein graph for multi-omics predictions. , Font: TimesNewRomanPSMT, Size: 12.0
Text: DeepMAPS constructs a graph of cells and genes and uses graph transformer to estimate the , Font: TimesNewRomanPSMT, Size: 12.0
Text: importance of genes to cells. The combination of GNNs and Transformers allows for a more , Font: TimesNewRomanPSMT, Size: 12.0
Text: comprehensive representation of the intricate relationships and dependencies present in single-cell , Font: TimesNewRomanPSMT, Size: 12.0
Text: data. GNNs excel at capturing local interactions within cellular neighborhoods, while , Font: TimesNewRomanPSMT, Size: 12.0
Text: Transformers effectively capture long-range dependencies. This synergy enables a holistic , Font: TimesNewRomanPSMT, Size: 12.0
Text: understanding of the cellular landscape, leading to improved feature learning. In summary, large , Font: TimesNewRomanPSMT, Size: 12.0
Text: language models excel at extracting meaningful features from raw data in single-cell analysis. , Font: TimesNewRomanPSMT, Size: 12.0
Text: They can learn representations of gene expression patterns, cell types, and other relevant , Font: TimesNewRomanPSMT, Size: 12.0
Text: information from the data, even without prior domain-specific knowledge.  , Font: TimesNewRomanPSMT, Size: 12.0
Text: In conclusion, today‚Äôs large language models have reached a level of sophistication that enables , Font: TimesNewRomanPSMT, Size: 12.0
Text: them to effectively model the intricacies of molecular biology. With continuous advancements in , Font: TimesNewRomanPSMT, Size: 12.0
Text: single-cell technologies and the expanding landscape of omics sciences, including proteomics, , Font: TimesNewRomanPSMT, Size: 12.0

Text: metabolomics, lipidomics, and other-omic assays, there is a growing capacity for conducting , Font: TimesNewRomanPSMT, Size: 12.0
Text: increasingly detailed and efficient measurements. These developments contribute to our ability to , Font: TimesNewRomanPSMT, Size: 12.0
Text: unravel the complexities inherent in the diverse molecular layers spanning from DNA to the , Font: TimesNewRomanPSMT, Size: 12.0
Text: intricacies of human physiology. As we delve deeper into the realms of these cutting-edge , Font: TimesNewRomanPSMT, Size: 12.0
Text: technologies, we unlock new insights that pave the way for a more comprehensive understanding , Font: TimesNewRomanPSMT, Size: 12.0
Text: of the dynamic interplay within the molecular landscape. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: Acknowledgements , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: We would like to express our gratitude to our colleagues and friends who provided invaluable , Font: TimesNewRomanPSMT, Size: 12.0
Text: advice and support throughout the duration of this study. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: Funding , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: This work was partially supported by the National Institutes of Health [R01GM123037, , Font: TimesNewRomanPSMT, Size: 12.0
Text: U01AR069395-01A1, R01CA241930 to X.Z] and the National Science Foundation [2217515, , Font: TimesNewRomanPSMT, Size: 12.0
Text: 2326879 to X.Z]; M.Y. was supported by the China Postdoctoral Science Foundation , Font: TimesNewRomanPSMT, Size: 12.0
Text: [2022M712900, 2023T160590]. The funders had no role in study design, data collection and , Font: TimesNewRomanPSMT, Size: 12.0
Text: analysis, decision to publish or preparation of the manuscript. Funding for open access charge: Dr , Font: TimesNewRomanPSMT, Size: 12.0
Text: & Mrs Carl V. Vartian Chair Professorship Funds to Dr. Zhou from the University of Texas Health , Font: TimesNewRomanPSMT, Size: 12.0
Text: Science Center at Houston. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Conflict of interest statement., Font: TimesNewRomanPS-ItalicMT, Size: 12.0
Text:  None declared. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: References , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: 1. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Radford, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Improving language understanding by generative pre-training., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Devlin, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Bert: Pre-training of deep bidirectional transformers for language understanding., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: arXiv preprint arXiv:1810.04805, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 3. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Vaswani, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Attention is all you need., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Advances in neural information processing systems, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2017. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 30, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: . , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 4. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Sarkar, S., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Decoding" coding": Information and DNA., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  BioScience, 1996. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 46, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (11): p. 857-864. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 5. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ji, Y., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DNABERT: pre-trained Bidirectional Encoder Representations from Transformers , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: model for DNA-language in genome., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Bioinformatics, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 37, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (15): p. 2112-2120. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 6. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhou, Z., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Dnabert-2: Efficient foundation model and benchmark for multi-species genome., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: arXiv preprint arXiv:2306.15006, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 7. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Sanabria, M., J. Hirsch, and A.R. Poetsch, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The human genome's vocabulary as proposed by the , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: DNA language model GROVER., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. 2023.07. 19.549677. , Font: TimesNewRomanPSMT, Size: 11.039999961853027

Text: 8. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Dalla-Torre, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The nucleotide transformer: Building and evaluating robust foundation , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: models for human genomics., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. 2023.01. 11.523679. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 9. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Sinden, R.R. and R.D. Wells, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DNA structure, mutations, and human genetic disease., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Current , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: opinion in biotechnology, 1992. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 3, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6): p. 612-622. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 10. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Benegas, G., S.S. Batra, and Y.S. Song, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DNA language models are powerful zero-shot predictors , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: of genome-wide variant effects., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2022: p. 2022.08. 22.504706. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 11. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wittkopp, P.J. and G. Kalay, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cis-regulatory elements: molecular mechanisms and evolutionary , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: processes underlying divergence., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature Reviews Genetics, 2012. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 13, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 59-69. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 12. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Yella, V.R., A. Kumar, and M. Bansal, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Identification of putative promoters in 48 eukaryotic , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: genomes on the basis of DNA free energy., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Scientific reports, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 8, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 4520. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 13. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Le, N.Q.K., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: BERT-Promoter: An improved sequence-based predictor of DNA promoter using , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: BERT pre-trained model and SHAP feature selection., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Computational Biology and Chemistry, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 99, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 107732. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 14. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Claringbould, A. and J.B. Zaugg, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Enhancers in disease: molecular basis and emerging treatment , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: strategies., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Trends in Molecular Medicine, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 27, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (11): p. 1060-1073. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 15. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Nasser, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Genome-wide enhancer maps link risk variants to disease genes., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 593, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7858): p. 238-243. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 16. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Luo, H., et al. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: iEnhancer-BERT: A novel transfer learning architecture based on DNA-Language , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: model for identifying enhancers and their strength, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: International Conference on Intelligent , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Computing, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2022. Springer. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 17. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ferraz, R.A.C., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DNA‚Äìprotein interaction studies: a historical and comparative analysis., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Plant Methods, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 17, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 1-21. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 18. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Luo, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Improving language model of human genome for DNA‚Äìprotein binding prediction , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: based on task-specific pre-training., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Interdisciplinary Sciences: Computational Life Sciences, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 15, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 32-43. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 19. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: An, W., et al. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: MoDNA: motif-oriented pre-training for DNA language model, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Proceedings of the , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: 13th ACM International Conference on Bioinformatics, Computational Biology and Health , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Informatics, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 20. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Moore, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: L.D., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: T. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Le, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: and , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: G. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Fan, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DNA , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: methylation , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: and , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: its , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: basic , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: function., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Neuropsychopharmacology, 2013. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 38, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 23-38. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 21. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhang, L., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Comprehensive analysis of DNA 5-methylcytosine and N6-adenine methylation , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: by nanopore sequencing in hepatocellular carcinoma., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Frontiers in cell and developmental biology, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 10, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 827391. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 22. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Tsukiyama, S., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: BERT6mA: prediction of DNA N6-methyladenine site using deep learning-, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: based approaches., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Briefings in Bioinformatics, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (2): p. bbac053. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Yu, Y., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: iDNA-ABT: advanced deep learning model for detecting DNA methylation with , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: adaptive features and transductive information maximization., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Bioinformatics, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 37, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (24): p. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 4603-4610. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 24. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Jin, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: iDNA-ABF: multi-scale deep biological language learning model for the interpretable , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: prediction of DNA methylations., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Genome biology, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 1-23. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 25. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zeng, W., A. Gautam, and D.H. Huson, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: MuLan-Methyl-Multiple Transformer-based Language , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Models for Accurate DNA Methylation Prediction., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. 2023.01. 04.522704. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 26. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Sanh, V., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: preprint arXiv:1910.01108, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 27. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Lan, Z., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Albert: A lite bert for self-supervised learning of language representations., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: preprint arXiv:1909.11942, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 28. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Yang, Z., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Xlnet: Generalized autoregressive pretraining for language understanding., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Advances in neural information processing systems, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 32, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: . , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 29. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Clark, K., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Electra: Pre-training text encoders as discriminators rather than generators., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: preprint arXiv:2003.10555, 2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027

Text: 30. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wilkinson, M.E., C. Charenton, and K. Nagai, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: RNA splicing by the spliceosome., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Annual review of , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: biochemistry, 2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 89, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 359-388. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 31. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chen, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Interpretable RNA foundation model from unannotated data for highly accurate , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: RNA structure and function predictions., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2022: p. 2022.08. 06.503062. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 32. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhang, Y., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Multiple sequence-alignment-based RNA language model and its application to , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: structural inference., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. 2023.03. 15.532863. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 33. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhang, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Advances and opportunities in RNA structure experimental determination and , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: computational modeling., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature Methods, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 19, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (10): p. 1193-1207. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 34. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Kalicki, C.H. and E.D. Haritaoglu, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: RNABERT: RNA Family Classification and Secondary , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Structure Prediction with BERT pretrained on RNA sequences. , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: 35. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chen, K., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Self-supervised learning on millions of pre-mRNA sequences improves sequence-, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: based RNA splicing prediction., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. 2023.01. 31.526427. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 36. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Malbec, L., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Dynamic methylome of internal mRNA N 7-methylguanosine and its regulatory , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: role in translation., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Cell research, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 29, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (11): p. 927-941. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 37. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Feng, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: LncCat: An ORF attention model to identify LncRNA based on ensemble learning , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: strategy and fused sequence information., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Computational and Structural Biotechnology Journal, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 21, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 1433-1447. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 38. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Prokhorenkova, L., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: CatBoost: unbiased boosting with categorical features., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Advances in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: neural information processing systems, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 31, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: . , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 39. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Yamada, K. and M. Hamada, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Prediction of RNA‚Äìprotein interactions using a nucleotide language , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: model., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Bioinformatics Advances, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. vbac023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 40. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhang, L., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: BERT-m7G: a transformer architecture based on BERT and stacking ensemble to , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: identify RNA N7-Methylguanosine sites from sequence information., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Computational and , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Mathematical Methods in Medicine, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2021, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: . , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 41. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Gibb, E.A., C.J. Brown, and W.L. Lam, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The functional role of long non-coding RNA in human , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: carcinomas., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Molecular cancer, 2011. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 10, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 1-17. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 42. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Soylu, N.N. and E. Sefer, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: BERT2OME: Prediction of 2'-O-methylation Modifications from RNA , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Sequence by Transformer Architecture Based on BERT., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  IEEE/ACM Transactions on , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Computational Biology and Bioinformatics, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 43. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Pardi, N., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: mRNA vaccines‚Äîa new era in vaccinology., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature reviews Drug discovery, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 17, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4): p. 261-279. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 44. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Babjac, A.N., Z. Lu, and S.J. Emrich. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: CodonBERT: Using BERT for Sentiment Analysis to Better , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Predict Genes with Low Expression, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Proceedings of the 14th ACM International Conference on , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Bioinformatics, Computational Biology, and Health Informatics, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 45. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Gong, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Integrated mRNA sequence optimization using deep learning., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Brief Bioinform, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 24, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1). , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 46. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ding, W., K. Nakai, and H. Gong, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Protein design via deep learning., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Briefings in bioinformatics, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (3): p. bbac102. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 47. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Qiu, Y. and G.-W. Wei, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Artificial intelligence-aided protein engineering: from topological data , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: analysis to deep protein language models., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv preprint arXiv:2307.14587, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 48. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Kovaltsuk, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Observed antibody space: a resource for data mining next-generation , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: sequencing of antibody repertoires., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  The Journal of Immunology, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 201, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (8): p. 2502-2509. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 49. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Schauperl, M. and R.A. Denny, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: AI-based protein structure prediction in drug discovery: impacts , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: and challenges., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Journal of Chemical Information and Modeling, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 62, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (13): p. 3142-3156. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 50. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: David, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The AlphaFold database of protein structures: a biologist‚Äôs guide., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Journal of , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: molecular biology, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 434, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (2): p. 167336. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 51. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Rao, R.M., et al. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: MSA transformer, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: International Conference on Machine Learning, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 52. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Elnaggar, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: ProtTrans: Towards Cracking the Language of Lifes Code Through Self-, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Supervised Deep Learning and High Performance Computing., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  IEEE Transactions on Pattern , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Analysis and Machine Intelligence, 2021: p. 1-1. , Font: TimesNewRomanPSMT, Size: 11.039999961853027

Text: 53. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Dai, Z., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Transformer-xl: Attentive language models beyond a fixed-length context., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: preprint arXiv:1901.02860, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 54. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Raffel, C., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Exploring the limits of transfer learning with a unified text-to-text transformer., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The Journal of Machine Learning Research, 2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 21, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 5485-5551. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 55. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: UniProt: the universal protein knowledgebase in 2021., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nucleic acids research, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 49, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (D1): p. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: D480-D489. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 56. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Steinegger, M. and J. Sding, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Clustering huge protein sequence sets in linear time., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: communications, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 9, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 2542. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 57. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Steinegger, M., M. Mirdita, and J. Sding, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Protein-level assembly increases protein sequence , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: recovery from metagenomic samples manyfold., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature methods, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 16, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7): p. 603-606. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 58. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Strokach, A. and P.M. Kim, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Deep generative modeling for protein design., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Current opinion in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: structural biology, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 72, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 226-236. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 59. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ferruz, N. and B. Hcker, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Controllable protein design with language models., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature Machine , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Intelligence, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 4, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6): p. 521-532. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 60. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Madani, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Progen: Language modeling for protein generation., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv preprint , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: arXiv:2004.03497, 2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 61. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ferruz, N., S. Schmidt, and B. Hcker, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: ProtGPT2 is a deep unsupervised language model for protein , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: design., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature communications, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 13, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 4348. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 62. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Mirdita, M., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: ColabFold: making protein folding accessible to all., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature methods, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 19, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6): p. 679-682. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 63. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Jumper, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Highly accurate protein structure prediction with AlphaFold., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 596, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7873): p. 583-589. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 64. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhou, X., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: I-TASSER-MTD: a deep-learning-based platform for multi-domain protein , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: structure and function prediction., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature Protocols, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 17, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (10): p. 2326-2353. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 65. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ferruz, N., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: From sequence to function through structure: Deep learning for protein design., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Computational and Structural Biotechnology Journal, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 21, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 238-250. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 66. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wu, L., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: SPRoBERTa: protein embedding learning with local fragment modeling., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Briefings , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: in Bioinformatics, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6): p. bbac401. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 67. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Xu, M., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Protst: Multi-modality learning of protein sequences and biomedical texts., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: preprint arXiv:2301.12040, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 68. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wang, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Protein post-translational modifications in the regulation of cancer hallmarks., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cancer Gene Therapy, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 30, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4): p. 529-547. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 69. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: de Brevern, A.G. and J. Rebehmed, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Current status of PTMs structural databases: applications, , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: limitations and prospects., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Amino Acids, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 54, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4): p. 575-590. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 70. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Brandes, N., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: ProteinBERT: a universal deep-learning model of protein sequence and function., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Bioinformatics, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 38, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (8): p. 2102-2110. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 71. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Savino, S., T. Desmet, and J. Franceus, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Insertions and deletions in protein evolution and , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: engineering., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Biotechnology Advances, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 60, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 108010. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 72. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Horne, J. and D. Shukla, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Recent advances in machine learning variant effect prediction tools for , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: protein engineering., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Industrial \& engineering chemistry research, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 61, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (19): p. 6235-6245. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 73. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Asgari, E. and M.R.K. Mofrad, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Continuous distributed representation of biological sequences for , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: deep proteomics and genomics., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  PloS one, 2015. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 10, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (11): p. e0141287. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 74. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Suzek, B.E., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: UniRef clusters: a comprehensive and scalable alternative for improving , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: sequence similarity searches., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Bioinformatics, 2015. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 31, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6): p. 926-932. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 75. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Alley, E.C., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Unified rational protein engineering with sequence-based deep representation , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: learning., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature methods, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 16, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (12): p. 1315-1322. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 76. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Rives, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Biological structure and function emerge from scaling unsupervised learning to , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: 250 million protein sequences. bioRxiv, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 77. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Pucci, F., M. Schwersensky, and M. Rooman, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Artificial intelligence challenges for predicting the , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: impact of mutations on protein stability., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Current opinion in structural biology, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 72, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 161-, Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 168. , Font: TimesNewRomanPSMT, Size: 11.039999961853027

Text: 78. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Rao, R., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Evaluating protein transfer learning with TAPE., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Advances in neural information , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: processing systems, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 32, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: . , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 79. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wang, Z., et al. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Multi-level Protein Structure Pre-training via Prompt Learning, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The Eleventh , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: International Conference on Learning Representations, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 80. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Tang, T., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Machine learning on protein--protein interaction prediction: models, challenges , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: and trends., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Briefings in Bioinformatics, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 24, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (2): p. bbad076. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 81. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Durham, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Recent advances in predicting and modeling protein--protein interactions., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Trends in biochemical sciences, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 82. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhou, H.-Y., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Protein Representation Learning via Knowledge Enhanced Primary Structure , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Modeling., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. 2023-01. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 83. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhang, N., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ontoprotein: Protein pretraining with gene ontology embedding., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv preprint , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: arXiv:2201.11147, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 84. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Janeway, C., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Immunobiology: the immune system in health and disease, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . Vol. 2. 2001: Garland , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Pub. New York. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 85. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Peters, B., M. Nielsen, and A.J.A.R.o.I. Sette, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: T cell epitope predictions., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 38, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 123-145. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 86. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: O‚ÄôDonnell, T.J., A. Rubinsteyn, and U.J.C.s. Laserson, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: MHCflurry 2.0: improved pan-allele , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: prediction of MHC class I-presented peptides by incorporating antigen processing., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 11, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 42-48. e7. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 87. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wang, F., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: MHCRoBERTa: pan-specific peptide-MHC class I binding prediction through , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: transfer learning with label-agnostic protein sequences., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Brief Bioinform, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (3). , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 88. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cheng, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: BERTMHC: improved MHC‚Äìpeptide class II interaction prediction with , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: transformer and multiple instance learning., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Bioinformatics, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 37, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (22): p. 4172-4179. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 89. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Greiff, V., G. Yaari, and L.G.J.C.O.i.S.B. Cowell, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Mining adaptive immune receptor repertoires , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: for biological and clinical information using machine learning., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 24, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 109-119. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 90. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wu, K., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: TCR-BERT: learning the grammar of T-cell receptors for flexible antigenbinding , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: analyses., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 91. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhao, Y., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: SC-AIR-BERT: a pre-trained single-cell model for predicting the antigen-binding , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: specificity of the adaptive immune receptor., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Brief Bioinform, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 24, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4). , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 92. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Davies, D.R., E.A. Padlan, and S.J.A.r.o.b. Sheriff, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Antibody-antigen complexes., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  1990. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 59, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 439-473. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 93. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Olsen, T.H., I.H. Moal, and C.M. Deane, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: AbLang: an antibody language model for completing , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: antibody sequences., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Bioinformatics Advances, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. vbac046. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 94. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Liu, Y., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Roberta: A robustly optimized bert pretraining approach., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv preprint , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: arXiv:1907.11692, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 95. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Giudicelli, V., D. Chaume, and M.-P. Lefranc, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: IMGT/GENE-DB: a comprehensive database for , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: human and mouse immunoglobulin and T cell receptor genes., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nucleic acids research, 2005. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 33, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (suppl\_1): p. D256-D261. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 96. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Leem, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Deciphering the language of antibodies using self-supervised learning., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Patterns, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 3, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7). , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 97. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wang, D., F. Ye, and H. Zhou, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: On pre-trained language models for antibody., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2023-01. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 98. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Askr, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Deep learning in drug discovery: an integrative review and future challenges., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Artificial Intelligence Review, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 56, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7): p. 5975-6037. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 99. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Xiaobo, Z. and S.T.C. Wong, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: High content cellular imaging for drug development., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  IEEE Signal , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Processing Magazine, 2006. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (2): p. 170-174. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 100. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Sun, X., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Multi-scale agent-based brain cancer modeling and prediction of TKI treatment , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: response: incorporating EGFR signaling pathway and angiogenesis., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  BMC Bioinformatics, 2012. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 13, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 218. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 101. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Vargason, A.M., A.C. Anselmo, and S.J.N.b.e. Mitragotri, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The evolution of commercial drug , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: delivery technologies., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 5, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (9): p. 951-967. , Font: TimesNewRomanPSMT, Size: 11.039999961853027

Text: 102. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Leeson, P.D. and B.J.N.r.D.d. Springthorpe, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The influence of drug-like concepts on decision-, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: making in medicinal chemistry., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2007. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 6, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (11): p. 881-890. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 103. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: zelik, R., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Structure-based Drug discovery with Deep Learning., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  ChemBioChem, 2022: p. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: e202200776. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 104. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Li, Z., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Deep learning methods for molecular representation and property prediction., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Drug , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Discovery Today, 2022: p. 103373. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 105. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chen, W., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Artificial intelligence for drug discovery: Resources, methods, and applications., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Molecular Therapy-Nucleic Acids, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 106. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wang, S., et al. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Smiles-bert: large scale unsupervised pre-training for molecular property , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: prediction, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Proceedings of the 10th ACM international conference on bioinformatics, , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: computational biology and health informatics, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 107. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chithrananda, S., G. Grand, and B. Ramsundar, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: ChemBERTa: large-scale self-supervised , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: pretraining for molecular property prediction., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv preprint arXiv:2010.09885, 2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 108. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wu, Z., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Knowledge-based BERT: a method to extract molecular features like computational , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: chemists., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Briefings in Bioinformatics, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (3): p. bbac131. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 109. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Xia, J., et al. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Mole-bert: Rethinking pre-training graph neural networks for molecules, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Eleventh International Conference on Learning Representations, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 110. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Bilodeau, C., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Generative models for molecular discovery: Recent advances and challenges., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wiley Interdisciplinary Reviews: Computational Molecular Science, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 12, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (5): p. e1608. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 111. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Meyers, J., B. Fabian, and N. Brown, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: De novo molecular design and generative models., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Drug , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Discovery Today, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 26, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (11): p. 2707-2715. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 112. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Bagal, V., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: MolGPT: molecular generation using a transformer-decoder model., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Journal of , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chemical Information and Modeling, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 62, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (9): p. 2064-2076. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 113. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Abbasi, K., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Deep learning in drug target interaction prediction: current and future , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: perspectives., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Current Medicinal Chemistry, 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 28, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (11): p. 2100-2113. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 114. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhang, Z., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Graph neural network approaches for drug-target interactions., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Current Opinion , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: in Structural Biology, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 73, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 102327. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 115. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zheng, J., X. Xiao, and W.-R. Qiu, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DTI-BERT: identifying drug-target interactions in cellular , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: networking based on BERT and deep learning method., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Frontiers in Genetics, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 13, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 859188. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 116. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Kalakoti, Y., S. Yadav, and D. Sundar, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: TransDTI: transformer-based language models for , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: estimating DTIs and building a drug recommendation workflow., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  ACS omega, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 7, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (3): p. 2706-, Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2717. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 117. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Kang, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Fine-tuning of bert model to accurately predict drug--target interactions., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Pharmaceutics, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 14, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (8): p. 1710. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 118. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Nguyen, T.M., T. Nguyen, and T. Tran, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Mitigating cold-start problems in drug-target affinity , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: prediction with interaction knowledge transferring., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Briefings in Bioinformatics, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4): p. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: bbac269. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 119. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ragoza, M., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Protein--ligand scoring with convolutional neural networks., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Journal of chemical , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: information and modeling, 2017. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 57, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4): p. 942-957. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 120. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Li, S., et al. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Structure-aware interactive graph neural networks for the prediction of protein-ligand , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: binding affinity, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . in , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: \& Data Mining, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: . 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 121. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Jiang, D., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: InteractionGraphNet: a novel and efficient deep graph representation learning , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: framework for accurate protein--ligand interaction predictions., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Journal of medicinal chemistry, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2021. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 64, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (24): p. 18209-18232. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 122. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Wang, Y., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: A point cloud-based deep learning strategy for protein--ligand binding affinity , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: prediction., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Briefings in Bioinformatics, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. bbab474. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 123. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Hecht, J.R., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: A randomized phase IIIB trial of chemotherapy, bevacizumab, and panitumumab , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: compared with chemotherapy and bevacizumab alone for metastatic colorectal cancer., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2009. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 27, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (5): , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: p. 672-680. , Font: TimesNewRomanPSMT, Size: 11.039999961853027

Text: 124. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Tol, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chemotherapy, bevacizumab, and cetuximab in metastatic colorectal cancer., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  2009. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 360, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6): p. 563-572. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 125. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhang, W., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DCE-DForest: a deep forest model for the prediction of anticancer drug , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: combination effects., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Computational and Mathematical Methods in Medicine, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2022, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: . , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 126. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Xu, M., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: DFFNDDS: prediction of synergistic drug combinations with dual feature fusion , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: networks., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Journal of Cheminformatics, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 15, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 1-12. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 127. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Kowalczyk, M.S., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Single-cell RNA-seq reveals changes in cell cycle and differentiation , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: programs upon aging of hematopoietic stem cells., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Genome Res, 2015. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 25, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (12): p. 1860-72. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 128. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Macaulay, I.C., C.P. Ponting, and T. Voet, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Single-Cell Multiomics: Multiple Measurements from , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Single Cells., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Trends Genet, 2017. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 33, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (2): p. 155-168. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 129. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Papalexi, E. and R. Satija, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Single-cell RNA sequencing to explore immune cell heterogeneity., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nat , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Rev Immunol, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 18, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 35-45. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 130. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Tanay, A. and A. Regev, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Scaling single-cell genomics from phenomenology to mechanism., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2017. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 541, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7637): p. 331-338. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 131. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cao, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Comprehensive single-cell transcriptional profiling of a multicellular organism., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Science, 2017. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 357, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6352): p. 661-667. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 132. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cao, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The single-cell transcriptional landscape of mammalian organogenesis., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 566, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7745): p. 496-502. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 133. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Fincher, C.T., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cell type transcriptome atlas for the planarian Schmidtea mediterranea., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Science, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 360, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6391). , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 134. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Han, X., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Mapping the Mouse Cell Atlas by Microwell-Seq., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Cell, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 172, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (5): p. 1091-1107 , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: e17. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 135. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Plass, M., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cell type atlas and lineage tree of a whole complex animal by single-cell , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: transcriptomics., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Science, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 360, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6391). , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 136. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Tabula Muris, C., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Nature, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 562, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7727): p. 367-372. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 137. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Shen, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Generative pretraining from large-scale transcriptomes for single-cell deciphering., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: iScience, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 26, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (5): p. 106536. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 138. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Regev, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The human cell atlas white paper., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv preprint arXiv:1810.05192, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 139. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Han, X., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Construction of a human cell landscape at single-cell level., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature, 2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 581, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7808): , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: p. 303-309. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 140. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Peng, Y.R., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Molecular Classification and Comparative Taxonomics of Foveal and Peripheral , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Cells in Primate Retina., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Cell, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 176, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (5): p. 1222-1237 e22. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 141. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Traag, V.A., L. Waltman, and N.J. Van Eck, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: From Louvain to Leiden: guaranteeing well-connected , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: communities., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Scientific reports, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 9, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 5233. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 142. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Minsheng, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Large Scale Foundation Model on Single-cell Transcriptomics., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: p. 2023.05.29.542705. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 143. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Xu, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: CIForm as a Transformer-based model for cell-type annotation of large-scale single-, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: cell RNA-seq data., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Brief Bioinform, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 24, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4). , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 144. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chen, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Transformer for one stop interpretable cell type annotation., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nat Commun, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 14, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 223. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 145. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Jiao, L., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: scTransSort: Transformers for Intelligent Annotation of Cell Types by Gene , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Embeddings., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Biomolecules, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 13, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4). , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 146. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Song, T., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: TransCluster: A Cell-Type Identification Method for single-cell RNA-Seq data , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: using deep learning based on transformer., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Front Genet, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 13, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: : p. 1038919. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 147. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Yang, F., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: scBERT as a large-scale pretrained deep language model for cell type annotation , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: of single-cell RNA-seq data., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature Machine Intelligence, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 4, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (10): p. 852-866. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 148. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cui, H., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Generative AI., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. 2023.04. 30.538439. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 149. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Choromanski, K., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Rethinking attention with performers., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv preprint arXiv:2009.14794, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027

Text: 150. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Roohani, Y., K. Huang, and J. Leskovec, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: GEARS: Predicting transcriptional outcomes of novel , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: multi-gene perturbations., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  BioRxiv, 2022: p. 2022.07. 12.499735. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 151. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Theodoris, C.V., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Transfer learning enables predictions in network biology., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nature, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 618, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (7965): p. 616-624. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 152. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Li, G., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: A deep generative model for multi-view profiling of single-cell RNA-seq and ATAC-, Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: seq data., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Genome Biol, 2022. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 23, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 20. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 153. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ma, S., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chromatin Potential Identified by Shared Single-Cell Profiling of RNA and , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: Chromatin., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Cell, 2020. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 183, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (4): p. 1103-1116 e20. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 154. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Cao, J., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Joint profiling of chromatin accessibility and gene expression in thousands of single , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: cells., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Science, 2018. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 361, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (6409): p. 1380-1385. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 155. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Chen, S., B.B. Lake, and K. Zhang, , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: High-throughput sequencing of the transcriptome and , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: chromatin accessibility in the same cell., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nat Biotechnol, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 37, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (12): p. 1452-1457. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 156. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Zhu, C., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: An ultra high-throughput method for single-cell joint analysis of open chromatin , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: and transcriptome., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nat Struct Mol Biol, 2019. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 26, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (11): p. 1063-1070. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 157. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Ma, A., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Single-cell biological network inference using a heterogeneous graph transformer., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Nat Commun, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 14, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (1): p. 964. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 158. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Linjing, L., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: A pre-trained large language model for translating single-cell transcriptome to , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: proteome., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  bioRxiv, 2023: p. 2023.07.04.547619. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 159. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Tang, W., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Single-Cell Multimodal Prediction via Transformers., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  arXiv preprint , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: arXiv:2303.00233, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 160. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: Szklarczyk, D., et al., , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: The STRING database in 2023: protein-protein association networks and , Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text: functional enrichment analyses for any sequenced genome of interest., Font: TimesNewRomanPS-ItalicMT, Size: 11.039999961853027
Text:  Nucleic Acids Res, 2023. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text: 51, Font: TimesNewRomanPS-BoldMT, Size: 11.039999961853027
Text: (D1): p. D638-D646. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0

Text: Legends of figures , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Figure 1. Summary of the application of large language models in bioinformatics in this , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: review. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Applications of large language models in bioinformatics include applications in genomics, , Font: TimesNewRomanPSMT, Size: 12.0
Text: transcriptomics, proteomics, drug discovery and single cell analysis. Applications of LLMs in , Font: TimesNewRomanPSMT, Size: 12.0
Text: genomics focus on LLMs using DNA sequence; applications of LLMs in transcriptomics using , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA sequence; applications of LLMs in proteomics focus on LLMs using protein sequence; , Font: TimesNewRomanPSMT, Size: 12.0
Text: applications of LLMs in drug discovery focus on LLMs using molecular data and applications of , Font: TimesNewRomanPSMT, Size: 12.0
Text: LLMs in single-cell analysis focus on LLMs using gene expression data from scRNA-seq or , Font: TimesNewRomanPSMT, Size: 12.0
Text: scMulti-omics data., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: Each corresponds to a variety of biological downstream tasks. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Figure 2. Schematic diagram of large language model. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: The input of large language models is , Font: TimesNewRomanPSMT, Size: 12.0
Text: tokenized and fed into embedding layers. Large language models, particularly exemplified by , Font: TimesNewRomanPSMT, Size: 12.0
Text: BERT-based (transformer encoder) and GPT-based (transformer decoder) models, their core is , Font: TimesNewRomanPSMT, Size: 12.0
Text: attention mechanism. The training process of large language models usually includes pre-training , Font: TimesNewRomanPSMT, Size: 12.0
Text: and fine-tuning. Pre-training is mainly to train a general/foundation model with strong , Font: TimesNewRomanPSMT, Size: 12.0
Text: generalization ability on a large-scale unlabeled reference dataset using self-supervised learning. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Fine-tuning relies on the pre-trained model, undergoing additional training for a specific task. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 3. Applications of large language models in genomics., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  The DNA language models take , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNA sequence as input, use transformer, BERT, GPT models to solve multiple biological tasks, , Font: TimesNewRomanPSMT, Size: 12.0
Text: including genome-wide variant effects prediction, DNA cis-regulatory regions prediction, DNA-, Font: TimesNewRomanPSMT, Size: 12.0
Text: protein interaction prediction, DNA methylation (6mA,4mC 5hmC) prediction, splice sites , Font: TimesNewRomanPSMT, Size: 12.0
Text: prediction from DNA sequence.  , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 4. Applications of large language models in transcriptomics., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  The RNA language , Font: TimesNewRomanPSMT, Size: 12.0
Text: models take RNA sequences as input, use transformer, BERT, GPT models to solve multiple , Font: TimesNewRomanPSMT, Size: 12.0
Text: biological tasks, including RNA 2D/3D structure prediction, RNA structural alignment,, RNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: family clustering, RNA splice sites prediction from RNA sequence, RNA N7-methylguanosine , Font: TimesNewRomanPSMT, Size: 12.0
Text: modification prediction, RNA 2‚Äô-O-methylation modifications prediction, multiple types of RNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: modifications prediction, predicting the association between miRNA, lncRNA and disease, , Font: TimesNewRomanPSMT, Size: 12.0

Text: identifying lncRNAs, lncRNAs‚Äô coding potential prediction, protein expression and mRNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: degradation prediction. , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text: Figure 5. Applications of large language models in proteomics., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  The protein language models , Font: TimesNewRomanPSMT, Size: 12.0
Text: take multiple sequence alignment, protein sequence, gene ontology and protein-relation-attribute , Font: TimesNewRomanPSMT, Size: 12.0
Text: as input, use transformer, BERT, GPT models to solve multiple biological tasks, including , Font: TimesNewRomanPSMT, Size: 12.0
Text: predicting secondary structure, predicting protein generation, predicting protein function, , Font: TimesNewRomanPSMT, Size: 12.0
Text: predicting post-translational modifications, predicting evolution and mutation, predicting , Font: TimesNewRomanPSMT, Size: 12.0
Text: biophysical properties, predicting protein-protein interaction and predicting antigen-receptor or , Font: TimesNewRomanPSMT, Size: 12.0
Text: antigen-antibody binding., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri-Bold, Size: 11.039999961853027
Text:  , Font: Calibri-Bold, Size: 11.039999961853027
Text: Figure 6. Applications of large language models in drug discovery., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  The language models for , Font: TimesNewRomanPSMT, Size: 12.0
Text: drug discovery take molecular SMILES, protein sequence, molecular fingerprints and molecular , Font: TimesNewRomanPSMT, Size: 12.0
Text: graphs as input, use transformer, BERT, GPT models to solve multiple biological tasks, including , Font: TimesNewRomanPSMT, Size: 12.0
Text: predicting molecular properties, predicting drug-target interaction, generating molecules and , Font: TimesNewRomanPSMT, Size: 12.0
Text: predicting synergistic effects., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri-Bold, Size: 11.039999961853027
Text:  , Font: Calibri-Bold, Size: 11.039999961853027
Text: Figure 7. Applications of large language models in single cell analysis., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  The single cell language , Font: TimesNewRomanPSMT, Size: 12.0
Text: models take gene expression or single cell multi-omics data as input, use transformer, BERT, GPT , Font: TimesNewRomanPSMT, Size: 12.0
Text: models to solve multiple biological tasks, including cell type annotation, batch effect removal, , Font: TimesNewRomanPSMT, Size: 12.0
Text: multi-omics integration, gene regulation network inference perturbation prediction, dropout , Font: TimesNewRomanPSMT, Size: 12.0
Text: imputation., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri-Bold, Size: 11.039999961853027
Text:  , Font: Calibri-Bold, Size: 11.039999961853027
Text:  , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0

Text: Figure 1. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0

Text:  , Font: Calibri, Size: 12.0
Text: Figure 2. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0

Text: Figure 3. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0

Text: Figure 4., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0

Text: Figure 5., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0

Text: Figure 6. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0

Text: Figure 7. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text:  , Font: Calibri, Size: 12.0

Text: *These are horizontal tables. , Font: TimesNewRomanPSMT, Size: 12.0
Text: Table 1. Large language models for genomic and transcriptomic tasks, Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Input data , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Biological tasks , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Models , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: DNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequence , Font: TimesNewRomanPSMT, Size: 12.0
Text: Genome-wide variant effects prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNABERT, DNABERT-2, GPN,, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text: Nucleotide , Font: TimesNewRomanPSMT, Size: 12.0
Text: Transformer , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNA cis-regulatory regions prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNABERT, DNABERT-2, BERT-Promoter, iEnhancer-, Font: TimesNewRomanPSMT, Size: 12.0
Text: BERT,, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 12.0
Text: Nucleotide Transformer , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNA-protein interaction prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNABERT, DNABERT-2, TFBert, GROVER, and , Font: TimesNewRomanPSMT, Size: 12.0
Text: MoDNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNA methylation (6mA,4mC 5hmC) prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: BERT6mA, iDNA-ABF, iDNA-ABT, and MuLan-, Font: TimesNewRomanPSMT, Size: 12.0
Text: Methyl , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA splice sites prediction from DNA sequence , Font: TimesNewRomanPSMT, Size: 12.0
Text: DNABERT, DNABERT-2 , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: sequence , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA 2D/3D structure prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA-FM, RNA-MSM, and RNA-FM , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA structural alignment, RNA family clustering , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNABERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA splice sites prediction from RNA sequence , Font: TimesNewRomanPSMT, Size: 12.0
Text: SpliceBERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA N7-Methylguanosine modification prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: BERT-m7G , Font: TimesNewRomanPSMT, Size: 12.0
Text: RNA 2'-O-methylation Modifications prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: Bert2Ome , Font: TimesNewRomanPSMT, Size: 12.0
Text: Multiple types of RNA modifications prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: Rm-LR , Font: TimesNewRomanPSMT, Size: 12.0
Text: Predicting the association between miRNA, lncRNA , Font: TimesNewRomanPSMT, Size: 12.0
Text: and disease , Font: TimesNewRomanPSMT, Size: 12.0
Text: BertNDA , Font: TimesNewRomanPSMT, Size: 12.0
Text: Identifying lncRNAs , Font: TimesNewRomanPSMT, Size: 12.0
Text: LncCat , Font: TimesNewRomanPSMT, Size: 12.0
Text: lncRNAs‚Äô coding potential prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: LSCPP-BERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: Protein expression and mRNA degradation prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: CodonBERT , Font: TimesNewRomanPSMT, Size: 12.0

Text: Table 2. Large language models for proteomic tasks., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: Input data , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Downstream tasks , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Models , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Protein sequences , Font: TimesNewRomanPSMT, Size: 12.0
Text: MSAs , Font: TimesNewRomanPSMT, Size: 12.0
Text: Gene ontology annotations , Font: TimesNewRomanPSMT, Size: 12.0
Text: Triplets of protein-relation-, Font: TimesNewRomanPSMT, Size: 12.0
Text: attribute , Font: TimesNewRomanPSMT, Size: 12.0
Text: Protein property descriptions , Font: TimesNewRomanPSMT, Size: 12.0
Text: Secondary structure and contact prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: MSA Transformer, ProtTrans, Font: TimesNewRomanPSMT, Size: 12.0
Text: Ôºå, Font: DengXian, Size: 12.0
Text: SPRoBERTa, , Font: TimesNewRomanPSMT, Size: 12.0
Text: TAPE,, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 10.5600004196167
Text: KeAP , Font: TimesNewRomanPSMT, Size: 12.0
Text: Protein sequence generation , Font: TimesNewRomanPSMT, Size: 12.0
Text: ProGen, ProtGPT2 , Font: TimesNewRomanPSMT, Size: 12.0
Text: Protein function prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: SPRoBERTa, ProtST,, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 10.5600004196167
Text: PromptProtein , Font: TimesNewRomanPSMT, Size: 12.0
Text: Major PTMs prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: ProteinBERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: Evolution and mutation prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: SPRoBERTa, UniRep, ESM-1b, TAPE , Font: TimesNewRomanPSMT, Size: 12.0
Text: Biophysical properties prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: TAPE, PromptProtein , Font: TimesNewRomanPSMT, Size: 12.0
Text: Protein-protein interaction and binding , Font: TimesNewRomanPSMT, Size: 12.0
Text: affinity prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: KeAP , Font: TimesNewRomanPSMT, Size: 12.0
Text: Antigen-Recepter binding prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: MHCRoBERTa, BERTMHC, TCR-BERT, SC-, Font: TimesNewRomanPSMT, Size: 12.0
Text: AIR-BERT , Font: TimesNewRomanPSMT, Size: 12.0
Text: Antigen-Antibody binding prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: AbLang, AntiBERTa, EATLM , Font: TimesNewRomanPSMT, Size: 12.0
Text: * The input of models is one or several types of protein language data. , Font: TimesNewRomanPSMT, Size: 11.039999961853027
Text:  , Font: Calibri, Size: 11.039999961853027
Text:  , Font: Calibri, Size: 11.039999961853027
Text:  , Font: Calibri, Size: 11.039999961853027

Text: Table 3. Large language models for drug discovery tasks., Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text: Input data , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Downstream tasks , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Models , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Molecular SMILES, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Predicting Molecular Properties, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: SMILES-BERT, ChemBERTa, K-BERT, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Generating Molecules, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: MolGPT, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Molecular graphs, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Predicting Molecular Properties, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: MOLE-BERT, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Molecular fingerprints and protein sequences, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Predicting Drug-Target Interaction, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: DTI-BERT, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Molecular SMILES and protein sequences, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Predicting Synergistic Effects, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: TransDTI, C2P2, Hyeunseok Kang et al., Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027
Text:  , Font: Calibri, Size: 11.039999961853027
Text:  , Font: Calibri, Size: 11.039999961853027

Text: Table 4. Large language models for single cell tasks. , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Input data , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Downstream tasks , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Models , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: scRNA-seq data, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: Cell clustering, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: tGPT, scFoundation , Font: TimesNewRomanPSMT, Size: 12.0
Text: Cell type annotation, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: CIForm, TOSICA, scTransSort, , Font: TimesNewRomanPSMT, Size: 12.0
Text: TransCluster, scBERT, scGPT , Font: TimesNewRomanPSMT, Size: 12.0
Text: Gene function analyses (Gene expression prediction, gene network , Font: TimesNewRomanPSMT, Size: 12.0
Text: inference, gene perturbation prediction, discovery of key network , Font: TimesNewRomanPSMT, Size: 12.0
Text: regulators, and identifying candidate therapeutic targets) , Font: TimesNewRomanPSMT, Size: 12.0
Text: scGPT, scFounfation, Geneformer, Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: TimesNewRomanPS-BoldMT, Size: 12.0
Text: scMuti-omics data , Font: TimesNewRomanPSMT, Size: 12.0
Text: Single-cell multi-omics integration , Font: TimesNewRomanPSMT, Size: 12.0
Text: scGPT, scMVP, DeepMAPS , Font: TimesNewRomanPSMT, Size: 12.0
Text: Biological network inference , Font: TimesNewRomanPSMT, Size: 12.0
Text: DeepMAPS , Font: TimesNewRomanPSMT, Size: 12.0
Text: Cell-cell communications , Font: TimesNewRomanPSMT, Size: 12.0
Text: Data imputation , Font: TimesNewRomanPSMT, Size: 12.0
Text: scMVP , Font: TimesNewRomanPSMT, Size: 12.0
Text: Translating gene expression to protein abundance , Font: TimesNewRomanPSMT, Size: 12.0
Text: scTranslator, scMoFormer , Font: TimesNewRomanPSMT, Size: 12.0
Text: Single-cell multimodal prediction , Font: TimesNewRomanPSMT, Size: 12.0
Text: scMoFormer , Font: TimesNewRomanPSMT, Size: 12.0
Text: Integrative regulatory inference , Font: TimesNewRomanPSMT, Size: 12.0
Text: scTranslator , Font: TimesNewRomanPSMT, Size: 12.0
Text:  , Font: Calibri, Size: 11.039999961853027