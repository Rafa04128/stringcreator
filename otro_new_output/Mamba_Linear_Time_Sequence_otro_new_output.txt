Text: Mamba: Linear-Time Sequence Modeling with Selective State Spaces, Font: SFRM1728, Size: 17.198217391967773
Text: Albert Gu, Font: STIXTwoText, Size: 11.955169677734375
Text: *, Font: STIXTwoText, Size: 10.361169815063477
Text: 1, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  and Tri Dao, Font: STIXTwoText, Size: 11.955169677734375
Text: *, Font: STIXTwoText, Size: 10.361169815063477
Text: 2, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 1, Font: STIXTwoMath, Size: 5.9775800704956055
Text: Machine Learning Department, Carnegie Mellon University, Font: STIXTwoText, Size: 11.955169677734375
Text: 2, Font: STIXTwoMath, Size: 5.9775800704956055
Text: Department of Computer Science, Princeton University, Font: STIXTwoText, Size: 11.955169677734375
Text: agu@cs.cmu.edu, Font: LMMono12-Regular, Size: 11.955169677734375
Text: ,, Font: STIXTwoText, Size: 11.955169677734375
Text:  tri@tridao.me, Font: LMMono12-Regular, Size: 11.955169677734375
Text: Abstract, Font: SFRM0900, Size: 8.96638011932373
Text: Foundation models, now powering most of the exciting applications in deep learning, are almost universally, Font: SFRM0900, Size: 9.033377647399902
Text: based on the Transformer architecture and its core attention module. Many subquadratic-time architectures, Font: SFRM0900, Size: 8.993239402770996
Text: such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs), Font: SFRM0900, Size: 9.020017623901367
Text: have been developed to address Transformers’ computational ineﬃciency on long sequences, but they have not, Font: SFRM0900, Size: 8.89887809753418
Text: performed as well as attention on important modalities such as language. We identify that a key weakness of, Font: SFRM0900, Size: 8.957408905029297
Text: such models is their inability to perform content-based reasoning, and make several improvements. First, simply, Font: SFRM0900, Size: 8.876263618469238
Text: letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing, Font: SFRM0900, Size: 8.876263618469238
Text: the model to selectively propagate or forget information along the sequence length dimension depending on, Font: SFRM0900, Size: 9.037826538085938
Text: the current token. Second, even though this change prevents the use of eﬃcient convolutions, we design a, Font: SFRM0900, Size: 9.055599212646484
Text: hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliﬁed, Font: SFRM0900, Size: 9.055599212646484
Text: end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast, Font: SFRM0900, Size: 8.988768577575684
Text: inference (5, Font: SFRM0900, Size: 8.876263618469238
Text: ×, Font: STIXTwoMath, Size: 8.96638011932373
Text:  higher throughput than Transformers) and linear scaling in sequence length, and its performance, Font: SFRM0900, Size: 8.876263618469238
Text: improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves, Font: SFRM0900, Size: 8.885315895080566
Text: state-of-the-art performance across several modalities such as language, audio, and genomics. On language, Font: SFRM0900, Size: 9.055599212646484
Text: modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice, Font: SFRM0900, Size: 8.984294891357422
Text: its size, both in pretraining and downstream evaluation., Font: SFRM0900, Size: 8.96638011932373
Text: 1, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Introduction, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have, Font: SFRM1000, Size: 9.952672004699707
Text: emerged as an eﬀective paradigm in modern machine learning. The backbone of these FMs are often sequence, Font: SFRM1000, Size: 10.061773300170898
Text: models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images,, Font: SFRM1000, Size: 10.061773300170898
Text: speech, audio, time series, and genomics (Brown et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2020, Font: SFRM1000, Size: 10.061773300170898
Text: ; Dosovitskiy et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2020, Font: SFRM1000, Size: 10.061773300170898
Text: ; Ismail Fawaz et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2019, Font: SFRM1000, Size: 10.061773300170898
Text: ;, Font: SFRM1000, Size: 10.061773300170898
Text: Oord et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2016, Font: SFRM1000, Size: 10.061773300170898
Text: ; Poli et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ; Sutskever, Vinyals, and Quoc V Le, Font: SFRM1000, Size: 10.061773300170898
Text:  2014, Font: SFRM1000, Size: 10.061773300170898
Text: ). While this concept is agnostic to, Font: SFRM1000, Size: 10.061773300170898
Text: a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence, Font: SFRM1000, Size: 10.061773300170898
Text: model: the Transformer (Vaswani et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2017, Font: SFRM1000, Size: 10.061773300170898
Text: ) and its core attention layer (Bahdanau, Cho, and Bengio, Font: SFRM1000, Size: 10.061773300170898
Text:  2015, Font: SFRM1000, Size: 10.061773300170898
Text: ), Font: SFRM1000, Size: 10.061773300170898
Text: The eﬃcacy of self-attention is attributed to its ability to route information densely within a context window,, Font: SFRM1000, Size: 10.061773300170898
Text: allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model, Font: SFRM1000, Size: 9.947684288024902
Text: anything outside of a ﬁnite window, and quadratic scaling with respect to the window length. An enormous body, Font: SFRM1000, Size: 9.8876371383667
Text: of research has appeared on more eﬃcient variants of attention to overcome these drawbacks (Tay, Dehghani,, Font: SFRM1000, Size: 10.061773300170898
Text: Bahri, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ), but often at the expense of the very properties that makes it eﬀective. As of yet, none of these, Font: SFRM1000, Size: 9.862509727478027
Text: variants have been shown to be empirically eﬀective at scale across domains., Font: SFRM1000, Size: 9.962639808654785
Text: Recently, structured state space sequence models (SSMs) (Gu, Goel, and Ré, Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ; Gu, Johnson, Goel, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2021, Font: SFRM1000, Size: 9.862509727478027
Text: ), Font: SFRM1000, Size: 9.862509727478027
Text: have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a, Font: SFRM1000, Size: 9.977572441101074
Text: combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration, Font: SFRM1000, Size: 10.042024612426758
Text: from classical state space models (Kalman, Font: SFRM1000, Size: 9.862509727478027
Text:  1960, Font: SFRM1000, Size: 9.862509727478027
Text: ). This class of models can be computed very eﬃciently as either a, Font: SFRM1000, Size: 9.862509727478027
Text: recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled, Font: SFRM1000, Size: 9.882617950439453
Text: *, Font: STIXTwoText, Size: 7.770880222320557
Text: Equal contribution., Font: STIXTwoText, Size: 7.970109939575195
Text: 1, Font: STIXTwoText, Size: 9.962639808654785
Text: mechanisms for modeling long-range dependencies (Gu, Dao, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2020, Font: SFRM1000, Size: 10.061773300170898
Text: ) in certain data modalities, and have, Font: SFRM1000, Size: 10.061773300170898
Text: dominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ). Many ﬂavors of, Font: SFRM1000, Size: 10.061773300170898
Text: SSMs (Gu, Goel, and Ré, Font: SFRM1000, Size: 9.927709579467773
Text:  2022, Font: SFRM1000, Size: 9.927709579467773
Text: ; Gu, Gupta, et al., Font: SFRM1000, Size: 9.927709579467773
Text:  2022, Font: SFRM1000, Size: 9.927709579467773
Text: ; Gupta, Gu, and Berant, Font: SFRM1000, Size: 9.927709579467773
Text:  2022, Font: SFRM1000, Size: 9.927709579467773
Text: ; Y. Li et al., Font: SFRM1000, Size: 9.927709579467773
Text:  2023, Font: SFRM1000, Size: 9.927709579467773
Text: ; Ma et al., Font: SFRM1000, Size: 9.927709579467773
Text: 2023, Font: SFRM1000, Size: 10.061773300170898
Text: ; Orvieto et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ; Smith, Warrington, and Linderman, Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ) have been successful in domains involving, Font: SFRM1000, Size: 10.061773300170898
Text: continuous signal data such as audio and vision (Goel et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ; Nguyen, Goel, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ; Saon, Gupta, and Cui, Font: SFRM1000, Size: 9.862509727478027
Text: 2023, Font: SFRM1000, Size: 9.862509727478027
Text: ). However, they have been less eﬀective at modeling discrete and information-dense data such as text., Font: SFRM1000, Size: 9.862509727478027
Text: We propose a new class of selective state space models, that improves on prior work on several axes to achieve the, Font: SFRM1000, Size: 9.862509727478027
Text: modeling power of Transformers while scaling linearly in sequence length., Font: SFRM1000, Size: 9.962639808654785
Text: Selection Mechanism., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: First, we identify a key limitation of prior models: the ability to eﬃciently select, Font: SFRM1000, Size: 10.061773300170898
Text: data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on, Font: SFRM1000, Size: 10.061773300170898
Text: important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by, Font: SFRM1000, Size: 9.927709579467773
Text: parameterizing the SSM parameters based on the input. This allows the model to ﬁlter out irrelevant information, Font: SFRM1000, Size: 9.877593994140625
Text: and remember relevant information indeﬁnitely., Font: SFRM1000, Size: 9.962639808654785
Text: Hardware-aware Algorithm., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: This simple change poses a technical challenge for the computation of the model;, Font: SFRM1000, Size: 9.867541313171387
Text: in fact, all prior SSMs models must be time- and input-invariant in order to be computationally eﬃcient. We, Font: SFRM1000, Size: 10.061773300170898
Text: overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of, Font: SFRM1000, Size: 10.061773300170898
Text: convolution, but does not materialize the expanded state in order to avoid IO access between diﬀerent levels of the, Font: SFRM1000, Size: 9.862509727478027
Text: GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling, Font: SFRM1000, Size: 10.037081718444824
Text: linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware, Font: SFRM1000, Size: 9.942694664001465
Text: (up to 3, Font: SFRM1000, Size: 9.962639808654785
Text: ×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  faster on A100 GPUs)., Font: SFRM1000, Size: 9.962639808654785
Text: Architecture., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We simplify prior deep sequence model architectures by combining the design of prior SSM, Font: SFRM1000, Size: 10.061773300170898
Text: architectures (Dao, Fu, Saab, et al., Font: SFRM1000, Size: 10.032135963439941
Text:  2023, Font: SFRM1000, Size: 10.032135963439941
Text: ) with the MLP block of Transformers into a single block, leading to a, Font: SFRM1000, Size: 10.032135963439941
Text: simple and homogenous architecture design (Mamba) incorporating selective state spaces., Font: SFRM1000, Size: 9.962639808654785
Text: Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that, Font: SFRM1000, Size: 10.061773300170898
Text: make them suitable as the backbone of general foundation models operating on sequences. (i) High quality:, Font: SFRM1000, Size: 10.061773300170898
Text: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and, Font: SFRM1000, Size: 9.987515449523926
Text: inference: computation and memory scales linearly in sequence length during training, and unrolling the model, Font: SFRM1000, Size: 10.0123291015625
Text: autoregressively during inference requires only constant time per step since it does not require a cache of previous, Font: SFRM1000, Size: 9.87256908416748
Text: elements. (iii) Long context: the quality and eﬃciency together yield performance improvements on real data up, Font: SFRM1000, Size: 9.932706832885742
Text: to sequence length 1M., Font: SFRM1000, Size: 9.962639808654785
Text: We empirically validate Mamba’s potential as a general sequence FM backbone, in both pretraining quality and, Font: SFRM1000, Size: 9.982544898986816
Text: domain-speciﬁc task performance, on several types of modalities and settings:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being, Font: SFRM1000, Size: 9.862509727478027
Text: key to large language models, Mamba not only solves them easily but can extrapolate solutions indeﬁnitely long, Font: SFRM1000, Size: 9.862509727478027
Text: (, Font: SFRM1000, Size: 9.962639808654785
Text: >, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1M tokens)., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform-, Font: SFRM1000, Size: 9.862509727478027
Text: ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g., Font: SFRM1000, Size: 9.87256908416748
Text: reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance, Font: SFRM1000, Size: 9.922708511352539
Text: improves with longer context up to million-length sequences., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Language Modeling. Mamba is the ﬁrst linear-time sequence model that truly achieves Transformer-quality, Font: SFRM1000, Size: 10.061773300170898
Text: performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters,, Font: SFRM1000, Size: 9.862509727478027
Text: we show that Mamba exceeds the performance of a large range of baselines, including very strong modern, Font: SFRM1000, Size: 10.061773300170898
Text: Transformer training recipes based on LLaMa (Touvron et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ). Our Mamba language model has 5, Font: SFRM1000, Size: 10.061773300170898
Text: ×, Font: STIXTwoMath, Size: 9.962639808654785
Text: generation throughput compared to Transformers of similar size, and Mamba-3B’s quality matches that of, Font: SFRM1000, Size: 10.061773300170898
Text: Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and, Font: SFRM1000, Size: 10.017284393310547
Text: even exceeding Pythia-7B)., Font: SFRM1000, Size: 9.962639808654785
Text: Model code and pre-trained checkpoints are open-sourced at, Font: SFRM1000, Size: 9.927709579467773
Text:  https://github.com/state-spaces/mamba, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.927709579467773
Text: 2, Font: STIXTwoText, Size: 9.962639808654785
Text: Project, Font: OpenSansCondensed-Bold, Size: 4.916999340057373
Text: Discretize, Font: OpenSansCondensed-Bold, Size: 4.916999340057373
Text: 𝑥, Font: CambriaMath, Size: 8.85059928894043
Text: !, Font: CambriaMath, Size: 6.392097473144531
Text: ℎ, Font: CambriaMath, Size: 11.800799369812012
Text: !"#, Font: CambriaMath, Size: 8.604747772216797
Text: ℎ, Font: CambriaMath, Size: 11.800799369812012
Text: !, Font: CambriaMath, Size: 8.604747772216797
Text: 𝑦, Font: CambriaMath, Size: 11.800799369812012
Text: !, Font: CambriaMath, Size: 8.604747772216797
Text: 𝐴, Font: CambriaMath, Size: 11.800799369812012
Text: 𝐶, Font: CambriaMath, Size: 11.800799369812012
Text: !, Font: CambriaMath, Size: 8.604747772216797
Text: 𝐵, Font: CambriaMath, Size: 11.800799369812012
Text: !, Font: CambriaMath, Size: 8.604747772216797
Text: Selection Mechanism, Font: OpenSansCondensed-Bold, Size: 6.8838019371032715
Text: GPU , Font: OpenSansCondensed-Bold, Size: 5.900399684906006
Text: SRAM, Font: OpenSansCondensed-Bold, Size: 5.900399684906006
Text: GPU HBM, Font: OpenSansCondensed-Bold, Size: 5.900399684906006
Text: ∆, Font: CambriaMath, Size: 11.800799369812012
Text: !, Font: CambriaMath, Size: 8.604747772216797
Text: Selective State Space Model, Font: OpenSans-Bold, Size: 11.800799369812012
Text: with Hardware-aware State Expansion, Font: OpenSans-SemiBoldItalic, Size: 8.85059928894043
Text: Figure 1: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Overview, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Structured SSMs independently map each channel (e.g., Font: STIXTwoText, Size: 8.96638011932373
Text:  퐷 = 5, Font: STIXTwoMath, Size: 8.96638011932373
Text: ) of an input, Font: STIXTwoText, Size: 8.96638011932373
Text:  푥, Font: STIXTwoMath, Size: 8.96638011932373
Text:  to output, Font: STIXTwoText, Size: 8.96638011932373
Text:  푦, Font: STIXTwoMath, Size: 8.96638011932373
Text:  through a higher, Font: STIXTwoText, Size: 8.96638011932373
Text: dimensional latent state, Font: STIXTwoText, Size: 8.96638011932373
Text:  ℎ, Font: STIXTwoMath, Size: 8.96638011932373
Text:  (e.g., Font: STIXTwoText, Size: 8.96638011932373
Text:  푁 = 4, Font: STIXTwoMath, Size: 8.96638011932373
Text: ). Prior SSMs avoid materializing this large efective state (, Font: STIXTwoText, Size: 8.96638011932373
Text: 퐷푁, Font: STIXTwoMath, Size: 8.96638011932373
Text: , times batch size, Font: STIXTwoText, Size: 8.96638011932373
Text:  퐵, Font: STIXTwoMath, Size: 8.96638011932373
Text:  and sequence, Font: STIXTwoText, Size: 8.96638011932373
Text: length, Font: STIXTwoText, Size: 8.96638011932373
Text:  퐿, Font: STIXTwoMath, Size: 8.96638011932373
Text: ) through clever alternate computation paths requiring time-invariance: the, Font: STIXTwoText, Size: 8.96638011932373
Text:  (∆,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  A, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  C, Font: CMMIB9, Size: 8.96638011932373
Text: ), Font: STIXTwoMath, Size: 8.96638011932373
Text:  parameters are constant across, Font: STIXTwoText, Size: 8.96638011932373
Text: time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to, Font: STIXTwoText, Size: 8.96638011932373
Text: only materialize the expanded states in more efcient levels of the GPU memory hierarchy., Font: STIXTwoText, Size: 8.96638011932373
Text: 2, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: State Space Models, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are, Font: SFRM1000, Size: 10.061773300170898
Text: broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous, Font: SFRM1000, Size: 9.862509727478027
Text: system, Font: SFRM1000, Size: 9.977572441101074
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  that maps a 1-dimensional function or sequence, Font: SFRM1000, Size: 9.977572441101074
Text:  푥(푡) ∈ ℝ ↦ 푦(푡) ∈ ℝ, Font: STIXTwoMath, Size: 9.962639808654785
Text:  through an implicit latent state, Font: SFRM1000, Size: 9.977572441101074
Text: ℎ(푡) ∈ ℝ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푁, Font: STIXTwoMath, Size: 7.471980094909668
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: Concretely, S4 models are deﬁned with four parameters, Font: SFRM1000, Size: 9.977572441101074
Text:  (∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: , which deﬁne a sequence-to-sequence trans-, Font: SFRM1000, Size: 9.977572441101074
Text: formation in two stages., Font: SFRM1000, Size: 9.962639808654785
Text: ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: ′, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푡) =, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ℎ(푡) +, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: 푥(푡), Font: STIXTwoMath, Size: 9.962639808654785
Text: (1a), Font: STIXTwoText, Size: 9.962639808654785
Text: 푦(푡) =, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ℎ(푡), Font: STIXTwoMath, Size: 9.962639808654785
Text: (1b), Font: STIXTwoText, Size: 9.962639808654785
Text: ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  =, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡−1, Font: STIXTwoMath, Size: 7.471980094909668
Text:  +, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: 푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: (2a), Font: STIXTwoText, Size: 9.962639808654785
Text: 푦, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  =, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: (2b), Font: STIXTwoText, Size: 9.962639808654785
Text: 푲 = (, Font: STIXTwoMath, Size: 9.962639808654785
Text: C, Font: CMMIB10, Size: 9.962639808654785
Text: 푩,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: 푨푩, … ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: 푨, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푘, Font: STIXTwoMath, Size: 7.471980094909668
Text: 푩, … ), Font: STIXTwoMath, Size: 9.962639808654785
Text: (3a), Font: STIXTwoText, Size: 9.962639808654785
Text: 푦 = 푥 ∗ 푲, Font: STIXTwoMath, Size: 9.962639808654785
Text: (3b), Font: STIXTwoText, Size: 9.962639808654785
Text: Discretization., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: The ﬁrst stage transforms the “continuous parameters”, Font: SFRM1000, Size: 9.862509727478027
Text:  (∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  to “discrete parameters”, Font: SFRM1000, Size: 9.862509727478027
Text:  (, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: through ﬁxed formulas, Font: SFRM1000, Size: 9.89767074584961
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  = 푓, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐴, Font: STIXTwoMath, Size: 7.471980094909668
Text: (∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  and, Font: SFRM1000, Size: 9.89767074584961
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text:  = 푓, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐵, Font: STIXTwoMath, Size: 7.471980094909668
Text: (∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: , where the pair, Font: SFRM1000, Size: 9.89767074584961
Text:  (푓, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐴, Font: STIXTwoMath, Size: 7.471980094909668
Text: , 푓, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐵, Font: STIXTwoMath, Size: 7.471980094909668
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  is called a discretization rule., Font: SFRM1000, Size: 9.89767074584961
Text: Various rules can be used such as the zero-order hold (ZOH) deﬁned in equation, Font: SFRM1000, Size: 9.962639808654785
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 4, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text:  = exp(∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: B, Font: CMMIB10, Size: 9.962639808654785
Text:  = (∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: −1, Font: STIXTwoMath, Size: 7.471980094909668
Text: (exp(∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ) −, Font: STIXTwoMath, Size: 9.962639808654785
Text:  I, Font: CMMIB10, Size: 9.962639808654785
Text: ) ⋅ ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: B, Font: CMMIB10, Size: 9.962639808654785
Text: (4), Font: STIXTwoText, Size: 9.962639808654785
Text: Discretization has deep connections to continuous-time systems which can endow them with additional properties, Font: SFRM1000, Size: 9.8876371383667
Text: such as resolution invariance (Nguyen, Goel, et al., Font: SFRM1000, Size: 10.002410888671875
Text:  2022, Font: SFRM1000, Size: 10.002410888671875
Text: ) and automatically ensuring that the model is properly, Font: SFRM1000, Size: 10.002410888671875
Text: normalized (Gu, Johnson, Timalsina, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ; Orvieto et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ). It also has connections to gating mechanisms, Font: SFRM1000, Size: 9.862509727478027
Text: of RNNs (Gu, Gulcehre, et al., Font: SFRM1000, Size: 9.917706489562988
Text:  2020, Font: SFRM1000, Size: 9.917706489562988
Text: ; Tallec and Ollivier, Font: SFRM1000, Size: 9.917706489562988
Text:  2018, Font: SFRM1000, Size: 9.917706489562988
Text: ) which we will revisit in Section, Font: SFRM1000, Size: 9.917706489562988
Text:  3.5, Font: SFRM1000, Size: 9.917706489562988
Text: . However, from, Font: SFRM1000, Size: 9.917706489562988
Text: a mechanical point of view discretization can simply be viewed as the ﬁrst step of the computation graph in the, Font: SFRM1000, Size: 9.972597122192383
Text: forward pass of an SSM. Alternate ﬂavors of SSMs can bypass the discretization step and parameterize, Font: SFRM1000, Size: 10.0123291015625
Text:  (, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: directly instead (Zhang et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ), which may be easier to reason about., Font: SFRM1000, Size: 9.962639808654785
Text: Computation., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: After the parameters have been transformed from, Font: SFRM1000, Size: 10.022237777709961
Text:  (∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ) ↦ (, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: , the model can be, Font: SFRM1000, Size: 10.022237777709961
Text: computed in two ways, either as a linear recurrence, Font: SFRM1000, Size: 9.962639808654785
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  or a global convolution, Font: SFRM1000, Size: 9.962639808654785
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 3, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 3, Font: STIXTwoText, Size: 9.962639808654785
Text: Commonly, the model uses the convolutional mode, Font: SFRM1000, Size: 9.932706832885742
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 3, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  for eﬃcient parallelizable training (where the whole input, Font: SFRM1000, Size: 9.932706832885742
Text: sequence is seen ahead of time), and switched into recurrent mode, Font: SFRM1000, Size: 9.89767074584961
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  for eﬃcient autoregressive inference (where, Font: SFRM1000, Size: 9.89767074584961
Text: the inputs are seen one timestep at a time)., Font: SFRM1000, Size: 9.962639808654785
Text: Linear Time Invariance (LTI)., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: An important property of equations, Font: SFRM1000, Size: 9.87256908416748
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  to, Font: SFRM1000, Size: 9.87256908416748
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 3, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  is that the model’s dynamics are, Font: SFRM1000, Size: 9.87256908416748
Text: constant through time. In other words, Font: SFRM1000, Size: 9.937702178955078
Text:  (∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: , and consequently, Font: SFRM1000, Size: 9.937702178955078
Text:  (, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  as well, are ﬁxed for all time-steps., Font: SFRM1000, Size: 9.937702178955078
Text: This property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions., Font: SFRM1000, Size: 10.061773300170898
Text: Informally, we think of LTI SSMs as being equivalent to any linear recurrence, Font: SFRM1000, Size: 10.027188301086426
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 2a, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  or convolution, Font: SFRM1000, Size: 10.027188301086426
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 3b, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text: , and use, Font: SFRM1000, Size: 10.027188301086426
Text: LTI as an umbrella term for these classes of models., Font: SFRM1000, Size: 9.962639808654785
Text: Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental eﬃciency, Font: SFRM1000, Size: 10.017284393310547
Text: constraints, discussed in Section, Font: SFRM1000, Size: 10.017284393310547
Text:  3.3, Font: SFRM1000, Size: 10.017284393310547
Text: . However, a core insight of this work is that LTI models have fundamental, Font: SFRM1000, Size: 10.017284393310547
Text: limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint, Font: SFRM1000, Size: 9.877593994140625
Text: while overcoming the eﬃciency bottlenecks., Font: SFRM1000, Size: 9.962639808654785
Text: Structure and Dimensions., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Finally, we note that structured SSMs are so named because computing them, Font: SFRM1000, Size: 10.061773300170898
Text: eﬃciently also requires imposing structure on the, Font: SFRM1000, Size: 10.061773300170898
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  matrix. The most popular form of structure is diagonal, Font: SFRM1000, Size: 10.061773300170898
Text: (Gu, Gupta, et al., Font: SFRM1000, Size: 9.987515449523926
Text:  2022, Font: SFRM1000, Size: 9.987515449523926
Text: ; Gupta, Gu, and Berant, Font: SFRM1000, Size: 9.987515449523926
Text:  2022, Font: SFRM1000, Size: 9.987515449523926
Text: ; Smith, Warrington, and Linderman, Font: SFRM1000, Size: 9.987515449523926
Text:  2023, Font: SFRM1000, Size: 9.987515449523926
Text: ), which we also, Font: SFRM1000, Size: 9.987515449523926
Text: use., Font: SFRM1000, Size: 9.962639808654785
Text: In this case, the, Font: SFRM1000, Size: 9.862509727478027
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  ∈ ℝ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푁×푁, Font: STIXTwoMath, Size: 7.471980094909668
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text:  ∈ ℝ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푁×1, Font: STIXTwoMath, Size: 7.471980094909668
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text:  ∈ ℝ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1×푁, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.862509727478027
Text: matrices can all be represented by, Font: SFRM1000, Size: 9.862509727478027
Text:  푁, Font: STIXTwoMath, Size: 9.962639808654785
Text:  numbers. To operate over, Font: SFRM1000, Size: 9.862509727478027
Text: an input sequence, Font: SFRM1000, Size: 10.061773300170898
Text:  푥, Font: STIXTwoMath, Size: 9.962639808654785
Text:  of batch size, Font: SFRM1000, Size: 10.061773300170898
Text:  퐵, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and length, Font: SFRM1000, Size: 10.061773300170898
Text:  퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text:  with, Font: SFRM1000, Size: 10.061773300170898
Text:  퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text:  channels, the SSM is applied independently to each, Font: SFRM1000, Size: 10.061773300170898
Text: channel. Note that in this case, the total hidden state has dimension, Font: SFRM1000, Size: 10.046965599060059
Text:  퐷푁, Font: STIXTwoMath, Size: 9.962639808654785
Text:  per input, and computing it over the, Font: SFRM1000, Size: 10.046965599060059
Text: sequence length requires, Font: SFRM1000, Size: 10.061773300170898
Text:  푂(퐵퐿퐷푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  time and memory; this is the root of the fundamental eﬃciency bottleneck, Font: SFRM1000, Size: 10.061773300170898
Text: addressed in Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3.3, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: General State Space Models., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We note that the term state space model has a very broad meaning which simply, Font: SFRM1000, Size: 9.862509727478027
Text: represents the notion of any recurrent process with a latent state. It has been used to refer to many disparate, Font: SFRM1000, Size: 10.061773300170898
Text: concepts in diﬀerent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner, Font: SFRM1000, Size: 10.061773300170898
Text: et al., Font: SFRM1000, Size: 9.877593994140625
Text:  2020, Font: SFRM1000, Size: 9.877593994140625
Text: )), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny, Font: SFRM1000, Size: 9.877593994140625
Text:  2003, Font: SFRM1000, Size: 9.877593994140625
Text: )),, Font: SFRM1000, Size: 9.877593994140625
Text: Kalman ﬁlters (controls (Kalman, Font: SFRM1000, Size: 10.061773300170898
Text:  1960, Font: SFRM1000, Size: 10.061773300170898
Text: )), hidden Markov models (HMM) and linear dynamical systems (LDS), Font: SFRM1000, Size: 10.061773300170898
Text: (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning)., Font: SFRM1000, Size: 9.962639808654785
Text: Throughout this entire paper we use the term “SSM” to refer exclusively to the class of structured SSMs or S4, Font: SFRM1000, Size: 10.061773300170898
Text: models (Gu, Goel, and Ré, Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ; Gu, Gupta, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ; Gupta, Gu, and Berant, Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ; Hasani et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ; Ma et al., Font: SFRM1000, Size: 9.862509727478027
Text: 2023, Font: SFRM1000, Size: 9.862509727478027
Text: ; Smith, Warrington, and Linderman, Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ) and use these terms interchangeably. For convenience we may also, Font: SFRM1000, Size: 9.862509727478027
Text: include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution, Font: SFRM1000, Size: 10.061773300170898
Text: viewpoints (Y. Li et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ; Orvieto et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ; Poli et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ), and clarify nuances when necessary., Font: SFRM1000, Size: 9.962639808654785
Text: SSM Architectures., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: SSMs are standalone sequence transformations that can be incorporated into end-to-end, Font: SFRM1000, Size: 10.022237777709961
Text: neural network architectures., Font: SFRM1000, Size: 10.061773300170898
Text: (We also sometimes call SSM architectures SSNNs, which are to SSM layers as, Font: SFRM1000, Size: 10.061773300170898
Text: CNNs are to linear convolution layers.), Font: SFRM1000, Size: 10.061773300170898
Text: We discuss some of the most well-known SSM architectures, many of, Font: SFRM1000, Size: 10.061773300170898
Text: which will also serve as our primary baselines., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Linear attention (Katharopoulos et al., Font: SFRM1000, Size: 9.917706489562988
Text:  2020, Font: SFRM1000, Size: 9.917706489562988
Text: ) is an approximation of self-attention involving a recurrence which, Font: SFRM1000, Size: 9.917706489562988
Text: can be viewed as a degenerate linear SSM., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  H3 (Dao, Fu, Saab, et al., Font: SFRM1000, Size: 9.957657814025879
Text:  2023, Font: SFRM1000, Size: 9.957657814025879
Text: ) generalized this recurrence to use S4; it can be viewed as an architecture with, Font: SFRM1000, Size: 9.957657814025879
Text: an SSM sandwiched by two gated connections (Figure, Font: SFRM1000, Size: 9.997447967529297
Text:  3, Font: SFRM1000, Size: 9.997447967529297
Text: ). H3 also inserts a standard local convolution, which, Font: SFRM1000, Size: 9.997447967529297
Text: they frame as a shift-SSM, before the main SSM layer., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Hyena (Poli et al., Font: SFRM1000, Size: 9.867541313171387
Text:  2023, Font: SFRM1000, Size: 9.867541313171387
Text: ) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized, Font: SFRM1000, Size: 9.867541313171387
Text: global convolution (Romero et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2021, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  RetNet (Y. Sun et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ) adds an additional gate to the architecture and uses a simpler SSM, allowing, Font: SFRM1000, Size: 10.061773300170898
Text: an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of, Font: SFRM1000, Size: 10.061773300170898
Text: convolutions., Font: SFRM1000, Size: 9.962639808654785
Text: 4, Font: STIXTwoText, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  RWKV (B. Peng et al., Font: SFRM1000, Size: 9.867541313171387
Text:  2023, Font: SFRM1000, Size: 9.867541313171387
Text: ) is a recent RNN designed for language modeling based on another linear attention, Font: SFRM1000, Size: 9.867541313171387
Text: approximation (attention-free Transformer (S. Zhai et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: )). Its main “WKV” mechanism involves LTI, Font: SFRM1000, Size: 10.061773300170898
Text: recurrences and can be viewed as the ratio of two SSMs., Font: SFRM1000, Size: 9.962639808654785
Text: Other closely related SSMs and architectures are discussed further in an extended related work (Appendix, Font: SFRM1000, Size: 9.862509727478027
Text:  B, Font: SFRM1000, Size: 9.862509727478027
Text: ). We, Font: SFRM1000, Size: 9.862509727478027
Text: highlight in particular S5 (Smith, Warrington, and Linderman, Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ), QRNN (Bradbury et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2016, Font: SFRM1000, Size: 9.862509727478027
Text: ), and SRU (Lei, Font: SFRM1000, Size: 9.862509727478027
Text: et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2017, Font: SFRM1000, Size: 9.962639808654785
Text: ), which we view as the most closely related methods to our core selective SSM., Font: SFRM1000, Size: 9.962639808654785
Text: 3, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Selective State Space Models, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: We motivate our selection mechanism using intuition from synthetic tasks (Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.1, Font: SFRM1000, Size: 10.061773300170898
Text: ), then explain how to, Font: SFRM1000, Size: 10.061773300170898
Text: incorporate this mechanism into state space models (Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.2, Font: SFRM1000, Size: 10.061773300170898
Text: ). The resulting time-varying SSMs cannot, Font: SFRM1000, Size: 10.061773300170898
Text: use convolutions, presenting a technical challenge of how to compute them eﬃciently. We overcome this with, Font: SFRM1000, Size: 10.061773300170898
Text: a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.3, Font: SFRM1000, Size: 10.061773300170898
Text: ). We then, Font: SFRM1000, Size: 10.061773300170898
Text: describe a simple SSM architecture without attention or even MLP blocks (Section, Font: SFRM1000, Size: 9.90268325805664
Text:  3.4, Font: SFRM1000, Size: 9.90268325805664
Text: ). Finally, we discuss some, Font: SFRM1000, Size: 9.90268325805664
Text: additional properties of selection mechanisms (Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3.5, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: 3.1, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Motivation: Selection as a Means of Compression, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: We argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact,, Font: SFRM1000, Size: 9.977572441101074
Text: we can view the tradeoﬀs of popular sequence models from this point of view. For example, attention is both, Font: SFRM1000, Size: 10.061773300170898
Text: eﬀective and ineﬃcient because it explicitly does not compress context at all. This can be seen from the fact that, Font: SFRM1000, Size: 9.8876371383667
Text: autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the, Font: SFRM1000, Size: 9.862509727478027
Text: slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are, Font: SFRM1000, Size: 9.942694664001465
Text: eﬃcient because they have a ﬁnite state, implying constant-time inference and linear-time training. However, their, Font: SFRM1000, Size: 9.862509727478027
Text: eﬀectiveness is limited by how well this state has compressed the context., Font: SFRM1000, Size: 9.962639808654785
Text: To understand this principle, we focus on two running examples of synthetic tasks (Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  The Selective Copying task modiﬁes the popular Copying task (Arjovsky, Shah, and Bengio, Font: SFRM1000, Size: 10.002410888671875
Text:  2016, Font: SFRM1000, Size: 10.002410888671875
Text: ) by varying, Font: SFRM1000, Size: 10.002410888671875
Text: the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant, Font: SFRM1000, Size: 9.882617950439453
Text: tokens (colored) and ﬁlter out the irrelevant ones (white)., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning, Font: SFRM1000, Size: 9.862509727478027
Text: abilities of LLMs (Olsson et al., Font: SFRM1000, Size: 9.877593994140625
Text:  2022, Font: SFRM1000, Size: 9.877593994140625
Text: ). It requires context-aware reasoning to know when to produce the correct, Font: SFRM1000, Size: 9.877593994140625
Text: output in the appropriate context (black)., Font: SFRM1000, Size: 9.962639808654785
Text: These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the, Font: SFRM1000, Size: 10.056839942932129
Text: (, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  transitions in, Font: SFRM1000, Size: 9.992483139038086
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text: ) cannot let them select the correct information from their context, or aﬀect the hidden, Font: SFRM1000, Size: 9.992483139038086
Text: state passed along the sequence an in input-dependent way. From the convolutional view, it is known that global, Font: SFRM1000, Size: 9.912701606750488
Text: convolutions can solve the vanilla Copying task (Romero et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ) because it only requires time-awareness,, Font: SFRM1000, Size: 10.061773300170898
Text: but that they have diﬃculty with the Selective Copying task because of lack of content-awareness (Figure, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: SFRM1000, Size: 10.061773300170898
Text: )., Font: SFRM1000, Size: 10.061773300170898
Text: More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution, Font: SFRM1000, Size: 9.992483139038086
Text: kernels., Font: SFRM1000, Size: 9.962639808654785
Text: In summary, the eﬃciency vs. eﬀectiveness tradeoﬀ of sequence models is characterized by how well they compress, Font: SFRM1000, Size: 9.862509727478027
Text: their state: eﬃcient models must have a small state, while eﬀective models must have a state that contains all, Font: SFRM1000, Size: 10.061773300170898
Text: necessary information from the context. In turn, we propose that a fundamental principle for building sequence, Font: SFRM1000, Size: 9.987515449523926
Text: models is selectivity: or the context-aware ability to focus on or ﬁlter out inputs into a sequential state. In, Font: SFRM1000, Size: 10.061773300170898
Text: particular, a selection mechanism controls how information propagates or interacts along the sequence dimension, Font: SFRM1000, Size: 9.922708511352539
Text: (see Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3.5, Font: SFRM1000, Size: 9.962639808654785
Text:  for more discussion)., Font: SFRM1000, Size: 9.962639808654785
Text: 3.2, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Improving SSMs with Selection, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: One method of incorporating a selection mechanism into models is by letting their parameters that aﬀect, Font: SFRM1000, Size: 10.061773300170898
Text: interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be, Font: SFRM1000, Size: 9.987515449523926
Text: input-dependent., Font: SFRM1000, Size: 9.962639808654785
Text: 5, Font: STIXTwoText, Size: 9.962639808654785
Text: Input, Font: Muli-Light, Size: 7.116660118103027
Text: Output, Font: Muli-Light, Size: 7.116660118103027
Text: ?, Font: HelveticaNeue-Light, Size: 7.116660118103027
Text: Output, Font: Muli-Light, Size: 7.116660118103027
Text: Copying, Font: Muli-SemiBold, Size: 9.488880157470703
Text: Selective Copying, Font: Muli-SemiBold, Size: 9.488880157470703
Text: Input, Font: Muli-Light, Size: 7.116660118103027
Text: Induction Heads, Font: Muli-SemiBold, Size: 9.488880157470703
Text: Solution, Font: Muli-Light, Size: 7.116660118103027
Text: Perfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs, Font: Muli-Light, Size: 4.744440078735352
Text: Figure 2: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Left, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) The standard version of the Copying task involves constant spacing between input and output elements and is, Font: STIXTwoText, Size: 8.96638011932373
Text: easily solved by time-invariant models such as linear recurrences and global convolutions. (, Font: STIXTwoText, Size: 8.96638011932373
Text: Right Top, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) The Selective Copying task, Font: STIXTwoText, Size: 8.96638011932373
Text: has random spacing in between inputs and requires time-varying models that can, Font: STIXTwoText, Size: 8.96638011932373
Text:  selectively, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text:  remember or ignore inputs depending, Font: STIXTwoText, Size: 8.96638011932373
Text: on their content. (, Font: STIXTwoText, Size: 8.96638011932373
Text: Right Bottom, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) The Induction Heads task is an example of associative recall that requires retrieving an answer, Font: STIXTwoText, Size: 8.96638011932373
Text: based on context, a key ability for LLMs., Font: STIXTwoText, Size: 8.96638011932373
Text: Algorithm 1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  SSM (S4), Font: STIXTwoText, Size: 9.962639808654785
Text: Input:, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: 푥 ∶ (홱, 홻, 홳), Font: STIXTwoMath, Size: 8.96638011932373
Text: Output:, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: 푦 ∶ (홱, 홻, 홳), Font: STIXTwoMath, Size: 8.96638011932373
Text: 1:, Font: STIXTwoText, Size: 7.970109939575195
Text:  A, Font: CMMIB9, Size: 8.96638011932373
Text:  ∶ (홳, 홽) ← 햯햺헋햺헆햾헍햾헋, Font: STIXTwoMath, Size: 8.96638011932373
Text: ⊳, Font: STIXTwoMath, Size: 8.96638011932373
Text:  Represents structured, Font: STIXTwoText, Size: 8.96638011932373
Text:  푁 × 푁, Font: STIXTwoMath, Size: 8.96638011932373
Text:  matrix, Font: STIXTwoText, Size: 8.96638011932373
Text: 2:, Font: STIXTwoText, Size: 7.970109939575195
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text:  ∶ (홳, 홽) ← 햯햺헋햺헆햾헍햾헋, Font: STIXTwoMath, Size: 8.96638011932373
Text: 3:, Font: STIXTwoText, Size: 7.970109939575195
Text:  C, Font: CMMIB9, Size: 8.96638011932373
Text:  ∶ (홳, 홽) ← 햯햺헋햺헆햾헍햾헋, Font: STIXTwoMath, Size: 8.96638011932373
Text: 4:, Font: STIXTwoText, Size: 7.970109939575195
Text:  ∆ ∶ (홳) ← 휏, Font: STIXTwoMath, Size: 8.96638011932373
Text: ∆, Font: STIXTwoMath, Size: 5.9775800704956055
Text: (햯햺헋햺헆햾헍햾헋), Font: STIXTwoMath, Size: 8.96638011932373
Text: 5:, Font: STIXTwoText, Size: 7.970109939575195
Text:  A, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text:  ∶ (홳, 홽) ← 햽헂헌햼헋햾헍헂헓햾(∆,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  A, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text: ), Font: STIXTwoMath, Size: 8.96638011932373
Text: 6:, Font: STIXTwoText, Size: 7.970109939575195
Text:  푦 ← 햲햲햬(, Font: STIXTwoMath, Size: 8.96638011932373
Text: A, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  C, Font: CMMIB9, Size: 8.96638011932373
Text: )(푥), Font: STIXTwoMath, Size: 8.96638011932373
Text: ⊳, Font: STIXTwoMath, Size: 8.96638011932373
Text:  Time-invariant: recurrence or convolution, Font: STIXTwoText, Size: 8.96638011932373
Text: 7:, Font: STIXTwoText, Size: 7.970109939575195
Text:  return, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text:  푦, Font: STIXTwoMath, Size: 8.96638011932373
Text: Algorithm 2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  SSM + Selection (S6), Font: STIXTwoText, Size: 9.962639808654785
Text: Input:, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: 푥 ∶ (홱, 홻, 홳), Font: STIXTwoMath, Size: 8.96638011932373
Text: Output:, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: 푦 ∶ (홱, 홻, 홳), Font: STIXTwoMath, Size: 8.96638011932373
Text: 1:, Font: STIXTwoText, Size: 7.970109939575195
Text:  A, Font: CMMIB9, Size: 8.96638011932373
Text:  ∶ (홳, 홽) ← 햯햺헋햺헆햾헍햾헋, Font: STIXTwoMath, Size: 8.96638011932373
Text: ⊳, Font: STIXTwoMath, Size: 8.96638011932373
Text:  Represents structured, Font: STIXTwoText, Size: 8.96638011932373
Text:  푁 × 푁, Font: STIXTwoMath, Size: 8.96638011932373
Text:  matrix, Font: STIXTwoText, Size: 8.96638011932373
Text: 2:, Font: STIXTwoText, Size: 7.970109939575195
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text:  ∶, Font: STIXTwoMath, Size: 8.96638011932373
Text:  (홱, 홻, 홽), Font: STIXTwoMath, Size: 8.96638011932373
Text:  ←, Font: STIXTwoMath, Size: 8.96638011932373
Text:  푠, Font: STIXTwoMath, Size: 8.96638011932373
Text: 퐵, Font: STIXTwoMath, Size: 5.9775800704956055
Text: (푥), Font: STIXTwoMath, Size: 8.96638011932373
Text: 3:, Font: STIXTwoText, Size: 7.970109939575195
Text:  C, Font: CMMIB9, Size: 8.96638011932373
Text:  ∶, Font: STIXTwoMath, Size: 8.96638011932373
Text:  (홱, 홻, 홽), Font: STIXTwoMath, Size: 8.96638011932373
Text:  ←, Font: STIXTwoMath, Size: 8.96638011932373
Text:  푠, Font: STIXTwoMath, Size: 8.96638011932373
Text: 퐶, Font: STIXTwoMath, Size: 5.9775800704956055
Text: (푥), Font: STIXTwoMath, Size: 8.96638011932373
Text: 4:, Font: STIXTwoText, Size: 7.970109939575195
Text:  ∆ ∶, Font: STIXTwoMath, Size: 8.96638011932373
Text:  (홱, 홻, 홳), Font: STIXTwoMath, Size: 8.96638011932373
Text:  ← 휏, Font: STIXTwoMath, Size: 8.96638011932373
Text: ∆, Font: STIXTwoMath, Size: 5.9775800704956055
Text: (햯햺헋햺헆햾헍햾헋, Font: STIXTwoMath, Size: 8.96638011932373
Text: +푠, Font: STIXTwoMath, Size: 8.96638011932373
Text: ∆, Font: STIXTwoMath, Size: 5.9775800704956055
Text: (푥), Font: STIXTwoMath, Size: 8.96638011932373
Text: ), Font: STIXTwoMath, Size: 8.96638011932373
Text: 5:, Font: STIXTwoText, Size: 7.970109939575195
Text:  A, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text:  ∶, Font: STIXTwoMath, Size: 8.96638011932373
Text:  (홱, 홻, 홳, 홽), Font: STIXTwoMath, Size: 8.96638011932373
Text:  ← 햽헂헌햼헋햾헍헂헓햾(∆,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  A, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text: ), Font: STIXTwoMath, Size: 8.96638011932373
Text: 6:, Font: STIXTwoText, Size: 7.970109939575195
Text:  푦 ← 햲햲햬(, Font: STIXTwoMath, Size: 8.96638011932373
Text: A, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text: ,, Font: STIXTwoMath, Size: 8.96638011932373
Text:  C, Font: CMMIB9, Size: 8.96638011932373
Text: )(푥), Font: STIXTwoMath, Size: 8.96638011932373
Text: ⊳, Font: STIXTwoMath, Size: 8.96638011932373
Text:  Time-varying, Font: STIXTwoText, Size: 8.96638011932373
Text: : recurrence (, Font: STIXTwoText, Size: 8.96638011932373
Text: scan, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) only, Font: STIXTwoText, Size: 8.96638011932373
Text: 7:, Font: STIXTwoText, Size: 7.970109939575195
Text:  return, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text:  푦, Font: STIXTwoMath, Size: 8.96638011932373
Text: Algorithms, Font: SFRM1000, Size: 9.977572441101074
Text:  1, Font: SFRM1000, Size: 9.977572441101074
Text:  and, Font: SFRM1000, Size: 9.977572441101074
Text:  2, Font: SFRM1000, Size: 9.977572441101074
Text:  illustrates the main selection mechanism that we use. The main diﬀerence is simply making, Font: SFRM1000, Size: 9.977572441101074
Text: several parameters, Font: SFRM1000, Size: 9.892655372619629
Text:  ∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text:  functions of the input, along with the associated changes to tensor shapes throughout., Font: SFRM1000, Size: 9.892655372619629
Text: In particular, we highlight that these parameters now have a length dimension, Font: SFRM1000, Size: 10.061773300170898
Text:  퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text: , meaning that the model has, Font: SFRM1000, Size: 10.061773300170898
Text: changed from time-invariant to time-varying. (Note that shape annotations were described in Section, Font: SFRM1000, Size: 10.056839942932129
Text:  2, Font: SFRM1000, Size: 10.056839942932129
Text: ). This, Font: SFRM1000, Size: 10.056839942932129
Text: loses the equivalence to convolutions, Font: SFRM1000, Size: 9.962639808654785
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 3, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  with implications for its eﬃciency, discussed next., Font: SFRM1000, Size: 9.962639808654785
Text: We speciﬁcally choose, Font: SFRM1000, Size: 10.061773300170898
Text:  푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐵, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥) = 햫헂헇햾햺헋, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푁, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥), Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.061773300170898
Text:  푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐶, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥) = 햫헂헇햾햺헋, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푁, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥), Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.061773300170898
Text:  푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥) = 햡헋허햺햽햼햺헌헍, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐷, Font: STIXTwoMath, Size: 7.471980094909668
Text: (햫헂헇햾햺헋, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥)), Font: STIXTwoMath, Size: 9.962639808654785
Text: , and, Font: SFRM1000, Size: 10.061773300170898
Text:  휏, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = 헌허햿헍헉헅헎헌, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.061773300170898
Text: where, Font: SFRM1000, Size: 10.061773300170898
Text:  햫헂헇햾햺헋, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푑, Font: STIXTwoMath, Size: 7.471980094909668
Text:  is a parameterized projection to dimension, Font: SFRM1000, Size: 10.061773300170898
Text:  푑, Font: STIXTwoMath, Size: 9.962639808654785
Text: . The choice of, Font: SFRM1000, Size: 10.061773300170898
Text:  푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  and, Font: SFRM1000, Size: 10.061773300170898
Text:  휏, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  is due to a connection to, Font: SFRM1000, Size: 10.061773300170898
Text: RNN gating mechanisms explained in Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3.5, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 3.3, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Efcient Implementation of Selective SSMs, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Hardware-friendly architectures such as convolutions (Krizhevsky, Sutskever, and Hinton, Font: SFRM1000, Size: 10.027188301086426
Text:  2012, Font: SFRM1000, Size: 10.027188301086426
Text: ) and Transform-, Font: SFRM1000, Size: 10.027188301086426
Text: ers (Vaswani et al., Font: SFRM1000, Size: 9.952672004699707
Text:  2017, Font: SFRM1000, Size: 9.952672004699707
Text: ) enjoy widespread application. Here we aim to make selective SSMs eﬃcient on modern, Font: SFRM1000, Size: 9.952672004699707
Text: hardware (GPU) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate, Font: SFRM1000, Size: 10.002410888671875
Text: special cases of selection, such as letting, Font: SFRM1000, Size: 9.89767074584961
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  vary over time in recurrent SSMs (Gu, Dao, et al., Font: SFRM1000, Size: 9.89767074584961
Text:  2020, Font: SFRM1000, Size: 9.89767074584961
Text: ). However, as, Font: SFRM1000, Size: 9.89767074584961
Text: previously mentioned a core limitation in the usage of SSMs is their computational eﬃciency, which was why S4, Font: SFRM1000, Size: 9.922708511352539
Text: and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions., Font: SFRM1000, Size: 9.962639808654785
Text: 3.3.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Motivation of Prior Models, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We ﬁrst revisit this motivation and overview our approach to overcome limitations of prior methods., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  At a high level, recurrent models such as SSMs always balance a tradeoﬀ between expressivity and speed: as, Font: SFRM1000, Size: 10.037081718444824
Text: discussed in Section, Font: SFRM1000, Size: 9.967619895935059
Text:  3.1, Font: SFRM1000, Size: 9.967619895935059
Text: , models with larger hidden state dimension should be more eﬀective but slower. Thus, Font: SFRM1000, Size: 9.967619895935059
Text: 6, Font: STIXTwoText, Size: 9.962639808654785
Text: we want to maximize hidden state dimension without paying speed and memory costs., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Note that the recurrent mode is more ﬂexible than the convolution mode, since the latter, Font: SFRM1000, Size: 10.037081718444824
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 3, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  is derived from, Font: SFRM1000, Size: 10.037081718444824
Text: expanding the former, Font: SFRM1000, Size: 9.87256908416748
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  (Gu, Goel, and Ré, Font: SFRM1000, Size: 9.87256908416748
Text:  2022, Font: SFRM1000, Size: 9.87256908416748
Text: ; Gu, Johnson, Goel, et al., Font: SFRM1000, Size: 9.87256908416748
Text:  2021, Font: SFRM1000, Size: 9.87256908416748
Text: ). However, this would require, Font: SFRM1000, Size: 9.87256908416748
Text: computing and materializing the latent state, Font: SFRM1000, Size: 9.997447967529297
Text:  ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text:  with shape, Font: SFRM1000, Size: 9.997447967529297
Text:  (홱, 홻, 홳, 홽), Font: STIXTwoMath, Size: 9.962639808654785
Text: , much larger (by a factor of, Font: SFRM1000, Size: 9.997447967529297
Text:  푁, Font: STIXTwoMath, Size: 9.962639808654785
Text: , the SSM, Font: SFRM1000, Size: 9.997447967529297
Text: state dimension) than the input, Font: SFRM1000, Size: 9.862509727478027
Text:  푥, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and output, Font: SFRM1000, Size: 9.862509727478027
Text:  푦, Font: STIXTwoMath, Size: 9.962639808654785
Text:  of shape, Font: SFRM1000, Size: 9.862509727478027
Text:  (홱, 홻, 홳), Font: STIXTwoMath, Size: 9.962639808654785
Text: . Thus the more eﬃcient convolution mode was, Font: SFRM1000, Size: 9.862509727478027
Text: introduced which could bypass the state computation and materializes a convolution kernel, Font: SFRM1000, Size: 9.867541313171387
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 3a, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  of only, Font: SFRM1000, Size: 9.867541313171387
Text:  (홱, 홻, 홳), Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.867541313171387
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Prior LTI SSMs leverage the dual recurrent-convolutional forms to increase the eﬀective state dimension by a, Font: SFRM1000, Size: 9.992483139038086
Text: factor of, Font: SFRM1000, Size: 9.962639808654785
Text:  푁, Font: STIXTwoMath, Size: 9.962639808654785
Text:  (, Font: SFRM1000, Size: 9.962639808654785
Text: ≈ 10 − 100, Font: STIXTwoMath, Size: 9.962639808654785
Text: ), much larger than traditional RNNs, without eﬃciency penalties., Font: SFRM1000, Size: 9.962639808654785
Text: 3.3.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Overview of Selective Scan: Hardware-Aware State Expansion, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: The selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore, Font: SFRM1000, Size: 10.017284393310547
Text: need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion,, Font: SFRM1000, Size: 9.957657814025879
Text: parallel scan, and recomputation. We make two main observations:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  The naive recurrent computation uses, Font: SFRM1000, Size: 9.862509727478027
Text:  푂(퐵퐿퐷푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  FLOPs while the convolutional computation uses, Font: SFRM1000, Size: 9.862509727478027
Text:  푂(퐵퐿퐷 log(퐿)), Font: STIXTwoMath, Size: 9.962639808654785
Text: FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension, Font: SFRM1000, Size: 9.907693862915039
Text: 푁, Font: STIXTwoMath, Size: 9.962639808654785
Text: , the recurrent mode can actually use fewer FLOPs., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter,, Font: SFRM1000, Size: 9.927709579467773
Text: just like the convolutional mode, we can attempt to not actually materialize the full state, Font: SFRM1000, Size: 9.962639808654785
Text:  ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state, Font: SFRM1000, Size: 10.061773300170898
Text:  ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text:  only in more, Font: SFRM1000, Size: 10.061773300170898
Text: eﬃcient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded, Font: SFRM1000, Size: 9.862509727478027
Text: by memory bandwidth (Dao, Fu, Ermon, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2022, Font: SFRM1000, Size: 10.061773300170898
Text: ; Ivanov et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ; Williams, Waterman, and Patterson, Font: SFRM1000, Size: 10.061773300170898
Text: 2009, Font: SFRM1000, Size: 9.862509727478027
Text: ). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to, Font: SFRM1000, Size: 9.862509727478027
Text: a signiﬁcant speedup compared to a standard implementation., Font: SFRM1000, Size: 9.962639808654785
Text: Concretely, instead of preparing the scan input, Font: SFRM1000, Size: 9.882617950439453
Text:  (, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  of size, Font: SFRM1000, Size: 9.882617950439453
Text:  (홱, 홻, 홳, 홽), Font: STIXTwoMath, Size: 9.962639808654785
Text:  in GPU HBM (high-bandwidth memory),, Font: SFRM1000, Size: 9.882617950439453
Text: we load the SSM parameters, Font: SFRM1000, Size: 9.927709579467773
Text:  (∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  directly from slow HBM to fast SRAM, perform the discretization and, Font: SFRM1000, Size: 9.927709579467773
Text: recurrence in SRAM, and then write the ﬁnal outputs of size, Font: SFRM1000, Size: 9.962639808654785
Text:  (홱, 홻, 홳), Font: STIXTwoMath, Size: 9.962639808654785
Text:  back to HBM., Font: SFRM1000, Size: 9.962639808654785
Text: To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a, Font: SFRM1000, Size: 10.061773300170898
Text: work-eﬃcient parallel scan algorithm (Blelloch, Font: SFRM1000, Size: 9.87256908416748
Text:  1990, Font: SFRM1000, Size: 9.87256908416748
Text: ; Martin and Cundy, Font: SFRM1000, Size: 9.87256908416748
Text:  2018, Font: SFRM1000, Size: 9.87256908416748
Text: ; Smith, Warrington, and Linderman, Font: SFRM1000, Size: 9.87256908416748
Text: 2023, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully, Font: SFRM1000, Size: 9.892655372619629
Text: apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not, Font: SFRM1000, Size: 9.907693862915039
Text: stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the, Font: SFRM1000, Size: 9.977572441101074
Text: fused selective scan layer has the same memory requirements as an optimized transformer implementation with, Font: SFRM1000, Size: 10.007370948791504
Text: FlashAttention., Font: SFRM1000, Size: 9.962639808654785
Text: Details of the fused kernel and recomputation are in Appendix, Font: SFRM1000, Size: 9.937702178955078
Text:  D, Font: SFRM1000, Size: 9.937702178955078
Text: . The full Selective SSM layer and algorithm is, Font: SFRM1000, Size: 9.937702178955078
Text: illustrated in Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  1, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 3.4, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: A Simplifed SSM Architecture, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: As with structured SSMs, selective SSMs are standalone sequence transformations that can be ﬂexibly incorporated, Font: SFRM1000, Size: 9.862509727478027
Text: into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section, Font: SFRM1000, Size: 9.862509727478027
Text:  2, Font: SFRM1000, Size: 9.862509727478027
Text: ), which, Font: SFRM1000, Size: 9.862509727478027
Text: are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron), Font: SFRM1000, Size: 9.952672004699707
Text: block. We simplify this architecture by combining these two components into one, which is stacked homogenously, Font: SFRM1000, Size: 9.8876371383667
Text: (Figure, Font: SFRM1000, Size: 9.89767074584961
Text:  3, Font: SFRM1000, Size: 9.89767074584961
Text: ). This is inspired by the gated attention unit (GAU) (Hua et al., Font: SFRM1000, Size: 9.89767074584961
Text:  2022, Font: SFRM1000, Size: 9.89767074584961
Text: ), which did something similar for, Font: SFRM1000, Size: 9.89767074584961
Text: attention., Font: SFRM1000, Size: 9.962639808654785
Text: This architecture involves expanding the model dimension, Font: SFRM1000, Size: 10.061773300170898
Text:  퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text:  by a controllable expansion factor, Font: SFRM1000, Size: 10.061773300170898
Text:  퐸, Font: STIXTwoMath, Size: 9.962639808654785
Text: . For each, Font: SFRM1000, Size: 10.061773300170898
Text: block, most of the parameters (, Font: SFRM1000, Size: 10.037081718444824
Text: 3퐸퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text: ) are in the linear projections (, Font: SFRM1000, Size: 10.037081718444824
Text: 2퐸퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 10.037081718444824
Text: for input projections,, Font: SFRM1000, Size: 10.037081718444824
Text:  퐸퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 10.037081718444824
Text: for output, Font: SFRM1000, Size: 10.037081718444824
Text: projection) while the inner SSM contributes less., Font: SFRM1000, Size: 9.952672004699707
Text: The number of SSM parameters (projections for, Font: SFRM1000, Size: 9.952672004699707
Text:  ∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: , and, Font: SFRM1000, Size: 9.952672004699707
Text: 7, Font: STIXTwoText, Size: 9.962639808654785
Text: H3, Font: OpenSans-SemiBold, Size: 10.271998405456543
Text: Gated MLP, Font: OpenSans-SemiBold, Size: 10.271998405456543
Text: Mamba, Font: OpenSans-SemiBold, Size: 10.271998405456543
Text: Linear , Font: OpenSans-Regular, Size: 6.163199424743652
Text: projection, Font: OpenSans-Regular, Size: 6.163199424743652
Text: Sequence , Font: OpenSans-Regular, Size: 6.163199424743652
Text: transformation, Font: OpenSans-Regular, Size: 6.163199424743652
Text: Nonlinearity , Font: OpenSans-Regular, Size: 6.163199424743652
Text: (activation or , Font: OpenSans-Regular, Size: 6.163199424743652
Text: multiplication), Font: OpenSans-Regular, Size: 6.163199424743652
Text: X, Font: Muli-Light, Size: 9.244799613952637
Text: X, Font: Muli-Light, Size: 9.244799613952637
Text: X, Font: Muli-Light, Size: 9.244799613952637
Text: !, Font: CambriaMath, Size: 9.244799613952637
Text: X, Font: Muli-Light, Size: 9.244799613952637
Text: Conv, Font: OpenSansCondensed-Bold, Size: 8.217597961425781
Text: SSM, Font: OpenSansCondensed-Bold, Size: 8.217597961425781
Text: X, Font: Muli-Light, Size: 9.244799613952637
Text: !, Font: CambriaMath, Size: 9.244799613952637
Text: !, Font: CambriaMath, Size: 9.244799613952637
Text: Conv, Font: OpenSansCondensed-Bold, Size: 8.217597961425781
Text: SSM, Font: OpenSansCondensed-Bold, Size: 8.217597961425781
Text: ⨂, Font: CambriaMath, Size: 10.271998405456543
Text: Figure 3: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Architecture, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Our simplifed block design combines the H3 block, which is the basis of most SSM architectures, with, Font: STIXTwoText, Size: 8.96638011932373
Text: the ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block, Font: STIXTwoText, Size: 8.96638011932373
Text: homogenously. Compared to the H3 block, Mamba replaces the frst multiplicative gate with an activation function. Compared to, Font: STIXTwoText, Size: 8.96638011932373
Text: the MLP block, Mamba adds an SSM to the main branch. For, Font: STIXTwoText, Size: 8.96638011932373
Text:  휎, Font: STIXTwoMath, Size: 8.96638011932373
Text:  we use the SiLU / Swish activation (Hendrycks and Gimpel, Font: STIXTwoText, Size: 8.96638011932373
Text:  2016, Font: STIXTwoText, Size: 8.96638011932373
Text: ;, Font: STIXTwoText, Size: 8.96638011932373
Text: Ramachandran, Zoph, and Quoc V Le, Font: STIXTwoText, Size: 8.96638011932373
Text:  2017, Font: STIXTwoText, Size: 8.96638011932373
Text: )., Font: STIXTwoText, Size: 8.96638011932373
Text: the matrix, Font: SFRM1000, Size: 10.002410888671875
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ) are much smaller in comparison., Font: SFRM1000, Size: 10.002410888671875
Text: We repeat this block, interleaved with standard normalization, Font: SFRM1000, Size: 10.002410888671875
Text: and residual connections, to form the Mamba architecture. We always ﬁx to, Font: SFRM1000, Size: 9.862509727478027
Text:  퐸 = 2, Font: STIXTwoMath, Size: 9.962639808654785
Text:  in our experiments and use two, Font: SFRM1000, Size: 9.862509727478027
Text: stacks of the block to match the, Font: SFRM1000, Size: 9.867541313171387
Text:  12퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.867541313171387
Text: parameters of a Transformer’s interleaved MHA (multi-head attention) and, Font: SFRM1000, Size: 9.867541313171387
Text: MLP blocks., Font: SFRM1000, Size: 9.942694664001465
Text: We use the SiLU / Swish activation function (Hendrycks and Gimpel, Font: SFRM1000, Size: 9.942694664001465
Text:  2016, Font: SFRM1000, Size: 9.942694664001465
Text: ; Ramachandran, Zoph,, Font: SFRM1000, Size: 9.942694664001465
Text: and Quoc V Le, Font: SFRM1000, Size: 10.056839942932129
Text:  2017, Font: SFRM1000, Size: 10.056839942932129
Text: ), motivated so that the Gated MLP becomes the popular “SwiGLU” variant (Chowdhery, Font: SFRM1000, Size: 10.056839942932129
Text: et al., Font: SFRM1000, Size: 9.877593994140625
Text:  2023, Font: SFRM1000, Size: 9.877593994140625
Text: ; Shazeer, Font: SFRM1000, Size: 9.877593994140625
Text:  2020, Font: SFRM1000, Size: 9.877593994140625
Text: ; Touvron et al., Font: SFRM1000, Size: 9.877593994140625
Text:  2023, Font: SFRM1000, Size: 9.877593994140625
Text: )., Font: SFRM1000, Size: 9.877593994140625
Text: Finally, we additionally use an optional normalization layer (we, Font: SFRM1000, Size: 9.877593994140625
Text: choose LayerNorm (J. L. Ba, Kiros, and Hinton, Font: SFRM1000, Size: 9.87256908416748
Text:  2016, Font: SFRM1000, Size: 9.87256908416748
Text: )), motivated by RetNet’s usage of a normalization layer in a, Font: SFRM1000, Size: 9.87256908416748
Text: similar location (Y. Sun et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: 3.5, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Properties of Selection Mechanisms, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: The selection mechanism is a broader concept that can be applied in diﬀerent ways, such as to more traditional, Font: SFRM1000, Size: 9.992483139038086
Text: RNNs or CNNs, to diﬀerent parameters (e.g., Font: SFRM1000, Size: 9.957657814025879
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  in Algorithm, Font: SFRM1000, Size: 9.957657814025879
Text:  2, Font: SFRM1000, Size: 9.957657814025879
Text: ), or using diﬀerent transformations, Font: SFRM1000, Size: 9.957657814025879
Text:  푠(푥), Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.957657814025879
Text: 3.5.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Connection to Gating Mechanisms, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection, Font: SFRM1000, Size: 9.862509727478027
Text: mechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time, Font: SFRM1000, Size: 9.862509727478027
Text: systems is well established (Funahashi and Nakamura, Font: SFRM1000, Size: 10.061773300170898
Text:  1993, Font: SFRM1000, Size: 10.061773300170898
Text: ; Tallec and Ollivier, Font: SFRM1000, Size: 10.061773300170898
Text:  2018, Font: SFRM1000, Size: 10.061773300170898
Text: ). In fact, Theorem, Font: SFRM1000, Size: 10.061773300170898
Text:  1, Font: SFRM1000, Size: 10.061773300170898
Text:  is, Font: SFRM1000, Size: 10.061773300170898
Text: an improvement of Gu, Johnson, Goel, et al. (, Font: SFRM1000, Size: 10.061773300170898
Text: 2021, Font: SFRM1000, Size: 10.061773300170898
Text: , Lemma 3.1) generalizing to the ZOH discretization and, Font: SFRM1000, Size: 10.061773300170898
Text: input-dependent gates (proof in Appendix, Font: SFRM1000, Size: 10.061773300170898
Text:  C, Font: SFRM1000, Size: 10.061773300170898
Text: ). More broadly,, Font: SFRM1000, Size: 10.061773300170898
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  in SSMs can be seen to play a generalized role, Font: SFRM1000, Size: 10.061773300170898
Text: of the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the, Font: SFRM1000, Size: 10.061773300170898
Text: principled foundation of heuristic gating mechanisms., Font: SFRM1000, Size: 9.962639808654785
Text: Theorem 1., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  When, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  푁 = 1,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  = −1,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text:  = 1, 푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = 햫헂헇햾햺헋(푥), Font: STIXTwoMath, Size: 9.962639808654785
Text: , and, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  휏, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = 헌허햿헍헉헅헎헌, Font: STIXTwoMath, Size: 9.962639808654785
Text: , then the selective SSM recurrence, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: (Algorithm, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  2, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ) takes the form, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: 푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = 휎(햫헂헇햾햺헋(푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )), Font: STIXTwoMath, Size: 9.962639808654785
Text: ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = (1 − 푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡−1, Font: STIXTwoMath, Size: 7.471980094909668
Text:  + 푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: 푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: ., Font: STIXTwoMath, Size: 9.962639808654785
Text: (5), Font: STIXTwoText, Size: 9.962639808654785
Text: As mentioned in Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.2, Font: SFRM1000, Size: 10.061773300170898
Text: , our speciﬁc choices of, Font: SFRM1000, Size: 10.061773300170898
Text:  푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text: , 휏, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  is from this connection. In particular, note that if a, Font: SFRM1000, Size: 10.061773300170898
Text: given input, Font: SFRM1000, Size: 9.917706489562988
Text:  푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  should be completely ignored (as necessary in the synthetic tasks), all, Font: SFRM1000, Size: 9.917706489562988
Text:  퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text:  channels should ignore it,, Font: SFRM1000, Size: 9.917706489562988
Text: and so we project the input down to, Font: SFRM1000, Size: 9.962639808654785
Text:  1, Font: STIXTwoMath, Size: 9.962639808654785
Text:  dimension before repeating/broadcasting with, Font: SFRM1000, Size: 9.962639808654785
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 8, Font: STIXTwoText, Size: 9.962639808654785
Text: 3.5.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Interpretation of Selection Mechanisms, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We elaborate on two particular mechanistic eﬀects of selection., Font: SFRM1000, Size: 9.962639808654785
Text: Variable Spacing., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Selectivity allows ﬁltering out irrelevant noise tokens that may occur between inputs of, Font: SFRM1000, Size: 10.061773300170898
Text: interest. This is exempliﬁed by the Selective Copying task, but occurs ubiquitously in common data modalities,, Font: SFRM1000, Size: 10.002410888671875
Text: particularly for discrete data – for example the presence of language ﬁllers such as “um”. This property arises, Font: SFRM1000, Size: 10.061773300170898
Text: because the model can mechanistically ﬁlter out any particular input, Font: SFRM1000, Size: 10.061773300170898
Text:  푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: , for example in the gated RNN case, Font: SFRM1000, Size: 10.061773300170898
Text: (Theorem, Font: SFRM1000, Size: 9.962639808654785
Text:  1, Font: SFRM1000, Size: 9.962639808654785
Text: ) when, Font: SFRM1000, Size: 9.962639808654785
Text:  푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  → 0, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: Filtering Context., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: It has been empirically observed that many sequence models do not improve with longer, Font: SFRM1000, Size: 10.061773300170898
Text: context (F. Shi et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ), despite the principle that more context should lead to strictly better performance. An, Font: SFRM1000, Size: 9.862509727478027
Text: explanation is that many sequence models cannot eﬀectively ignore irrelevant context when necessary; an intuitive, Font: SFRM1000, Size: 9.862509727478027
Text: example are global convolutions (and general LTI models). On the other hand, selective models can simply reset, Font: SFRM1000, Size: 9.937702178955078
Text: their state at any time to remove extraneous history, and thus their performance in principle improves monotonicly, Font: SFRM1000, Size: 9.862509727478027
Text: with context length (e.g. Section, Font: SFRM1000, Size: 9.962639808654785
Text:  4.3.2, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Boundary Resetting., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: In settings where multiple independent sequences are stitched together, Transformers, Font: SFRM1000, Size: 10.061773300170898
Text: can keep them separate by instantiating a particular attention mask, while LTI models will bleed information, Font: SFRM1000, Size: 10.061773300170898
Text: between the sequences. Selective SSMs can also reset their state at boundaries (e.g., Font: SFRM1000, Size: 9.937702178955078
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  → ∞, Font: STIXTwoMath, Size: 9.962639808654785
Text:  or Theorem, Font: SFRM1000, Size: 9.937702178955078
Text:  1, Font: SFRM1000, Size: 9.937702178955078
Text:  when, Font: SFRM1000, Size: 9.937702178955078
Text: 푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  → 1, Font: STIXTwoMath, Size: 9.962639808654785
Text: ). These settings may occur artiﬁcially (e.g. packing documents together to improve hardware utilization), Font: SFRM1000, Size: 9.952672004699707
Text: or naturally (e.g. episode boundaries in reinforcement learning (Lu et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ))., Font: SFRM1000, Size: 9.962639808654785
Text: Additionally, we elaborate on eﬀects of each selective parameter., Font: SFRM1000, Size: 9.962639808654785
Text: Interpretation of, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: In general,, Font: SFRM1000, Size: 9.942694664001465
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  controls the balance between how much to focus or ignore the current input, Font: SFRM1000, Size: 9.942694664001465
Text: 푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: . It generalizes RNN gates (e.g., Font: SFRM1000, Size: 9.912701606750488
Text:  푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  in Theorem, Font: SFRM1000, Size: 9.912701606750488
Text:  1, Font: SFRM1000, Size: 9.912701606750488
Text: ), mechanically, a large, Font: SFRM1000, Size: 9.912701606750488
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  resets the state, Font: SFRM1000, Size: 9.912701606750488
Text:  ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and focuses on the, Font: SFRM1000, Size: 9.912701606750488
Text: current input, Font: SFRM1000, Size: 9.862509727478027
Text:  푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: , while a small, Font: SFRM1000, Size: 9.862509727478027
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  persists the state and ignores the current input. SSMs, Font: SFRM1000, Size: 9.862509727478027
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text: -, Font: SFRM1000, Size: 9.862509727478027
Text: (, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text:  can be interpreted as, Font: SFRM1000, Size: 9.862509727478027
Text: a continuous system discretized by a timestep, Font: SFRM1000, Size: 9.912701606750488
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: , and in this context the intuition is that large, Font: SFRM1000, Size: 9.912701606750488
Text:  ∆ → ∞, Font: STIXTwoMath, Size: 9.962639808654785
Text:  represents, Font: SFRM1000, Size: 9.912701606750488
Text: the system focusing on the current input for longer (thus “selecting” it and forgetting its current state) while a, Font: SFRM1000, Size: 10.042024612426758
Text: small, Font: SFRM1000, Size: 9.962639808654785
Text:  ∆ → 0, Font: STIXTwoMath, Size: 9.962639808654785
Text:  represents a transient input that is ignored., Font: SFRM1000, Size: 9.962639808654785
Text: Interpretation of, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We remark that while the, Font: SFRM1000, Size: 10.061773300170898
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  parameter could also be selective, it ultimately aﬀects the, Font: SFRM1000, Size: 10.061773300170898
Text: model only through its interaction with, Font: SFRM1000, Size: 10.061773300170898
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  via, Font: SFRM1000, Size: 10.061773300170898
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  = exp(∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  (the discretization, Font: SFRM1000, Size: 10.061773300170898
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 4, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text: ). Thus selectivity in, Font: SFRM1000, Size: 10.061773300170898
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  is, Font: SFRM1000, Size: 10.061773300170898
Text: enough to ensure selectivity in, Font: SFRM1000, Size: 10.027188301086426
Text:  (, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: , and is the main source of improvement. We hypothesize that making, Font: SFRM1000, Size: 10.027188301086426
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: selective in addition to (or instead of), Font: SFRM1000, Size: 9.962639808654785
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  would have similar performance, and leave it out for simplicity., Font: SFRM1000, Size: 9.962639808654785
Text: Interpretation of, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text:  and, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: As discussed in Section, Font: SFRM1000, Size: 9.977572441101074
Text:  3.1, Font: SFRM1000, Size: 9.977572441101074
Text: , the most important property of selectivity is ﬁltering, Font: SFRM1000, Size: 9.977572441101074
Text: out irrelevant information so that a sequence model’s context can be compressed into an eﬃcient state. In an SSM,, Font: SFRM1000, Size: 9.862509727478027
Text: modifying, Font: SFRM1000, Size: 9.862509727478027
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text:  and, Font: SFRM1000, Size: 9.862509727478027
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text:  to be selective allows ﬁner-grained control over whether to let an input, Font: SFRM1000, Size: 9.862509727478027
Text:  푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  into the state, Font: SFRM1000, Size: 9.862509727478027
Text:  ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  or, Font: SFRM1000, Size: 9.862509727478027
Text: the state into the output, Font: SFRM1000, Size: 9.912701606750488
Text:  푦, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: . These can be interpreted as allowing the model to modulate the recurrent dynamics, Font: SFRM1000, Size: 9.912701606750488
Text: based on content (input) and context (hidden states) respectively., Font: SFRM1000, Size: 9.962639808654785
Text: 3.6, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Additional Model Details, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Real vs. Complex., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Most prior SSMs use complex numbers in their state, Font: SFRM1000, Size: 10.061773300170898
Text:  ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: , which is necessary for strong, Font: SFRM1000, Size: 10.061773300170898
Text: performance on many tasks (Gu, Goel, and Ré, Font: SFRM1000, Size: 9.927709579467773
Text:  2022, Font: SFRM1000, Size: 9.927709579467773
Text: ). However, it has been empirically observed that completely, Font: SFRM1000, Size: 9.927709579467773
Text: real-valued SSMs seem to work ﬁne, and possibly even better, in some settings (Ma et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ). We use real, Font: SFRM1000, Size: 10.061773300170898
Text: values as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoﬀ is, Font: SFRM1000, Size: 9.862509727478027
Text: related to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous, Font: SFRM1000, Size: 9.882617950439453
Text: modalities (e.g. audio, video) but not discrete (e.g. text, DNA)., Font: SFRM1000, Size: 9.962639808654785
Text: 9, Font: STIXTwoText, Size: 9.962639808654785
Text: Initialization., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Most prior SSMs also suggest special initializations, particularly in the complex-valued case,, Font: SFRM1000, Size: 10.061773300170898
Text: which can help in several settings such as low-data regimes. Our default initialization for the complex case is, Font: SFRM1000, Size: 10.061773300170898
Text: S4D-Lin and for the real case is S4D-Real (Gu, Gupta, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2022, Font: SFRM1000, Size: 10.061773300170898
Text: ), which is based on the HIPPO theory (Gu,, Font: SFRM1000, Size: 10.061773300170898
Text: Dao, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2020, Font: SFRM1000, Size: 9.862509727478027
Text: ). These deﬁne the, Font: SFRM1000, Size: 9.862509727478027
Text:  푛, Font: STIXTwoMath, Size: 9.962639808654785
Text: -th element of, Font: SFRM1000, Size: 9.862509727478027
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  as, Font: SFRM1000, Size: 9.862509727478027
Text:  −1∕2 + 푛푖, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and, Font: SFRM1000, Size: 9.862509727478027
Text:  −(푛 + 1), Font: STIXTwoMath, Size: 9.962639808654785
Text:  respectively. However, we expect, Font: SFRM1000, Size: 9.862509727478027
Text: many initializations to work ﬁne, particularly in the large-data and real-valued SSM regimes; some ablations are, Font: SFRM1000, Size: 9.952672004699707
Text: considered in Section, Font: SFRM1000, Size: 9.962639808654785
Text:  4.6, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: Parameterization of, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We deﬁned the selective adjustment to, Font: SFRM1000, Size: 9.952672004699707
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  as, Font: SFRM1000, Size: 9.952672004699707
Text:  푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥) = 햡헋허햺햽햼햺헌헍, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐷, Font: STIXTwoMath, Size: 7.471980094909668
Text: (햫헂헇햾햺헋, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥)), Font: STIXTwoMath, Size: 9.962639808654785
Text: , which was, Font: SFRM1000, Size: 9.952672004699707
Text: motivated by the mechanics of, Font: SFRM1000, Size: 9.862509727478027
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  (Section, Font: SFRM1000, Size: 9.862509727478027
Text:  3.5, Font: SFRM1000, Size: 9.862509727478027
Text: ). We observe that it can be generalized from dimension, Font: SFRM1000, Size: 9.862509727478027
Text:  1, Font: STIXTwoMath, Size: 9.962639808654785
Text:  to a larger, Font: SFRM1000, Size: 9.862509727478027
Text: dimension, Font: SFRM1000, Size: 9.992483139038086
Text:  횁, Font: STIXTwoMath, Size: 9.962639808654785
Text: . We set this to be a small fraction of, Font: SFRM1000, Size: 9.992483139038086
Text:  홳, Font: STIXTwoMath, Size: 9.962639808654785
Text: , which uses a negligible number of parameters compared to, Font: SFRM1000, Size: 9.992483139038086
Text: the main Linear projections in the block. We additionally note that the broadcasting operation can instead be, Font: SFRM1000, Size: 10.042024612426758
Text: viewed as another Linear projection, initialized to a speciﬁc pattern of, Font: SFRM1000, Size: 9.977572441101074
Text:  1, Font: STIXTwoMath, Size: 9.962639808654785
Text: ’s and, Font: SFRM1000, Size: 9.977572441101074
Text:  0, Font: STIXTwoMath, Size: 9.962639808654785
Text: ’s; if this projection is trainable,, Font: SFRM1000, Size: 9.977572441101074
Text: this leads to the alternative, Font: SFRM1000, Size: 9.962639808654785
Text:  푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥) = 햫헂헇햾햺헋, Font: STIXTwoMath, Size: 9.962639808654785
Text: 퐷, Font: STIXTwoMath, Size: 7.471980094909668
Text: (햫헂헇햾햺헋, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푅, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥)), Font: STIXTwoMath, Size: 9.962639808654785
Text: , which can be viewed as a low-rank projection., Font: SFRM1000, Size: 9.962639808654785
Text: In our experiments, the, Font: SFRM1000, Size: 9.862509727478027
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  parameter (which can be viewed as a bias term) is initialized to, Font: SFRM1000, Size: 9.862509727478027
Text:  휏, Font: STIXTwoMath, Size: 9.962639808654785
Text: −1, Font: STIXTwoMath, Size: 7.471980094909668
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: (햴헇헂햿허헋헆([0.001, 0.1])), Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 9.862509727478027
Text: following prior work on SSMs (Gu, Johnson, Timalsina, et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Remark 3.1., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  For brevity in our experimental results, we sometimes abbreviate selective SSMs as, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  S6 models, Font: STIXTwoText, Size: 9.962639808654785
Text: , because they, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: are S4 models with a, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  selection, Font: STIXTwoText, Size: 9.962639808654785
Text:  mechanism and computed with a, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  scan, Font: STIXTwoText, Size: 9.962639808654785
Text: ., Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: 4, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Empirical Evaluation, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: In Section, Font: SFRM1000, Size: 9.862509727478027
Text:  4.1, Font: SFRM1000, Size: 9.862509727478027
Text:  we test Mamba’s ability to solve the two synthetic tasks motivated in Section, Font: SFRM1000, Size: 9.862509727478027
Text:  3.1, Font: SFRM1000, Size: 9.862509727478027
Text: . We then evaluate, Font: SFRM1000, Size: 9.862509727478027
Text: on three domains, each evaluated on autoregressive pretraining as well as downstream tasks., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Section, Font: SFRM1000, Size: 9.962639808654785
Text:  4.2, Font: SFRM1000, Size: 9.962639808654785
Text: : language model pretraining (scaling laws), and zero-shot downstream evaluation., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Section, Font: SFRM1000, Size: 9.962639808654785
Text:  4.3, Font: SFRM1000, Size: 9.962639808654785
Text: : DNA sequence pretraining, and ﬁne-tuning on a long-sequence classiﬁcation task., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Section, Font: SFRM1000, Size: 9.962639808654785
Text:  4.4, Font: SFRM1000, Size: 9.962639808654785
Text: : audio waveform pretraining, and the quality of autoregressively generated speech clips., Font: SFRM1000, Size: 9.962639808654785
Text: Finally, Section, Font: SFRM1000, Size: 9.917706489562988
Text:  4.5, Font: SFRM1000, Size: 9.917706489562988
Text:  shows Mamba’s computational eﬃciency at both training and inference time, and Section, Font: SFRM1000, Size: 9.917706489562988
Text:  4.6, Font: SFRM1000, Size: 9.917706489562988
Text: ablates various components of the architecture and selective SSMs., Font: SFRM1000, Size: 9.962639808654785
Text: 4.1, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Synthetic Tasks, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Full experiment details for these tasks including task details and training protocol are in Appendix, Font: SFRM1000, Size: 9.962639808654785
Text:  E.1, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 4.1.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Selective Copying, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: The Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test, Font: SFRM1000, Size: 9.862509727478027
Text: the memorization abilities of recurrent models. As discussed in Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.1, Font: SFRM1000, Size: 10.061773300170898
Text: , LTI SSMs (linear recurrences and, Font: SFRM1000, Size: 10.061773300170898
Text: global convolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for, Font: SFRM1000, Size: 9.862509727478027
Text: example, by constructing a convolution kernel of exactly the right length (Figure, Font: SFRM1000, Size: 9.877593994140625
Text:  2, Font: SFRM1000, Size: 9.877593994140625
Text: ). This was explicitly validated, Font: SFRM1000, Size: 9.877593994140625
Text: in earlier work on global convolutions (Romero et al., Font: SFRM1000, Size: 10.05190372467041
Text:  2021, Font: SFRM1000, Size: 10.05190372467041
Text: ). The Selective Copying task prevents this shortcut, Font: SFRM1000, Size: 10.05190372467041
Text: by randomizing the spacing between tokens. Note that this task has been introduced before as the Denoising, Font: SFRM1000, Size: 10.061773300170898
Text: task (Jing et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2019, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Note that many previous works argue that adding architecture gating (multiplicative interactions) can endow, Font: SFRM1000, Size: 10.061773300170898
Text: models with “data-dependence” and solve related tasks (Dao, Fu, Saab, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ; Poli et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ). However,, Font: SFRM1000, Size: 10.061773300170898
Text: we ﬁnd this explanation insuﬃcient intuitively because such gating does not interact along the sequence axis,, Font: SFRM1000, Size: 10.061773300170898
Text: and cannot aﬀect the spacing between tokens. In particular architecture gating is not an instance of a selection, Font: SFRM1000, Size: 10.007370948791504
Text: mechanism (Appendix, Font: SFRM1000, Size: 9.962639808654785
Text:  A, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 9.977572441101074
Text:  1, Font: SFRM1000, Size: 9.977572441101074
Text:  conﬁrms that gated architectures such as H3 and Mamba only partially improve performance, while the, Font: SFRM1000, Size: 9.977572441101074
Text: selection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more, Font: SFRM1000, Size: 10.061773300170898
Text: powerful architectures., Font: SFRM1000, Size: 9.962639808654785
Text: 10, Font: STIXTwoText, Size: 9.962639808654785
Text: Model, Font: STIXTwoText, Size: 8.96638011932373
Text: Arch., Font: STIXTwoText, Size: 8.96638011932373
Text: Layer, Font: STIXTwoText, Size: 8.96638011932373
Text: Acc., Font: STIXTwoText, Size: 8.96638011932373
Text: S4, Font: STIXTwoText, Size: 8.96638011932373
Text: No gate, Font: STIXTwoText, Size: 8.96638011932373
Text: S4, Font: STIXTwoText, Size: 8.96638011932373
Text: 18.3, Font: STIXTwoText, Size: 8.96638011932373
Text: -, Font: STIXTwoText, Size: 8.96638011932373
Text: No gate, Font: STIXTwoText, Size: 8.96638011932373
Text: S6, Font: STIXTwoText, Size: 8.96638011932373
Text: 97.0, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: H3, Font: STIXTwoText, Size: 8.96638011932373
Text: H3, Font: STIXTwoText, Size: 8.96638011932373
Text: S4, Font: STIXTwoText, Size: 8.96638011932373
Text: 57.0, Font: STIXTwoText, Size: 8.96638011932373
Text: Hyena, Font: STIXTwoText, Size: 8.96638011932373
Text: H3, Font: STIXTwoText, Size: 8.96638011932373
Text: Hyena, Font: STIXTwoText, Size: 8.96638011932373
Text: 30.1, Font: STIXTwoText, Size: 8.96638011932373
Text: -, Font: STIXTwoText, Size: 8.96638011932373
Text: H3, Font: STIXTwoText, Size: 8.96638011932373
Text: S6, Font: STIXTwoText, Size: 8.96638011932373
Text: 99.7, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: -, Font: STIXTwoText, Size: 8.96638011932373
Text: Mamba, Font: STIXTwoText, Size: 8.96638011932373
Text: S4, Font: STIXTwoText, Size: 8.96638011932373
Text: 56.4, Font: STIXTwoText, Size: 8.96638011932373
Text: -, Font: STIXTwoText, Size: 8.96638011932373
Text: Mamba, Font: STIXTwoText, Size: 8.96638011932373
Text: Hyena, Font: STIXTwoText, Size: 8.96638011932373
Text: 28.4, Font: STIXTwoText, Size: 8.96638011932373
Text: Mamba, Font: STIXTwoText, Size: 8.96638011932373
Text: Mamba, Font: STIXTwoText, Size: 8.96638011932373
Text: S6, Font: STIXTwoText, Size: 8.96638011932373
Text: 99.8, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: Table 1: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Selective Copying, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .), Font: STIXTwoText, Size: 8.96638011932373
Text: Accuracy for combinations of architectures, Font: STIXTwoText, Size: 8.96638011932373
Text: and inner sequence layers., Font: STIXTwoText, Size: 8.96638011932373
Text: 10, Font: Roboto-Bold, Size: 5.499120235443115
Text: 2, Font: Roboto-Bold, Size: 3.849384069442749
Text: 10, Font: Roboto-Bold, Size: 5.499120235443115
Text: 3, Font: Roboto-Bold, Size: 3.849384069442749
Text: 10, Font: Roboto-Bold, Size: 5.499120235443115
Text: 4, Font: Roboto-Bold, Size: 3.849384069442749
Text: 10, Font: Roboto-Bold, Size: 5.499120235443115
Text: 5, Font: Roboto-Bold, Size: 3.849384069442749
Text: 10, Font: Roboto-Bold, Size: 5.499120235443115
Text: 6, Font: Roboto-Bold, Size: 3.849384069442749
Text: Test Sequence Length, Font: Roboto-Bold, Size: 6.998880386352539
Text: 0.0, Font: Roboto-Bold, Size: 5.499120235443115
Text: 0.2, Font: Roboto-Bold, Size: 5.499120235443115
Text: 0.4, Font: Roboto-Bold, Size: 5.499120235443115
Text: 0.6, Font: Roboto-Bold, Size: 5.499120235443115
Text: 0.8, Font: Roboto-Bold, Size: 5.499120235443115
Text: 1.0, Font: Roboto-Bold, Size: 5.499120235443115
Text: Accuracy, Font: Roboto-Bold, Size: 6.998880386352539
Text: Induction Heads Extrapolation, Font: Roboto-Bold, Size: 6.998880386352539
Text: MHA-Absolute, Font: Roboto-Bold, Size: 5.499120235443115
Text: MHA-RoPE, Font: Roboto-Bold, Size: 5.499120235443115
Text: MHA-xPos, Font: Roboto-Bold, Size: 5.499120235443115
Text: H3, Font: Roboto-Bold, Size: 5.499120235443115
Text: Hyena, Font: Roboto-Bold, Size: 5.499120235443115
Text: Mamba, Font: Roboto-Bold, Size: 5.499120235443115
Text: Random, Font: Roboto-Bold, Size: 5.499120235443115
Text: Train Length, Font: Roboto-Bold, Size: 5.499120235443115
Text: Table 2: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Induction Heads, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Models are trained on sequence length, Font: STIXTwoText, Size: 8.96638011932373
Text: 2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 8, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 256, Font: STIXTwoMath, Size: 8.96638011932373
Text: , and tested on increasing sequence lengths of, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 6, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 64, Font: STIXTwoMath, Size: 8.96638011932373
Text:  up to, Font: STIXTwoText, Size: 8.96638011932373
Text: 2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 20, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 1048576, Font: STIXTwoMath, Size: 8.96638011932373
Text: . Full numbers in Table, Font: STIXTwoText, Size: 8.96638011932373
Text:  11, Font: STIXTwoText, Size: 8.96638011932373
Text: ., Font: STIXTwoText, Size: 8.96638011932373
Text: 4.1.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Induction Heads, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Induction heads (Olsson et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ) is a simple task from the mechanistic interpretability lens (Elhage et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2021, Font: SFRM1000, Size: 9.862509727478027
Text: ), Font: SFRM1000, Size: 9.862509727478027
Text: that is surprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative, Font: SFRM1000, Size: 9.862509727478027
Text: recall and copy: for example, if the model has seen a bigram such as “Harry Potter” in the sequence, then the, Font: SFRM1000, Size: 10.061773300170898
Text: next time “Harry” appears in the same sequence, the model should be able to predict “Potter” by copying from, Font: SFRM1000, Size: 10.032135963439941
Text: history., Font: SFRM1000, Size: 9.962639808654785
Text: Dataset., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We train a 2-layer model on the induction heads task at sequence length, Font: SFRM1000, Size: 10.061773300170898
Text:  256, Font: STIXTwoMath, Size: 9.962639808654785
Text: , with a vocab size of, Font: SFRM1000, Size: 10.061773300170898
Text: 16, Font: STIXTwoMath, Size: 9.962639808654785
Text: , which is comparable to prior work on this task (Dao, Fu, Saab, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ) but with longer sequences. We, Font: SFRM1000, Size: 10.061773300170898
Text: additionally investigate generalization and extrapolation abilities by evaluating on a range of sequence lengths, Font: SFRM1000, Size: 10.061773300170898
Text: from, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 6, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 64, Font: STIXTwoMath, Size: 9.962639808654785
Text:  up to, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 1048576, Font: STIXTwoMath, Size: 9.962639808654785
Text:  at test time., Font: SFRM1000, Size: 9.962639808654785
Text: Models., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Following established work on induction heads, we use 2 layer models, which allows attention to, Font: SFRM1000, Size: 10.061773300170898
Text: mechanistically solve the induction heads task (Olsson et al., Font: SFRM1000, Size: 9.977572441101074
Text:  2022, Font: SFRM1000, Size: 9.977572441101074
Text: ). We test both multi-head attention (8 heads,, Font: SFRM1000, Size: 9.977572441101074
Text: with various positional encodings) and SSM variants. We use a model dimension, Font: SFRM1000, Size: 9.942694664001465
Text:  퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text:  of, Font: SFRM1000, Size: 9.942694664001465
Text:  64, Font: STIXTwoMath, Size: 9.962639808654785
Text:  for Mamba and, Font: SFRM1000, Size: 9.942694664001465
Text:  128, Font: STIXTwoMath, Size: 9.962639808654785
Text:  for, Font: SFRM1000, Size: 9.942694664001465
Text: the other models., Font: SFRM1000, Size: 9.962639808654785
Text: Results., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: SFRM1000, Size: 10.061773300170898
Text:  shows that Mamba—or more precisely, its selective SSM layer—has the ability to solve the, Font: SFRM1000, Size: 10.061773300170898
Text: task perfectly because of its ability to selectively remember the relevant token while ignoring everything else in, Font: SFRM1000, Size: 10.022237777709961
Text: between. It generalizes perfectly to million-length sequences, or, Font: SFRM1000, Size: 9.862509727478027
Text:  4000×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  longer than it saw during training, while no, Font: SFRM1000, Size: 9.862509727478027
Text: other method goes beyond, Font: SFRM1000, Size: 9.962639808654785
Text:  2×, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: Out of positional encoding variants for attention models, xPos (which was designed for length extrapolation), Font: SFRM1000, Size: 10.061773300170898
Text: is slightly better than the others; also note that all attention models were only tested up to sequence length, Font: SFRM1000, Size: 10.061773300170898
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 14, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 16384, Font: STIXTwoMath, Size: 9.962639808654785
Text:  due to memory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the ﬁndings in, Font: SFRM1000, Size: 9.862509727478027
Text: Poli et al. (, Font: SFRM1000, Size: 9.962639808654785
Text: 2023, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: 4.2, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Language Modeling, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: We evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on, Font: SFRM1000, Size: 9.862509727478027
Text: both pretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to, Font: SFRM1000, Size: 10.061773300170898
Text: mirror GPT3 speciﬁcations. We use the Pile dataset (L. Gao, Biderman, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2020, Font: SFRM1000, Size: 10.061773300170898
Text: ), and follow the training, Font: SFRM1000, Size: 10.061773300170898
Text: recipe described in Brown et al. (, Font: SFRM1000, Size: 9.962639808654785
Text: 2020, Font: SFRM1000, Size: 9.962639808654785
Text: ). All training details are in Appendix, Font: SFRM1000, Size: 9.962639808654785
Text:  E.2, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 4.2.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Scaling Laws, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: For baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the, Font: SFRM1000, Size: 10.061773300170898
Text: strongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa, Font: SFRM1000, Size: 10.007370948791504
Text: 11, Font: STIXTwoText, Size: 9.962639808654785
Text: 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 19, Font: Roboto-Bold, Size: 3.42264986038208
Text: 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 20, Font: Roboto-Bold, Size: 3.42264986038208
Text: FLOPs (log scale), Font: Roboto-Bold, Size: 6.2230000495910645
Text: 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 1, Font: Roboto-Bold, Size: 3.42264986038208
Text: 6 × 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 0, Font: Roboto-Bold, Size: 3.42264986038208
Text: 2 × 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 1, Font: Roboto-Bold, Size: 3.42264986038208
Text: Perplexity (log scale), Font: Roboto-Bold, Size: 6.2230000495910645
Text: Scaling Laws on The Pile (Sequence Length 2048), Font: Roboto-Bold, Size: 6.2230000495910645
Text: Hyena, Font: Roboto-Bold, Size: 4.889500141143799
Text: RWKV, Font: Roboto-Bold, Size: 4.889500141143799
Text: Transformer, Font: Roboto-Bold, Size: 4.889500141143799
Text: RetNet, Font: Roboto-Bold, Size: 4.889500141143799
Text: H3++, Font: Roboto-Bold, Size: 4.889500141143799
Text: Transformer++, Font: Roboto-Bold, Size: 4.889500141143799
Text: Mamba, Font: Roboto-Bold, Size: 4.889500141143799
Text: 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 19, Font: Roboto-Bold, Size: 3.42264986038208
Text: 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 20, Font: Roboto-Bold, Size: 3.42264986038208
Text: FLOPs (log scale), Font: Roboto-Bold, Size: 6.2230000495910645
Text: 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 1, Font: Roboto-Bold, Size: 3.42264986038208
Text: 6 × 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 0, Font: Roboto-Bold, Size: 3.42264986038208
Text: 2 × 10, Font: Roboto-Bold, Size: 4.889500141143799
Text: 1, Font: Roboto-Bold, Size: 3.42264986038208
Text: Perplexity (log scale), Font: Roboto-Bold, Size: 6.2230000495910645
Text: Scaling Laws on The Pile (Sequence Length 8192), Font: Roboto-Bold, Size: 6.2230000495910645
Text: Hyena, Font: Roboto-Bold, Size: 4.889500141143799
Text: RWKV, Font: Roboto-Bold, Size: 4.889500141143799
Text: Transformer, Font: Roboto-Bold, Size: 4.889500141143799
Text: RetNet, Font: Roboto-Bold, Size: 4.889500141143799
Text: H3++, Font: Roboto-Bold, Size: 4.889500141143799
Text: Transformer++, Font: Roboto-Bold, Size: 4.889500141143799
Text: Mamba, Font: Roboto-Bold, Size: 4.889500141143799
Text: Figure 4: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Scaling Laws, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Models of size, Font: STIXTwoText, Size: 8.96638011932373
Text:  ≈ 125푀, Font: STIXTwoMath, Size: 8.96638011932373
Text:  to, Font: STIXTwoText, Size: 8.96638011932373
Text:  ≈ 1.3퐵, Font: STIXTwoMath, Size: 8.96638011932373
Text:  parameters, trained on the Pile. Mamba scales better than all other, Font: STIXTwoText, Size: 8.96638011932373
Text: attention-free models and is the frst to match the performance of a very strong “Transformer++” recipe that has now become, Font: STIXTwoText, Size: 8.96638011932373
Text: standard, particularly as the sequence length grows., Font: STIXTwoText, Size: 8.96638011932373
Text: architectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher, Font: SFRM1000, Size: 9.912701606750488
Text: learning rates). We also compare against other recent subquadratic architectures (Figure, Font: SFRM1000, Size: 9.862509727478027
Text:  4, Font: SFRM1000, Size: 9.862509727478027
Text: ). All model details are, Font: SFRM1000, Size: 9.862509727478027
Text: in Appendix, Font: SFRM1000, Size: 9.962639808654785
Text:  E.2, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: Figure, Font: SFRM1000, Size: 10.061773300170898
Text:  4, Font: SFRM1000, Size: 10.061773300170898
Text:  shows scaling laws under the standard Chinchilla (Hoﬀmann et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2022, Font: SFRM1000, Size: 10.061773300170898
Text: ) protocol, on models from, Font: SFRM1000, Size: 10.061773300170898
Text: ≈ 125푀, Font: STIXTwoMath, Size: 9.962639808654785
Text:  to, Font: SFRM1000, Size: 10.061773300170898
Text:  ≈ 1.3퐵, Font: STIXTwoMath, Size: 9.962639808654785
Text:  parameters. Mamba is the ﬁrst attention-free model to match the performance of a very, Font: SFRM1000, Size: 10.061773300170898
Text: strong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length, Font: SFRM1000, Size: 9.982544898986816
Text: grows. We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior, Font: SFRM1000, Size: 10.061773300170898
Text: strong recurrent models that can also be interpreted as SSMs, due to a lack of eﬃcient implementation leading to, Font: SFRM1000, Size: 9.8876371383667
Text: out-of-memory or unrealistic computation requirements., Font: SFRM1000, Size: 9.962639808654785
Text: 4.2.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Downstream Evaluations, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 10.061773300170898
Text:  3, Font: SFRM1000, Size: 10.061773300170898
Text:  shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We, Font: SFRM1000, Size: 10.061773300170898
Text: compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al., Font: SFRM1000, Size: 9.862509727478027
Text: 2023, Font: SFRM1000, Size: 9.917706489562988
Text: ) and RWKV (B. Peng et al., Font: SFRM1000, Size: 9.917706489562988
Text:  2023, Font: SFRM1000, Size: 9.917706489562988
Text: ) which were trained with the same tokenizer, dataset, and training length, Font: SFRM1000, Size: 9.917706489562988
Text: (300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV, Font: SFRM1000, Size: 9.942694664001465
Text: was trained with context length 1024.), Font: SFRM1000, Size: 9.962639808654785
Text: 4.3, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: DNA Modeling, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Motivated by the success of large language models, there has been recent exploration into using the foundation, Font: SFRM1000, Size: 10.017284393310547
Text: model paradigm for genomics. DNA has been likened to language in that it consists of sequences of discrete, Font: SFRM1000, Size: 10.061773300170898
Text: tokens with a ﬁnite vocab. It is also known for requiring long-range dependencies to model (Avsec et al., Font: SFRM1000, Size: 10.05190372467041
Text:  2021, Font: SFRM1000, Size: 10.05190372467041
Text: )., Font: SFRM1000, Size: 10.05190372467041
Text: We investigate Mamba as a FM backbone for pretraining and ﬁne-tuning in the same setting as recent works on, Font: SFRM1000, Size: 9.967619895935059
Text: long-sequence models for DNA (Nguyen, Poli, et al., Font: SFRM1000, Size: 9.947684288024902
Text:  2023, Font: SFRM1000, Size: 9.947684288024902
Text: ). In particular, we focus on two explorations of scaling, Font: SFRM1000, Size: 9.947684288024902
Text: laws across model size and sequence length (Figure, Font: SFRM1000, Size: 10.061773300170898
Text:  5, Font: SFRM1000, Size: 10.061773300170898
Text: ), and a diﬃcult downstream synthetic classiﬁcation task, Font: SFRM1000, Size: 10.061773300170898
Text: requiring long context (Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  6, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: For pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training, Font: SFRM1000, Size: 9.862509727478027
Text: and model details (see also Appendix, Font: SFRM1000, Size: 9.992483139038086
Text:  E.2, Font: SFRM1000, Size: 9.992483139038086
Text: ). For the dataset, we largely follow the setup of HyenaDNA (Nguyen,, Font: SFRM1000, Size: 9.992483139038086
Text: Poli, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ), which uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5, Font: SFRM1000, Size: 9.862509727478027
Text: billion tokens (DNA base pairs) in the training split., Font: SFRM1000, Size: 9.962639808654785
Text: 4.3.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Scaling: Model Size, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: In this experiment, we investigate the scaling properties of genomics foundation models with various model, Font: SFRM1000, Size: 10.061773300170898
Text: backbones (Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  5, Font: SFRM1000, Size: 9.962639808654785
Text:  Left)., Font: SFRM1000, Size: 9.962639808654785
Text: Training., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: To advantage the baselines, we train on a short sequence length of, Font: SFRM1000, Size: 9.862509727478027
Text:  1024, Font: STIXTwoMath, Size: 9.962639808654785
Text: ; as shown in Section, Font: SFRM1000, Size: 9.862509727478027
Text:  4.3.2, Font: SFRM1000, Size: 9.862509727478027
Text: , we, Font: SFRM1000, Size: 9.862509727478027
Text: expect results to favor Mamba even more at longer sequence lengths. We ﬁx a global batch size of, Font: SFRM1000, Size: 10.061773300170898
Text:  1024, Font: STIXTwoMath, Size: 9.962639808654785
Text: , for a, Font: SFRM1000, Size: 10.061773300170898
Text: 12, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 3: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Zero-shot Evaluations, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Best results for each size in bold. We compare against open source LMs with various tokenizers,, Font: STIXTwoText, Size: 8.96638011932373
Text: trained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and, Font: STIXTwoText, Size: 8.96638011932373
Text: tokenizer (GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches, Font: STIXTwoText, Size: 8.96638011932373
Text: baselines at twice the model size., Font: STIXTwoText, Size: 8.96638011932373
Text: Model, Font: STIXTwoText, Size: 8.555720329284668
Text: Token., Font: STIXTwoText, Size: 8.555720329284668
Text: Pile, Font: STIXTwoText, Size: 8.555720329284668
Text: LAMBADA, Font: STIXTwoText, Size: 8.555720329284668
Text: LAMBADA, Font: STIXTwoText, Size: 8.555720329284668
Text: HellaSwag, Font: STIXTwoText, Size: 8.555720329284668
Text: PIQA, Font: STIXTwoText, Size: 8.555720329284668
Text: Arc-E, Font: STIXTwoText, Size: 8.555720329284668
Text: Arc-C, Font: STIXTwoText, Size: 8.555720329284668
Text: WinoGrande, Font: STIXTwoText, Size: 8.555720329284668
Text: Average, Font: STIXTwoText, Size: 8.555720329284668
Text: ppl, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↓, Font: STIXTwoMath, Size: 8.555720329284668
Text: ppl, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↓, Font: STIXTwoMath, Size: 8.555720329284668
Text: acc, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↑, Font: STIXTwoMath, Size: 8.555720329284668
Text: acc, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↑, Font: STIXTwoMath, Size: 8.555720329284668
Text: acc, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↑, Font: STIXTwoMath, Size: 8.555720329284668
Text: acc, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↑, Font: STIXTwoMath, Size: 8.555720329284668
Text: acc, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↑, Font: STIXTwoMath, Size: 8.555720329284668
Text: acc, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↑, Font: STIXTwoMath, Size: 8.555720329284668
Text: acc, Font: STIXTwoText, Size: 8.555720329284668
Text:  ↑, Font: STIXTwoMath, Size: 8.555720329284668
Text: Hybrid H3-130M, Font: STIXTwoText, Size: 8.555720329284668
Text: GPT2, Font: STIXTwoText, Size: 8.555720329284668
Text: —, Font: STIXTwoText, Size: 8.555720329284668
Text: 89.48, Font: STIXTwoText, Size: 8.555720329284668
Text: 25.77, Font: STIXTwoText, Size: 8.555720329284668
Text: 31.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 64.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 44.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 24.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 50.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 40.1, Font: STIXTwoText, Size: 8.555720329284668
Text: Pythia-160M, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 29.64, Font: STIXTwoText, Size: 8.555720329284668
Text: 38.10, Font: STIXTwoText, Size: 8.555720329284668
Text: 33.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 30.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 61.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 43.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 24.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 51.9, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 40.6, Font: STIXTwoText, Size: 8.555720329284668
Text: Mamba-130M, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 10.56, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 16.07, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 44.3, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 35.3, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 64.5, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 48.0, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 24.3, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 51.9, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 44.7, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: Hybrid H3-360M, Font: STIXTwoText, Size: 8.555720329284668
Text: GPT2, Font: STIXTwoText, Size: 8.555720329284668
Text: —, Font: STIXTwoText, Size: 8.555720329284668
Text: 12.58, Font: STIXTwoText, Size: 8.555720329284668
Text: 48.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 41.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 68.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 51.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 24.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 54.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 48.0, Font: STIXTwoText, Size: 8.555720329284668
Text: Pythia-410M, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 9.95, Font: STIXTwoText, Size: 8.555720329284668
Text: 10.84, Font: STIXTwoText, Size: 8.555720329284668
Text: 51.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 40.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 66.9, Font: STIXTwoText, Size: 8.555720329284668
Text: 52.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 24.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 53.8, Font: STIXTwoText, Size: 8.555720329284668
Text: 48.2, Font: STIXTwoText, Size: 8.555720329284668
Text: Mamba-370M, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 8.28, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 8.14, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 55.6, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 46.5, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 69.5, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 55.1, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 28.0, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 55.3, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 50.0, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: Pythia-1B, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.82, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.92, Font: STIXTwoText, Size: 8.555720329284668
Text: 56.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 47.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 70.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 57.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 27.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 53.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 51.9, Font: STIXTwoText, Size: 8.555720329284668
Text: Mamba-790M, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.33, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 6.02, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 62.7, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 55.1, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 72.1, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 61.2, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 29.5, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 56.1, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 57.1, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: GPT-Neo 1.3B, Font: STIXTwoText, Size: 8.555720329284668
Text: GPT2, Font: STIXTwoText, Size: 8.555720329284668
Text: —, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.50, Font: STIXTwoText, Size: 8.555720329284668
Text: 57.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 48.9, Font: STIXTwoText, Size: 8.555720329284668
Text: 71.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 56.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 25.9, Font: STIXTwoText, Size: 8.555720329284668
Text: 54.9, Font: STIXTwoText, Size: 8.555720329284668
Text: 52.4, Font: STIXTwoText, Size: 8.555720329284668
Text: Hybrid H3-1.3B, Font: STIXTwoText, Size: 8.555720329284668
Text: GPT2, Font: STIXTwoText, Size: 8.555720329284668
Text: —, Font: STIXTwoText, Size: 8.555720329284668
Text: 11.25, Font: STIXTwoText, Size: 8.555720329284668
Text: 49.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 52.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 71.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 28.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 56.9, Font: STIXTwoText, Size: 8.555720329284668
Text: 53.0, Font: STIXTwoText, Size: 8.555720329284668
Text: OPT-1.3B, Font: STIXTwoText, Size: 8.555720329284668
Text: OPT, Font: STIXTwoText, Size: 8.555720329284668
Text: —, Font: STIXTwoText, Size: 8.555720329284668
Text: 6.64, Font: STIXTwoText, Size: 8.555720329284668
Text: 58.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 53.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 72.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 56.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 29.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 55.0, Font: STIXTwoText, Size: 8.555720329284668
Text: Pythia-1.4B, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.51, Font: STIXTwoText, Size: 8.555720329284668
Text: 6.08, Font: STIXTwoText, Size: 8.555720329284668
Text: 61.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 52.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 71.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 60.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 28.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 57.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 55.2, Font: STIXTwoText, Size: 8.555720329284668
Text: RWKV-1.5B, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.70, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.04, Font: STIXTwoText, Size: 8.555720329284668
Text: 56.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 52.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 72.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 60.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 29.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 54.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 54.3, Font: STIXTwoText, Size: 8.555720329284668
Text: Mamba-1.4B, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 6.80, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 5.04, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 64.9, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 59.1, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 74.2, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 65.5, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 32.8, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 61.5, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 59.7, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: GPT-Neo 2.7B, Font: STIXTwoText, Size: 8.555720329284668
Text: GPT2, Font: STIXTwoText, Size: 8.555720329284668
Text: —, Font: STIXTwoText, Size: 8.555720329284668
Text: 5.63, Font: STIXTwoText, Size: 8.555720329284668
Text: 62.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 55.8, Font: STIXTwoText, Size: 8.555720329284668
Text: 72.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 61.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 30.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 57.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 56.5, Font: STIXTwoText, Size: 8.555720329284668
Text: Hybrid H3-2.7B, Font: STIXTwoText, Size: 8.555720329284668
Text: GPT2, Font: STIXTwoText, Size: 8.555720329284668
Text: —, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.92, Font: STIXTwoText, Size: 8.555720329284668
Text: 55.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 73.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 65.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 32.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 61.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 58.0, Font: STIXTwoText, Size: 8.555720329284668
Text: OPT-2.7B, Font: STIXTwoText, Size: 8.555720329284668
Text: OPT, Font: STIXTwoText, Size: 8.555720329284668
Text: —, Font: STIXTwoText, Size: 8.555720329284668
Text: 5.12, Font: STIXTwoText, Size: 8.555720329284668
Text: 63.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 60.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 74.8, Font: STIXTwoText, Size: 8.555720329284668
Text: 60.8, Font: STIXTwoText, Size: 8.555720329284668
Text: 31.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 61.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 58.7, Font: STIXTwoText, Size: 8.555720329284668
Text: Pythia-2.8B, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 6.73, Font: STIXTwoText, Size: 8.555720329284668
Text: 5.04, Font: STIXTwoText, Size: 8.555720329284668
Text: 64.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 74.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 64.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 32.9, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.1, Font: STIXTwoText, Size: 8.555720329284668
Text: RWKV-3B, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 7.00, Font: STIXTwoText, Size: 8.555720329284668
Text: 5.24, Font: STIXTwoText, Size: 8.555720329284668
Text: 63.9, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 73.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 67.8, Font: STIXTwoText, Size: 8.555720329284668
Text: 33.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 59.6, Font: STIXTwoText, Size: 8.555720329284668
Text: Mamba-2.8B, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 6.22, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 4.23, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 69.2, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 66.1, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 75.2, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 69.7, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 36.3, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 63.5, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: 63.3, Font: STIXTwoText-Bold, Size: 8.555720329284668
Text: GPT-J-6B, Font: STIXTwoText, Size: 8.555720329284668
Text: GPT2, Font: STIXTwoText, Size: 8.555720329284668
Text: –, Font: STIXTwoText, Size: 8.555720329284668
Text: 4.10, Font: STIXTwoText, Size: 8.555720329284668
Text: 68.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 66.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 75.4, Font: STIXTwoText, Size: 8.555720329284668
Text: 67.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 36.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 64.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 63.0, Font: STIXTwoText, Size: 8.555720329284668
Text: OPT-6.7B, Font: STIXTwoText, Size: 8.555720329284668
Text: OPT, Font: STIXTwoText, Size: 8.555720329284668
Text: –, Font: STIXTwoText, Size: 8.555720329284668
Text: 4.25, Font: STIXTwoText, Size: 8.555720329284668
Text: 67.7, Font: STIXTwoText, Size: 8.555720329284668
Text: 67.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 76.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 65.6, Font: STIXTwoText, Size: 8.555720329284668
Text: 34.9, Font: STIXTwoText, Size: 8.555720329284668
Text: 65.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 62.9, Font: STIXTwoText, Size: 8.555720329284668
Text: Pythia-6.9B, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 6.51, Font: STIXTwoText, Size: 8.555720329284668
Text: 4.45, Font: STIXTwoText, Size: 8.555720329284668
Text: 67.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 64.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 75.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 67.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 35.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 61.3, Font: STIXTwoText, Size: 8.555720329284668
Text: 61.7, Font: STIXTwoText, Size: 8.555720329284668
Text: RWKV-7.4B, Font: STIXTwoText, Size: 8.555720329284668
Text: NeoX, Font: STIXTwoText, Size: 8.555720329284668
Text: 6.31, Font: STIXTwoText, Size: 8.555720329284668
Text: 4.38, Font: STIXTwoText, Size: 8.555720329284668
Text: 67.2, Font: STIXTwoText, Size: 8.555720329284668
Text: 65.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 76.1, Font: STIXTwoText, Size: 8.555720329284668
Text: 67.8, Font: STIXTwoText, Size: 8.555720329284668
Text: 37.5, Font: STIXTwoText, Size: 8.555720329284668
Text: 61.0, Font: STIXTwoText, Size: 8.555720329284668
Text: 62.5, Font: STIXTwoText, Size: 8.555720329284668
Text: total of, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: ≈ 1푀, Font: STIXTwoMath, Size: 9.962639808654785
Text:  tokens per batch. Models were trained for, Font: SFRM1000, Size: 9.962639808654785
Text:  10퐾, Font: STIXTwoMath, Size: 9.962639808654785
Text:  gradient steps for a total of, Font: SFRM1000, Size: 9.962639808654785
Text:  10퐵, Font: STIXTwoMath, Size: 9.962639808654785
Text:  tokens., Font: SFRM1000, Size: 9.962639808654785
Text: Results., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Figure, Font: SFRM1000, Size: 10.061773300170898
Text:  5, Font: SFRM1000, Size: 10.061773300170898
Text:  (Left) shows that Mamba’s pretraining perplexity improves smoothly with model size, and, Font: SFRM1000, Size: 10.061773300170898
Text: that Mamba scales better than both HyenaDNA and Transformer++. For example, at the largest model size of, Font: SFRM1000, Size: 9.982544898986816
Text: ≈ 40푀, Font: STIXTwoMath, Size: 9.962639808654785
Text:  parameters, the curve shows that Mamba can match the Transformer++ and HyenaDNA models with, Font: SFRM1000, Size: 10.061773300170898
Text: roughly, Font: SFRM1000, Size: 9.962639808654785
Text:  3×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  to, Font: SFRM1000, Size: 9.962639808654785
Text:  4×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  fewer parameters., Font: SFRM1000, Size: 9.962639808654785
Text: 4.3.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Scaling: Context Length, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: In the next DNA experiment, we investigate the scaling properties of models with respect to sequence length., Font: SFRM1000, Size: 10.061773300170898
Text: We only compare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at, Font: SFRM1000, Size: 9.937702178955078
Text: longer sequence lengths. We pretrain models on sequence lengths, Font: SFRM1000, Size: 9.862509727478027
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 10, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 1024, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 9.862509727478027
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 12, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 4096, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 9.862509727478027
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 14, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 16384, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 9.862509727478027
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 16, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 65536, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 9.862509727478027
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 18, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 262144, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 9.862509727478027
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 1048576, Font: STIXTwoMath, Size: 9.962639808654785
Text: . We ﬁx a model size of 6 layers by width, Font: SFRM1000, Size: 9.862509727478027
Text:  128, Font: STIXTwoMath, Size: 9.962639808654785
Text:  (about 1.3M-1.4M parameters). Models, Font: SFRM1000, Size: 9.862509727478027
Text: were trained for, Font: SFRM1000, Size: 9.862509727478027
Text:  20퐾, Font: STIXTwoMath, Size: 9.962639808654785
Text:  gradient steps for a total of, Font: SFRM1000, Size: 9.862509727478027
Text:  ≈ 330퐵, Font: STIXTwoMath, Size: 9.962639808654785
Text:  tokens. The longer sequence lengths used sequence length, Font: SFRM1000, Size: 9.862509727478027
Text: warmup similar to (Nguyen, Poli, et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Results., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Figure, Font: SFRM1000, Size: 10.027188301086426
Text:  5, Font: SFRM1000, Size: 10.027188301086426
Text:  (Right) shows that Mamba is able to make use of longer context even up to extremely long, Font: SFRM1000, Size: 10.027188301086426
Text: sequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand,, Font: SFRM1000, Size: 10.061773300170898
Text: the HyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.5, Font: SFRM1000, Size: 10.061773300170898
Text:  on, Font: SFRM1000, Size: 10.061773300170898
Text: properties of the selection mechanism. In particular, LTI models cannot selectively ignore information; from a, Font: SFRM1000, Size: 10.061773300170898
Text: convolutional perspective, a very long convolution kernel is aggregating all information across a long sequence, Font: SFRM1000, Size: 10.061773300170898
Text: 13, Font: STIXTwoText, Size: 9.962639808654785
Text: 10, Font: Roboto-Bold, Size: 4.898740291595459
Text: 6, Font: Roboto-Bold, Size: 3.4291179180145264
Text: 10, Font: Roboto-Bold, Size: 4.898740291595459
Text: 7, Font: Roboto-Bold, Size: 3.4291179180145264
Text: Parameters (log scale), Font: Roboto-Bold, Size: 6.234760284423828
Text: 2.7, Font: Roboto-Bold, Size: 4.898740291595459
Text: 2.8, Font: Roboto-Bold, Size: 4.898740291595459
Text: 2.9, Font: Roboto-Bold, Size: 4.898740291595459
Text: 3.0, Font: Roboto-Bold, Size: 4.898740291595459
Text: 3.1, Font: Roboto-Bold, Size: 4.898740291595459
Text: Perplexity, Font: Roboto-Bold, Size: 6.234760284423828
Text: Scaling Laws on the Human Genome (HG38), Font: Roboto-Bold, Size: 6.234760284423828
Text: HyenaDNA, Font: Roboto-Bold, Size: 4.898740291595459
Text: Mamba, Font: Roboto-Bold, Size: 4.898740291595459
Text: Transformer++, Font: Roboto-Bold, Size: 4.898740291595459
Text: 10, Font: Roboto-Bold, Size: 4.898740291595459
Text: 3, Font: Roboto-Bold, Size: 3.4291179180145264
Text: 10, Font: Roboto-Bold, Size: 4.898740291595459
Text: 4, Font: Roboto-Bold, Size: 3.4291179180145264
Text: 10, Font: Roboto-Bold, Size: 4.898740291595459
Text: 5, Font: Roboto-Bold, Size: 3.4291179180145264
Text: 10, Font: Roboto-Bold, Size: 4.898740291595459
Text: 6, Font: Roboto-Bold, Size: 3.4291179180145264
Text: Sequence Length, Font: Roboto-Bold, Size: 6.234760284423828
Text: 2.75, Font: Roboto-Bold, Size: 4.898740291595459
Text: 2.80, Font: Roboto-Bold, Size: 4.898740291595459
Text: 2.85, Font: Roboto-Bold, Size: 4.898740291595459
Text: 2.90, Font: Roboto-Bold, Size: 4.898740291595459
Text: 2.95, Font: Roboto-Bold, Size: 4.898740291595459
Text: 3.00, Font: Roboto-Bold, Size: 4.898740291595459
Text: Perplexity, Font: Roboto-Bold, Size: 6.234760284423828
Text: Scaling Laws - Sequence Length (HG38), Font: Roboto-Bold, Size: 6.234760284423828
Text: HyenaDNA 1.4M, Font: Roboto-Bold, Size: 4.898740291595459
Text: Mamba 1.4M, Font: Roboto-Bold, Size: 4.898740291595459
Text: Mamba 7M, Font: Roboto-Bold, Size: 4.898740291595459
Text: Figure 5: (, Font: STIXTwoText, Size: 8.96638011932373
Text: DNA Scaling Laws, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Pretraining on the HG38 (human genome) dataset. (, Font: STIXTwoText, Size: 8.96638011932373
Text: Left, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Fixing short context length, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 10, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 1024, Font: STIXTwoMath, Size: 8.96638011932373
Text: and increasing size from, Font: STIXTwoText, Size: 8.96638011932373
Text:  ≈ 200퐾, Font: STIXTwoMath, Size: 8.96638011932373
Text:  to, Font: STIXTwoText, Size: 8.96638011932373
Text:  ≈ 40푀, Font: STIXTwoMath, Size: 8.96638011932373
Text:  parameters, Mamba scales better than baselines. (, Font: STIXTwoText, Size: 8.96638011932373
Text: Right, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Fixing model size and increasing, Font: STIXTwoText, Size: 8.96638011932373
Text: sequence lengths while keeping tokens/batch and total training tokens fxed. Unlike baselines, the selection mechanism of Mamba, Font: STIXTwoText, Size: 8.96638011932373
Text: facilitates better performance with increasing context length., Font: STIXTwoText, Size: 8.96638011932373
Text: 10, Font: Roboto-Bold, Size: 4.899069786071777
Text: 3, Font: Roboto-Bold, Size: 3.429348945617676
Text: 10, Font: Roboto-Bold, Size: 4.899069786071777
Text: 4, Font: Roboto-Bold, Size: 3.429348945617676
Text: 10, Font: Roboto-Bold, Size: 4.899069786071777
Text: 5, Font: Roboto-Bold, Size: 3.429348945617676
Text: 10, Font: Roboto-Bold, Size: 4.899069786071777
Text: 6, Font: Roboto-Bold, Size: 3.429348945617676
Text: Sequence Length, Font: Roboto-Bold, Size: 6.235179901123047
Text: 0.2, Font: Roboto-Bold, Size: 4.899069786071777
Text: 0.3, Font: Roboto-Bold, Size: 4.899069786071777
Text: 0.4, Font: Roboto-Bold, Size: 4.899069786071777
Text: 0.5, Font: Roboto-Bold, Size: 4.899069786071777
Text: 0.6, Font: Roboto-Bold, Size: 4.899069786071777
Text: 0.7, Font: Roboto-Bold, Size: 4.899069786071777
Text: 0.8, Font: Roboto-Bold, Size: 4.899069786071777
Text: Accuracy, Font: Roboto-Bold, Size: 6.235179901123047
Text: Finetuning Accuracy (Species DNA Classification), Font: Roboto-Bold, Size: 6.235179901123047
Text: HyenaDNA 1.4M, Font: Roboto-Bold, Size: 4.899069786071777
Text: Mamba 1.4M, Font: Roboto-Bold, Size: 4.899069786071777
Text: Mamba 7M, Font: Roboto-Bold, Size: 4.899069786071777
Text: Random, Font: Roboto-Bold, Size: 4.899069786071777
Text: Figure 6: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Great Apes DNA Classifcation, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Accuracy after, Font: STIXTwoText, Size: 8.96638011932373
Text: fne-tuning on sequences of length, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 10, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 1024, Font: STIXTwoMath, Size: 8.96638011932373
Text:  up to, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 20, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: =, Font: STIXTwoMath, Size: 8.96638011932373
Text: 1048576, Font: STIXTwoMath, Size: 8.96638011932373
Text:  using pretrained models of the same context length. Nu-, Font: STIXTwoText, Size: 8.96638011932373
Text: merical results in Table, Font: STIXTwoText, Size: 8.96638011932373
Text:  13, Font: STIXTwoText, Size: 8.96638011932373
Text: ., Font: STIXTwoText, Size: 8.96638011932373
Text: 10, Font: Roboto-Bold, Size: 4.843519687652588
Text: 4, Font: Roboto-Bold, Size: 3.3904638290405273
Text: 10, Font: Roboto-Bold, Size: 4.843519687652588
Text: 5, Font: Roboto-Bold, Size: 3.3904638290405273
Text: 10, Font: Roboto-Bold, Size: 4.843519687652588
Text: 6, Font: Roboto-Bold, Size: 3.3904638290405273
Text: Sequence Length, Font: Roboto-Bold, Size: 6.164479732513428
Text: 1.300, Font: Roboto-Bold, Size: 4.843519687652588
Text: 1.325, Font: Roboto-Bold, Size: 4.843519687652588
Text: 1.350, Font: Roboto-Bold, Size: 4.843519687652588
Text: 1.375, Font: Roboto-Bold, Size: 4.843519687652588
Text: 1.400, Font: Roboto-Bold, Size: 4.843519687652588
Text: 1.425, Font: Roboto-Bold, Size: 4.843519687652588
Text: 1.450, Font: Roboto-Bold, Size: 4.843519687652588
Text: 1.475, Font: Roboto-Bold, Size: 4.843519687652588
Text: Bits Per Byte, Font: Roboto-Bold, Size: 6.164479732513428
Text: Scaling Laws - Sequence Length (YouTubeMix), Font: Roboto-Bold, Size: 6.164479732513428
Text: S4+FFN, Font: Roboto-Bold, Size: 4.843519687652588
Text: Mamba, Font: Roboto-Bold, Size: 4.843519687652588
Text: Figure 7: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Audio Pretraining, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Mamba improves performance, Font: STIXTwoText, Size: 8.96638011932373
Text: over prior state-of-the-art (Sashimi) in autoregressive audio mod-, Font: STIXTwoText, Size: 8.96638011932373
Text: eling, while improving up to minute-long context or million-, Font: STIXTwoText, Size: 8.96638011932373
Text: length sequences (controlling for computation)., Font: STIXTwoText, Size: 8.96638011932373
Text: which may be very noisy. Note that while HyenaDNA claims to improve with longer context, their results do not, Font: SFRM1000, Size: 9.912701606750488
Text: control for computation time., Font: SFRM1000, Size: 9.962639808654785
Text: 4.3.3, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Synthetic Species Classifcation, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We evaluate models on a downstream task of classifying between 5 diﬀerent species by randomly sampling a contigu-, Font: SFRM1000, Size: 9.862509727478027
Text: ous segment of their DNA. This task is adapted from HyenaDNA, which used the species, Font: SFRM1000, Size: 9.862509727478027
Text:  {, Font: STIXTwoMath, Size: 9.962639808654785
Text: human, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  lemur, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  mouse, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  pig, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  hippo, Font: LMMono10-Regular, Size: 9.962639808654785
Text: }, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.862509727478027
Text: We modify the task to be signiﬁcantly more challenging by classifying between the ﬁve great apes species, Font: SFRM1000, Size: 9.962639808654785
Text: {, Font: STIXTwoMath, Size: 9.962639808654785
Text: human, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  chimpanzee, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  gorilla, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  orangutan, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  bonobo, Font: LMMono10-Regular, Size: 9.962639808654785
Text: }, Font: STIXTwoMath, Size: 9.962639808654785
Text: , which are known to share 99% of their DNA., Font: SFRM1000, Size: 9.962639808654785
Text: 4.4, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Audio Modeling and Generation, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: For the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel, Font: SFRM1000, Size: 9.882617950439453
Text: et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2022, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: This model comprises, Font: SFRM1000, Size: 9.962639808654785
Text: 1. a U-Net backbone with two stages of pooling by a factor, Font: SFRM1000, Size: 9.962639808654785
Text:  푝, Font: STIXTwoMath, Size: 9.962639808654785
Text:  that doubles the model dimension, Font: SFRM1000, Size: 9.962639808654785
Text:  퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text:  per stage,, Font: SFRM1000, Size: 9.962639808654785
Text: 2. alternating S4 and MLP blocks in each stage., Font: SFRM1000, Size: 9.962639808654785
Text: We consider replacing the S4+MLP blocks with Mamba blocks., Font: SFRM1000, Size: 9.942694664001465
Text: Experiment details are in Appendix, Font: SFRM1000, Size: 9.942694664001465
Text:  E.4, Font: SFRM1000, Size: 9.942694664001465
Text: ., Font: SFRM1000, Size: 9.942694664001465
Text: 4.4.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Long-Context Autoregressive Pretraining, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound, Font: SFRM1000, Size: 10.061773300170898
Text:  2017, Font: SFRM1000, Size: 10.061773300170898
Text: ), a, Font: SFRM1000, Size: 10.061773300170898
Text: standard piano music dataset used by prior work consisting of, Font: SFRM1000, Size: 9.997447967529297
Text:  4, Font: STIXTwoMath, Size: 9.962639808654785
Text:  hours of solo piano music, sampled at a rate of, Font: SFRM1000, Size: 9.997447967529297
Text: 14, Font: STIXTwoText, Size: 9.962639808654785
Text: 16000 Hz Pretraining details largely follow the standard language modeling setup (Section, Font: SFRM1000, Size: 9.862509727478027
Text:  4.2, Font: SFRM1000, Size: 9.862509727478027
Text: ). Figure, Font: SFRM1000, Size: 9.862509727478027
Text:  7, Font: SFRM1000, Size: 9.862509727478027
Text:  evaluates, Font: SFRM1000, Size: 9.862509727478027
Text: the eﬀect of increasing training sequence lengths from, Font: SFRM1000, Size: 10.0123291015625
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 13, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 8192, Font: STIXTwoMath, Size: 9.962639808654785
Text:  to, Font: SFRM1000, Size: 10.0123291015625
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: ≈ 10, Font: STIXTwoMath, Size: 9.962639808654785
Text: 6, Font: STIXTwoMath, Size: 7.471980094909668
Text: , while keeping computation ﬁxed., Font: SFRM1000, Size: 10.0123291015625
Text: (There are some slight edge cases to the way the data is curated, which may lead to kinks in the scaling curves., Font: SFRM1000, Size: 10.0123291015625
Text: For example, only minute-long clips were available so the maximum sequence length is actually bounded by, Font: SFRM1000, Size: 10.061773300170898
Text: 60푠 ⋅ 16000퐻푧 = 960000, Font: STIXTwoMath, Size: 9.962639808654785
Text: .), Font: SFRM1000, Size: 9.962639808654785
Text: Both Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is, Font: SFRM1000, Size: 10.002410888671875
Text: better throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a, Font: SFRM1000, Size: 10.061773300170898
Text: constant factor, Font: SFRM1000, Size: 9.962639808654785
Text:  log(2), Font: STIXTwoMath, Size: 9.962639808654785
Text:  of the standard negative log-likelihood (NLL) loss for pretraining other modalities., Font: SFRM1000, Size: 9.962639808654785
Text: We note one important detail: this is the only experiment in this paper in which we switched from the real, Font: SFRM1000, Size: 10.061773300170898
Text: parameterization to complex (Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3.6, Font: SFRM1000, Size: 9.962639808654785
Text: ). We show additional ablations in Appendix, Font: SFRM1000, Size: 9.962639808654785
Text:  E.4, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 4.4.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Autoregressive Speech Generation, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette, Font: SFRM1000, Size: 9.862509727478027
Text:  2019, Font: SFRM1000, Size: 9.862509727478027
Text: ; Warden, Font: SFRM1000, Size: 9.862509727478027
Text:  2018, Font: SFRM1000, Size: 9.862509727478027
Text: ), consisting, Font: SFRM1000, Size: 9.862509727478027
Text: of, Font: SFRM1000, Size: 9.922708511352539
Text:  1, Font: STIXTwoMath, Size: 9.962639808654785
Text: -second clips sampled at 16000 Hz of the digits “zero” through “nine” with highly variable characteristics. We, Font: SFRM1000, Size: 9.922708511352539
Text: largely follow the autoregressive training setup and generation protocol of Goel et al. (, Font: SFRM1000, Size: 9.962639808654785
Text: 2022, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 9.977572441101074
Text:  4, Font: SFRM1000, Size: 9.977572441101074
Text:  shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al., Font: SFRM1000, Size: 9.977572441101074
Text: (, Font: SFRM1000, Size: 9.862509727478027
Text: 2022, Font: SFRM1000, Size: 9.862509727478027
Text: ): WaveNet (Oord et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2016, Font: SFRM1000, Size: 9.862509727478027
Text: ), SampleRNN (Mehri et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2017, Font: SFRM1000, Size: 9.862509727478027
Text: ), WaveGAN (Donahue, McAuley, and Puckette, Font: SFRM1000, Size: 9.862509727478027
Text: 2019, Font: SFRM1000, Size: 10.061773300170898
Text: ), DiﬀWave (Z. Kong et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ), and SaShiMi. A small Mamba model outperforms the state-of-the-art, Font: SFRM1000, Size: 10.061773300170898
Text: (and much larger) GAN- and diﬀusion- based models. A larger model parameter-matched to the baselines further, Font: SFRM1000, Size: 9.8876371383667
Text: improves on ﬁdelity metrics dramatically., Font: SFRM1000, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 9.87256908416748
Text:  5, Font: SFRM1000, Size: 9.87256908416748
Text:  takes the small Mamba model and investigates combinations of diﬀerent architectures for the outer stages, Font: SFRM1000, Size: 9.87256908416748
Text: and center stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba, Font: SFRM1000, Size: 10.002410888671875
Text:  >, Font: STIXTwoMath, Size: 9.962639808654785
Text: S4+MLP, Font: SFRM1000, Size: 9.962639808654785
Text:  >, Font: STIXTwoMath, Size: 9.962639808654785
Text:  MHA+MLP in the center blocks., Font: SFRM1000, Size: 9.962639808654785
Text: Table 4: (, Font: STIXTwoText, Size: 8.96638011932373
Text: SC09, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: ) Automated metrics for unconditional generation, Font: STIXTwoText, Size: 8.96638011932373
Text: on a challenging dataset of fxed-length speech clips., Font: STIXTwoText, Size: 8.96638011932373
Text: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Top to, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: Bottom, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Autoregressive baselines, non-autoregressive baselines,, Font: STIXTwoText, Size: 8.96638011932373
Text: Mamba, and dataset metrics., Font: STIXTwoText, Size: 8.96638011932373
Text: Model, Font: STIXTwoText, Size: 6.973849773406982
Text: Params, Font: STIXTwoText, Size: 6.973849773406982
Text: NLL, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↓, Font: STIXTwoMath, Size: 6.973849773406982
Text: FID, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↓, Font: STIXTwoMath, Size: 6.973849773406982
Text: IS, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↑, Font: STIXTwoMath, Size: 6.973849773406982
Text: mIS, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↑, Font: STIXTwoMath, Size: 6.973849773406982
Text: AM, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↓, Font: STIXTwoMath, Size: 6.973849773406982
Text: SampleRNN, Font: STIXTwoText, Size: 6.973849773406982
Text: 35.0M, Font: STIXTwoText, Size: 6.973849773406982
Text: 2.042, Font: STIXTwoText, Size: 6.973849773406982
Text: 8.96, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.71, Font: STIXTwoText, Size: 6.973849773406982
Text: 3.02, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.76, Font: STIXTwoText, Size: 6.973849773406982
Text: WaveNet, Font: STIXTwoText, Size: 6.973849773406982
Text: 4.2M, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.925, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.08, Font: STIXTwoText, Size: 6.973849773406982
Text: 2.27, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.80, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.47, Font: STIXTwoText, Size: 6.973849773406982
Text: SaShiMi, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.8M, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.873, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.99, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.13, Font: STIXTwoText, Size: 6.973849773406982
Text: 42.57, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.74, Font: STIXTwoText, Size: 6.973849773406982
Text: WaveGAN, Font: STIXTwoText, Size: 6.973849773406982
Text: 19.1M, Font: STIXTwoText, Size: 6.973849773406982
Text: -, Font: STIXTwoText, Size: 6.973849773406982
Text: 2.03, Font: STIXTwoText, Size: 6.973849773406982
Text: 4.90, Font: STIXTwoText, Size: 6.973849773406982
Text: 36.10, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.80, Font: STIXTwoText, Size: 6.973849773406982
Text: DifWave, Font: STIXTwoText, Size: 6.973849773406982
Text: 24.1M, Font: STIXTwoText, Size: 6.973849773406982
Text: -, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.92, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.26, Font: STIXTwoText, Size: 6.973849773406982
Text: 51.21, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.68, Font: STIXTwoText, Size: 6.973849773406982
Text: + SaShiMi, Font: STIXTwoText, Size: 6.973849773406982
Text: 23.0M, Font: STIXTwoText, Size: 6.973849773406982
Text: -, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.42, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.94, Font: STIXTwoText, Size: 6.973849773406982
Text: 69.17, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.59, Font: STIXTwoText, Size: 6.973849773406982
Text: Mamba, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 6.1M, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.852, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 0.94, Font: STIXTwoText, Size: 6.973849773406982
Text: 6.26, Font: STIXTwoText, Size: 6.973849773406982
Text: 88.54, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.52, Font: STIXTwoText, Size: 6.973849773406982
Text: Mamba, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 24.3M, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.860, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.67, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 7.33, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 144.9, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 0.36, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: Train, Font: STIXTwoText, Size: 6.973849773406982
Text: -, Font: STIXTwoText, Size: 6.973849773406982
Text: -, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.00, Font: STIXTwoMath, Size: 6.973849773406982
Text: 8.56, Font: STIXTwoMath, Size: 6.973849773406982
Text: 292.5, Font: STIXTwoMath, Size: 6.973849773406982
Text: 0.16, Font: STIXTwoMath, Size: 6.973849773406982
Text: Test, Font: STIXTwoText, Size: 6.973849773406982
Text: -, Font: STIXTwoText, Size: 6.973849773406982
Text: -, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.02, Font: STIXTwoMath, Size: 6.973849773406982
Text: 8.33, Font: STIXTwoMath, Size: 6.973849773406982
Text: 257.6, Font: STIXTwoMath, Size: 6.973849773406982
Text: 0.19, Font: STIXTwoMath, Size: 6.973849773406982
Text: Table 5: (, Font: STIXTwoText, Size: 8.96638011932373
Text: SC09 Model Ablations, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: ) Models with 6M parameters., Font: STIXTwoText, Size: 8.96638011932373
Text: In SaShiMi’s U-Net backbone, there are 8 center blocks operat-, Font: STIXTwoText, Size: 8.96638011932373
Text: ing on sequence length, Font: STIXTwoText, Size: 8.96638011932373
Text:  1000, Font: STIXTwoMath, Size: 8.96638011932373
Text: , sandwiched on each side by 8 outer, Font: STIXTwoText, Size: 8.96638011932373
Text: blocks on sequence length, Font: STIXTwoText, Size: 8.96638011932373
Text:  4000, Font: STIXTwoMath, Size: 8.96638011932373
Text: , sandwiched by 8 outer blocks, Font: STIXTwoText, Size: 8.96638011932373
Text: on sequence length, Font: STIXTwoText, Size: 8.96638011932373
Text:  16000, Font: STIXTwoMath, Size: 8.96638011932373
Text:  (40 blocks total). The architecture of, Font: STIXTwoText, Size: 8.96638011932373
Text: the 8 center blocks are ablated independently of the rest. Note, Font: STIXTwoText, Size: 8.96638011932373
Text: that Transformers (MHA+MLP) were not tested in the more im-, Font: STIXTwoText, Size: 8.96638011932373
Text: portant outer blocks because of efciency constraints., Font: STIXTwoText, Size: 8.96638011932373
Text: Outer, Font: STIXTwoText, Size: 6.973849773406982
Text: Center, Font: STIXTwoText, Size: 6.973849773406982
Text: NLL, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↓, Font: STIXTwoMath, Size: 6.973849773406982
Text: FID, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↓, Font: STIXTwoMath, Size: 6.973849773406982
Text: IS, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↑, Font: STIXTwoMath, Size: 6.973849773406982
Text: mIS, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↑, Font: STIXTwoMath, Size: 6.973849773406982
Text: AM, Font: STIXTwoText, Size: 6.973849773406982
Text:  ↓, Font: STIXTwoMath, Size: 6.973849773406982
Text: S4+MLP, Font: STIXTwoText, Size: 6.973849773406982
Text: MHA+MLP, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.859, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.45, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.06, Font: STIXTwoText, Size: 6.973849773406982
Text: 47.03, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.70, Font: STIXTwoText, Size: 6.973849773406982
Text: S4+MLP, Font: STIXTwoText, Size: 6.973849773406982
Text: S4+MLP, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.867, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.43, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.42, Font: STIXTwoText, Size: 6.973849773406982
Text: 53.54, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.65, Font: STIXTwoText, Size: 6.973849773406982
Text: S4+MLP, Font: STIXTwoText, Size: 6.973849773406982
Text: Mamba, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.859, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.42, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.71, Font: STIXTwoText, Size: 6.973849773406982
Text: 56.51, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.64, Font: STIXTwoText, Size: 6.973849773406982
Text: Mamba, Font: STIXTwoText, Size: 6.973849773406982
Text: MHA+MLP, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.850, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 1.37, Font: STIXTwoText, Size: 6.973849773406982
Text: 5.63, Font: STIXTwoText, Size: 6.973849773406982
Text: 58.23, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.62, Font: STIXTwoText, Size: 6.973849773406982
Text: Mamba, Font: STIXTwoText, Size: 6.973849773406982
Text: S4+MLP, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.853, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.07, Font: STIXTwoText, Size: 6.973849773406982
Text: 6.05, Font: STIXTwoText, Size: 6.973849773406982
Text: 73.34, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.55, Font: STIXTwoText, Size: 6.973849773406982
Text: Mamba, Font: STIXTwoText, Size: 6.973849773406982
Text: Mamba, Font: STIXTwoText, Size: 6.973849773406982
Text: 1.852, Font: STIXTwoText, Size: 6.973849773406982
Text: 0.94, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 6.26, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 88.54, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 0.52, Font: STIXTwoText-Bold, Size: 6.973849773406982
Text: 4.5, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Speed and Memory Benchmarks, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: We benchmark the speed of the SSM scan operation (state expansion, Font: SFRM1000, Size: 9.877593994140625
Text:  푁 = 16, Font: STIXTwoMath, Size: 9.962639808654785
Text: ), as well as the end-to-end inference, Font: SFRM1000, Size: 9.877593994140625
Text: throughput of Mamba, in Figure, Font: SFRM1000, Size: 9.932706832885742
Text:  8, Font: SFRM1000, Size: 9.932706832885742
Text: . Our eﬃcient SSM scan is faster than the best attention implementation that, Font: SFRM1000, Size: 9.932706832885742
Text: we know of (FlashAttention-2 (Dao, Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: )) beyond sequence length 2K, and up to 20-40, Font: SFRM1000, Size: 9.962639808654785
Text: ×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  faster than a standard, Font: SFRM1000, Size: 9.962639808654785
Text: scan implementation in PyTorch. Mamba achieves 4-5, Font: SFRM1000, Size: 9.862509727478027
Text: ×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  higher inference throughput than a Transformer of similar, Font: SFRM1000, Size: 9.862509727478027
Text: size, since without the KV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained), Font: SFRM1000, Size: 10.061773300170898
Text: would have higher inference throughput than a, Font: SFRM1000, Size: 10.061773300170898
Text:  5×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  smaller Transformer-1.3B. Details in Appendix, Font: SFRM1000, Size: 10.061773300170898
Text:  E.5, Font: SFRM1000, Size: 10.061773300170898
Text: , which, Font: SFRM1000, Size: 10.061773300170898
Text: additionally includes a benchmark of memory consumption., Font: SFRM1000, Size: 9.962639808654785
Text: 15, Font: STIXTwoText, Size: 9.962639808654785
Text: 512, Font: Roboto-Bold, Size: 4.749030113220215
Text: 1k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 2k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 4k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 8k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 16k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 32k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 64k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 128k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 256k, Font: Roboto-Bold, Size: 4.749030113220215
Text: 512k, Font: Roboto-Bold, Size: 4.749030113220215
Text: Sequence length, Font: Roboto-Bold, Size: 6.044219970703125
Text: 0.1, Font: Roboto-Bold, Size: 4.749030113220215
Text: 1, Font: Roboto-Bold, Size: 4.749030113220215
Text: 10, Font: Roboto-Bold, Size: 4.749030113220215
Text: 100, Font: Roboto-Bold, Size: 4.749030113220215
Text: 1000, Font: Roboto-Bold, Size: 4.749030113220215
Text: Time (ms), Font: Roboto-Bold, Size: 6.044219970703125
Text: Scan vs Convolution vs Attention time (A100 80GB PCIe), Font: Roboto-Bold, Size: 6.044219970703125
Text: FlashAttention-2, Font: Roboto-Bold, Size: 4.749030113220215
Text: Convolution, Font: Roboto-Bold, Size: 4.749030113220215
Text: Scan (PyTorch), Font: Roboto-Bold, Size: 4.749030113220215
Text: Scan (ours), Font: Roboto-Bold, Size: 4.749030113220215
Text: OOM, Font: Roboto-Bold, Size: 4.749030113220215
Text: 1, Font: Roboto-Bold, Size: 4.091119766235352
Text: 2, Font: Roboto-Bold, Size: 4.091119766235352
Text: 4, Font: Roboto-Bold, Size: 4.091119766235352
Text: 8, Font: Roboto-Bold, Size: 4.091119766235352
Text: 16, Font: Roboto-Bold, Size: 4.091119766235352
Text: 32, Font: Roboto-Bold, Size: 4.091119766235352
Text: 64, Font: Roboto-Bold, Size: 4.091119766235352
Text: 128, Font: Roboto-Bold, Size: 4.091119766235352
Text: Batch size, Font: Roboto-Bold, Size: 5.206879615783691
Text: 500, Font: Roboto-Bold, Size: 4.091119766235352
Text: 1000, Font: Roboto-Bold, Size: 4.091119766235352
Text: 1500, Font: Roboto-Bold, Size: 4.091119766235352
Text: Throughput (tokens / s), Font: Roboto-Bold, Size: 5.206879615783691
Text: 140, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 247, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 441, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 744, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 1089, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 1445, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 1688, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 1814, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 79, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 132, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 199, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 265, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 323, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 364, Font: Roboto-Bold, Size: 1.8595999479293823
Text: OOM, Font: Roboto-Bold, Size: 1.8595999479293823
Text: OOM, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 58, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 101, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 172, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 261, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 364, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 443, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 490, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 515, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 46, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 66, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 91, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 109, Font: Roboto-Bold, Size: 1.8595999479293823
Text: 120, Font: Roboto-Bold, Size: 1.8595999479293823
Text: OOM, Font: Roboto-Bold, Size: 1.8595999479293823
Text: OOM, Font: Roboto-Bold, Size: 1.8595999479293823
Text: OOM, Font: Roboto-Bold, Size: 1.8595999479293823
Text: Inference throughput on A100 80GB (prompt length 2048), Font: Roboto-Bold, Size: 5.206879615783691
Text: Mamba 1.4B, Font: Roboto-Bold, Size: 4.091119766235352
Text: Transformer 1.3B, Font: Roboto-Bold, Size: 4.091119766235352
Text: Mamba 6.9B, Font: Roboto-Bold, Size: 4.091119766235352
Text: Transformer 6.7B, Font: Roboto-Bold, Size: 4.091119766235352
Text: Figure 8: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Efciency Benchmarks, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) (, Font: STIXTwoText, Size: 8.96638011932373
Text: Left, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Training: our efcient scan is, Font: STIXTwoText, Size: 8.96638011932373
Text:  40×, Font: STIXTwoMath, Size: 8.96638011932373
Text:  faster than a standard implementation. (, Font: STIXTwoText, Size: 8.96638011932373
Text: Right, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ), Font: STIXTwoText, Size: 8.96638011932373
Text: Inference: as a recurrent model, Mamba can achieve, Font: STIXTwoText, Size: 8.96638011932373
Text:  5×, Font: STIXTwoMath, Size: 8.96638011932373
Text:  higher throughput than Transformers., Font: STIXTwoText, Size: 8.96638011932373
Text: 4.6, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Model Ablations, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: We perform a series of detailed ablations on components of our model, focusing on the setting of language modeling, Font: SFRM1000, Size: 9.862509727478027
Text: with size, Font: SFRM1000, Size: 9.962639808654785
Text:  ≈ 350, Font: STIXTwoMath, Size: 9.962639808654785
Text: M models at Chinchilla token counts (same setting as Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  4, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: 4.6.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Architecture, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 9.87256908416748
Text:  6, Font: SFRM1000, Size: 9.87256908416748
Text:  investigates the eﬀects of the architecture (block) and its inner SSM layer (Figure, Font: SFRM1000, Size: 9.87256908416748
Text:  3, Font: SFRM1000, Size: 9.87256908416748
Text: ). We ﬁnd that, Font: SFRM1000, Size: 9.87256908416748
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very, Font: SFRM1000, Size: 10.061773300170898
Text: similar., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Replacing the complex-valued S4 variant from previous work with a real-valued one does not aﬀect performance, Font: SFRM1000, Size: 9.862509727478027
Text: much, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware, Font: SFRM1000, Size: 9.862509727478027
Text: eﬃciency., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Replacing any of these with a selective SSM (S6) signiﬁcantly improves performance, validating the motivation, Font: SFRM1000, Size: 9.912701606750488
Text: of Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a, Font: SFRM1000, Size: 10.061773300170898
Text: selective layer)., Font: SFRM1000, Size: 9.962639808654785
Text: We also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA, Font: SFRM1000, Size: 9.862509727478027
Text: (a hybrid attention architecture) in Appendix, Font: SFRM1000, Size: 9.962639808654785
Text:  E.2.2, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 4.6.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Selective SSM, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 10.061773300170898
Text:  7, Font: SFRM1000, Size: 10.061773300170898
Text:  ablates the selective SSM layer by considering diﬀerent combinations of selective, Font: SFRM1000, Size: 10.061773300170898
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.061773300170898
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: , and, Font: SFRM1000, Size: 10.061773300170898
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text:  param-, Font: SFRM1000, Size: 10.061773300170898
Text: eters (Algorithm, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: SFRM1000, Size: 10.061773300170898
Text: ), showing that, Font: SFRM1000, Size: 10.061773300170898
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  is the most important parameter due to its connection to RNN gating, Font: SFRM1000, Size: 10.061773300170898
Text: (Theorem, Font: SFRM1000, Size: 9.962639808654785
Text:  1, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 9.982544898986816
Text:  8, Font: SFRM1000, Size: 9.982544898986816
Text:  considers diﬀerent initializations of the SSM, which have been shown to make a large diﬀerence in some, Font: SFRM1000, Size: 9.982544898986816
Text: data modalities and settings (Gu, Goel, and Ré, Font: SFRM1000, Size: 10.056839942932129
Text:  2022, Font: SFRM1000, Size: 10.056839942932129
Text: ; Gu, Gupta, et al., Font: SFRM1000, Size: 10.056839942932129
Text:  2022, Font: SFRM1000, Size: 10.056839942932129
Text: ). On language modeling, we ﬁnd, Font: SFRM1000, Size: 10.056839942932129
Text: that simpler real-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued, Font: SFRM1000, Size: 10.061773300170898
Text: parameterizations (S4D-Lin, row 1) perform better. Random initializations also work well, consistent with ﬁndings, Font: SFRM1000, Size: 9.862509727478027
Text: from prior work (Mehta et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 10.061773300170898
Text:  9, Font: SFRM1000, Size: 10.061773300170898
Text:  and Table, Font: SFRM1000, Size: 10.061773300170898
Text:  10, Font: SFRM1000, Size: 10.061773300170898
Text:  consider varying the dimension of the, Font: SFRM1000, Size: 10.061773300170898
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and, Font: SFRM1000, Size: 10.061773300170898
Text:  (, Font: STIXTwoMath, Size: 9.962639808654785
Text: B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text:  projections respectively. Changing, Font: SFRM1000, Size: 10.061773300170898
Text: them from static to selective provides the most beneﬁt, while increasing the dimensions further generally improves, Font: SFRM1000, Size: 9.862509727478027
Text: performance modestly with a small increase in parameter count., Font: SFRM1000, Size: 9.962639808654785
Text: Of particular note is the dramatic improvement of the selective SSM when the state size, Font: SFRM1000, Size: 9.877593994140625
Text:  푁, Font: STIXTwoMath, Size: 9.962639808654785
Text:  is increased, with over, Font: SFRM1000, Size: 9.877593994140625
Text: a 1.0 perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in, Font: SFRM1000, Size: 9.972597122192383
Text: Sections, Font: SFRM1000, Size: 9.962639808654785
Text:  3.1, Font: SFRM1000, Size: 9.962639808654785
Text:  and, Font: SFRM1000, Size: 9.962639808654785
Text:  3.3, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 16, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 6: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Ablations: Architecture and SSM layer, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) The Mamba block performs similarly to H3 while being simpler. In the, Font: STIXTwoText, Size: 8.96638011932373
Text: inner layer, there is little diference among diferent parameterizations of LTI models, while selective SSMs (S6) provide a large, Font: STIXTwoText, Size: 8.96638011932373
Text: improvement. More specifcally, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin., Font: STIXTwoText, Size: 8.96638011932373
Text: Model, Font: STIXTwoText, Size: 9.962639808654785
Text: Arch., Font: STIXTwoText, Size: 9.962639808654785
Text: SSM Layer, Font: STIXTwoText, Size: 9.962639808654785
Text: Perplexity, Font: STIXTwoText, Size: 9.962639808654785
Text: Hyena, Font: STIXTwoText, Size: 9.962639808654785
Text: H3, Font: STIXTwoText, Size: 9.962639808654785
Text: Hyena, Font: STIXTwoText, Size: 9.962639808654785
Text: 10.24, Font: STIXTwoText, Size: 9.962639808654785
Text: H3, Font: STIXTwoText, Size: 9.962639808654785
Text: H3, Font: STIXTwoText, Size: 9.962639808654785
Text: S4 (complex), Font: STIXTwoText, Size: 9.962639808654785
Text: 10.30, Font: STIXTwoText, Size: 9.962639808654785
Text: -, Font: STIXTwoText, Size: 9.962639808654785
Text: H3, Font: STIXTwoText, Size: 9.962639808654785
Text: S4 (real), Font: STIXTwoText, Size: 9.962639808654785
Text: 10.34, Font: STIXTwoText, Size: 9.962639808654785
Text: -, Font: STIXTwoText, Size: 9.962639808654785
Text: H3, Font: STIXTwoText, Size: 9.962639808654785
Text: S6, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.95, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Model, Font: STIXTwoText, Size: 9.962639808654785
Text: Arch., Font: STIXTwoText, Size: 9.962639808654785
Text: SSM Layer, Font: STIXTwoText, Size: 9.962639808654785
Text: Perplexity, Font: STIXTwoText, Size: 9.962639808654785
Text: -, Font: STIXTwoText, Size: 9.962639808654785
Text: Mamba, Font: STIXTwoText, Size: 9.962639808654785
Text: Hyena, Font: STIXTwoText, Size: 9.962639808654785
Text: 10.75, Font: STIXTwoText, Size: 9.962639808654785
Text: -, Font: STIXTwoText, Size: 9.962639808654785
Text: Mamba, Font: STIXTwoText, Size: 9.962639808654785
Text: S4 (complex), Font: STIXTwoText, Size: 9.962639808654785
Text: 10.54, Font: STIXTwoText, Size: 9.962639808654785
Text: -, Font: STIXTwoText, Size: 9.962639808654785
Text: Mamba, Font: STIXTwoText, Size: 9.962639808654785
Text: S4 (real), Font: STIXTwoText, Size: 9.962639808654785
Text: 10.56, Font: STIXTwoText, Size: 9.962639808654785
Text: Mamba, Font: STIXTwoText, Size: 9.962639808654785
Text: Mamba, Font: STIXTwoText, Size: 9.962639808654785
Text: S6, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.69, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Table 7: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Ablations: Selective parameters, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .), Font: STIXTwoText, Size: 8.96638011932373
Text:  ∆, Font: STIXTwoMath, Size: 8.96638011932373
Text:  is the most im-, Font: STIXTwoText, Size: 8.96638011932373
Text: portant parameter (Theorem, Font: STIXTwoText, Size: 8.96638011932373
Text:  1, Font: STIXTwoText, Size: 8.96638011932373
Text: ), but using multiple selective pa-, Font: STIXTwoText, Size: 8.96638011932373
Text: rameters together synergizes., Font: STIXTwoText, Size: 8.96638011932373
Text: Selective, Font: STIXTwoText, Size: 9.962639808654785
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: Selective, Font: STIXTwoText, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: Selective, Font: STIXTwoText, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: Perplexity, Font: STIXTwoText, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: 10.93, Font: STIXTwoText, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: 10.15, Font: STIXTwoText, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: 9.98, Font: STIXTwoText, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: 9.81, Font: STIXTwoText, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: , Font: Dingbats, Size: 9.962639808654785
Text: 8.71, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 8: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Ablations: Parameterization of, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text:  A, Font: CMMIB9, Size: 8.96638011932373
Text: .) The more, Font: STIXTwoText, Size: 8.96638011932373
Text: standard initializations based on S4D-Lin (Gu, Gupta, et al., Font: STIXTwoText, Size: 8.96638011932373
Text: 2022, Font: STIXTwoText, Size: 8.96638011932373
Text: ) perform worse than S4D-Real or a random initializa-, Font: STIXTwoText, Size: 8.96638011932373
Text: tion, when the SSM is selective., Font: STIXTwoText, Size: 8.96638011932373
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: 푛, Font: STIXTwoMath, Size: 7.471980094909668
Text:  Initialization, Font: STIXTwoText, Size: 9.962639808654785
Text: Field, Font: STIXTwoText, Size: 9.962639808654785
Text: Perplexity, Font: STIXTwoText, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: 푛, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = −, Font: STIXTwoMath, Size: 9.962639808654785
Text:  , Font: STIXTwoMath, Size: 7.471980094909668
Text: 1, Font: STIXTwoMath, Size: 7.471980094909668
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: + 푛푖, Font: STIXTwoMath, Size: 9.962639808654785
Text: Complex, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.16, Font: STIXTwoText, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: 푛, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = −1∕2, Font: STIXTwoMath, Size: 9.962639808654785
Text: Real, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.85, Font: STIXTwoText, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: 푛, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = −(푛 + 1), Font: STIXTwoMath, Size: 9.962639808654785
Text: Real, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.71, Font: STIXTwoText, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: 푛, Font: STIXTwoMath, Size: 7.471980094909668
Text:  ∼ exp(풩(0, 1)), Font: STIXTwoMath, Size: 9.962639808654785
Text: Real, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.71, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 9: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Ablations: Expressivity of, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text:  ∆, Font: STIXTwoMath, Size: 8.96638011932373
Text: .), Font: STIXTwoText, Size: 8.96638011932373
Text: The selection mechanism of, Font: STIXTwoText, Size: 8.96638011932373
Text:  ∆, Font: STIXTwoMath, Size: 8.96638011932373
Text:  constructs, Font: STIXTwoText, Size: 8.96638011932373
Text: it with a projection of the input. Project-, Font: STIXTwoText, Size: 8.96638011932373
Text: ing it even to dim., Font: STIXTwoText, Size: 8.96638011932373
Text:  1, Font: STIXTwoMath, Size: 8.96638011932373
Text:  provides a large in-, Font: STIXTwoText, Size: 8.96638011932373
Text: crease in performance; increasing it fur-, Font: STIXTwoText, Size: 8.96638011932373
Text: ther provides further improvements at the, Font: STIXTwoText, Size: 8.96638011932373
Text: cost of a modest increase in parameters., Font: STIXTwoText, Size: 8.96638011932373
Text: State size fxed to, Font: STIXTwoText, Size: 8.96638011932373
Text:  푁 = 16, Font: STIXTwoMath, Size: 8.96638011932373
Text: ., Font: STIXTwoText, Size: 8.96638011932373
Text: Size of, Font: STIXTwoText, Size: 9.962639808654785
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  proj., Font: STIXTwoText, Size: 9.962639808654785
Text: Params (M), Font: STIXTwoText, Size: 9.962639808654785
Text: Perplexity, Font: STIXTwoText, Size: 9.962639808654785
Text: -, Font: STIXTwoText, Size: 9.962639808654785
Text: 358.9, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.12, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoMath, Size: 9.962639808654785
Text: 359.1, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.97, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 359.3, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.97, Font: STIXTwoText, Size: 9.962639808654785
Text: 4, Font: STIXTwoMath, Size: 9.962639808654785
Text: 359.7, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.91, Font: STIXTwoText, Size: 9.962639808654785
Text: 8, Font: STIXTwoMath, Size: 9.962639808654785
Text: 360.5, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.83, Font: STIXTwoText, Size: 9.962639808654785
Text: 16, Font: STIXTwoMath, Size: 9.962639808654785
Text: 362.1, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.84, Font: STIXTwoText, Size: 9.962639808654785
Text: 32, Font: STIXTwoMath, Size: 9.962639808654785
Text: 365.2, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.80, Font: STIXTwoText, Size: 9.962639808654785
Text: 64, Font: STIXTwoMath, Size: 9.962639808654785
Text: 371.5, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.71, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 10: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Ablations: SSM state dimension, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) (, Font: STIXTwoText, Size: 8.96638011932373
Text: Top, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Constant, Font: STIXTwoText, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text:  and, Font: STIXTwoText, Size: 8.96638011932373
Text:  C, Font: CMMIB9, Size: 8.96638011932373
Text:  (, Font: STIXTwoText, Size: 8.96638011932373
Text: Bottom, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ), Font: STIXTwoText, Size: 8.96638011932373
Text: Selective, Font: STIXTwoText, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text:  and, Font: STIXTwoText, Size: 8.96638011932373
Text:  C, Font: CMMIB9, Size: 8.96638011932373
Text: . Increasing the SSM state dimension, Font: STIXTwoText, Size: 8.96638011932373
Text:  푁, Font: STIXTwoMath, Size: 8.96638011932373
Text: , which can be viewed as, Font: STIXTwoText, Size: 8.96638011932373
Text: an expansion factor on the dimension of the recurrent state, can signifcantly improve, Font: STIXTwoText, Size: 8.96638011932373
Text: performance for a negligible cost in parameters/FLOPs, but only when, Font: STIXTwoText, Size: 8.96638011932373
Text:  B, Font: CMMIB9, Size: 8.96638011932373
Text:  and, Font: STIXTwoText, Size: 8.96638011932373
Text:  C, Font: CMMIB9, Size: 8.96638011932373
Text:  are, Font: STIXTwoText, Size: 8.96638011932373
Text: also selective. Size of, Font: STIXTwoText, Size: 8.96638011932373
Text:  ∆, Font: STIXTwoMath, Size: 8.96638011932373
Text:  projection fxed to, Font: STIXTwoText, Size: 8.96638011932373
Text:  64, Font: STIXTwoMath, Size: 8.96638011932373
Text: ., Font: STIXTwoText, Size: 8.96638011932373
Text: State dimension, Font: STIXTwoText, Size: 9.962639808654785
Text:  푁, Font: STIXTwoMath, Size: 9.962639808654785
Text: Params (M), Font: STIXTwoText, Size: 9.962639808654785
Text: Perplexity, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoMath, Size: 9.962639808654785
Text: 367.1, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.88, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 367.4, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.86, Font: STIXTwoText, Size: 9.962639808654785
Text: 4, Font: STIXTwoMath, Size: 9.962639808654785
Text: 368.0, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.82, Font: STIXTwoText, Size: 9.962639808654785
Text: 8, Font: STIXTwoMath, Size: 9.962639808654785
Text: 369.1, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.82, Font: STIXTwoText, Size: 9.962639808654785
Text: 16, Font: STIXTwoMath, Size: 9.962639808654785
Text: 371.5, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.81, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoMath, Size: 9.962639808654785
Text: 367.1, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.73, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 367.4, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.40, Font: STIXTwoText, Size: 9.962639808654785
Text: 4, Font: STIXTwoMath, Size: 9.962639808654785
Text: 368.0, Font: STIXTwoText, Size: 9.962639808654785
Text: 9.09, Font: STIXTwoText, Size: 9.962639808654785
Text: 8, Font: STIXTwoMath, Size: 9.962639808654785
Text: 369.1, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.84, Font: STIXTwoText, Size: 9.962639808654785
Text: 16, Font: STIXTwoMath, Size: 9.962639808654785
Text: 371.5, Font: STIXTwoText, Size: 9.962639808654785
Text: 8.71, Font: STIXTwoText, Size: 9.962639808654785
Text: 5, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Discussion, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: We discuss related work, limitations, and some future directions., Font: SFRM1000, Size: 9.962639808654785
Text: Related Work., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Appendix, Font: SFRM1000, Size: 9.882617950439453
Text:  A, Font: SFRM1000, Size: 9.882617950439453
Text:  discusses how the selection mechanism relates to similar concepts. Appendix, Font: SFRM1000, Size: 9.882617950439453
Text:  B, Font: SFRM1000, Size: 9.882617950439453
Text:  has, Font: SFRM1000, Size: 9.882617950439453
Text: an extended related work of SSMs and other related models., Font: SFRM1000, Size: 9.962639808654785
Text: No Free Lunch: Continuous-Discrete Spectrum., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Structured SSMs were originally deﬁned as discretizations, Font: SFRM1000, Size: 10.061773300170898
Text: of continuous systems, Font: SFRM1000, Size: 9.982544898986816
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text: , and have had a strong inductive bias toward continuous-time data modalities such as, Font: SFRM1000, Size: 9.982544898986816
Text: perceptual signals (e.g. audio, video). As discussed in Sections, Font: SFRM1000, Size: 10.042024612426758
Text:  3.1, Font: SFRM1000, Size: 10.042024612426758
Text:  and, Font: SFRM1000, Size: 10.042024612426758
Text:  3.5, Font: SFRM1000, Size: 10.042024612426758
Text: , the selection mechanism overcomes, Font: SFRM1000, Size: 10.042024612426758
Text: their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance, Font: SFRM1000, Size: 9.932706832885742
Text: 17, Font: STIXTwoText, Size: 9.962639808654785
Text: on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeoﬀ in more detail., Font: SFRM1000, Size: 9.962639808654785
Text: Downstream Afordances., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Transformer-based foundation models (particularly LLMs) have a rich ecosystem of, Font: SFRM1000, Size: 9.862509727478027
Text: properties and modes of interaction with pretrained models, such as ﬁne-tuning, adaptation, prompting, in-context, Font: SFRM1000, Size: 9.862509727478027
Text: learning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer, Font: SFRM1000, Size: 9.862509727478027
Text: alternatives such as SSMs have similar properties and aﬀordances., Font: SFRM1000, Size: 9.962639808654785
Text: Scaling., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source, Font: SFRM1000, Size: 9.862509727478027
Text: LLMs (e.g. Llama (Touvron et al., Font: SFRM1000, Size: 9.957657814025879
Text:  2023, Font: SFRM1000, Size: 9.957657814025879
Text: )) as well as other recurrent models such as RWKV (B. Peng et al., Font: SFRM1000, Size: 9.957657814025879
Text:  2023, Font: SFRM1000, Size: 9.957657814025879
Text: ), Font: SFRM1000, Size: 9.957657814025879
Text: and RetNet (Y. Sun et al., Font: SFRM1000, Size: 9.877593994140625
Text:  2023, Font: SFRM1000, Size: 9.877593994140625
Text: ), which have been evaluated at the 7B parameter scale and beyond. It remains to, Font: SFRM1000, Size: 9.877593994140625
Text: assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve, Font: SFRM1000, Size: 9.90268325805664
Text: further engineering challenges and adjustments to the model that are not discussed in this paper., Font: SFRM1000, Size: 9.962639808654785
Text: 6, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Conclusion, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: We introduce a selection mechanism to structured state space models, allowing them to perform context-dependent, Font: SFRM1000, Size: 9.862509727478027
Text: reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture,, Font: SFRM1000, Size: 9.977572441101074
Text: Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance, Font: SFRM1000, Size: 9.892655372619629
Text: of strong Transformer models. We are excited about the broad applications of selective state space models to, Font: SFRM1000, Size: 10.061773300170898
Text: build foundation models for diﬀerent domains, especially in emerging modalities requiring long context such as, Font: SFRM1000, Size: 10.032135963439941
Text: genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model, Font: SFRM1000, Size: 9.862509727478027
Text: backbone., Font: SFRM1000, Size: 9.962639808654785
Text: Acknowledgments, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft., Font: SFRM1000, Size: 9.962639808654785
Text: References, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: [1], Font: STIXTwoText, Size: 9.962639808654785
Text: Martin Arjovsky, Amar Shah, and Yoshua Bengio. “Unitary Evolution Recurrent Neural Networks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: International Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2016, pp. 1120–1128., Font: STIXTwoText, Size: 9.962639808654785
Text: [2], Font: STIXTwoText, Size: 9.962639808654785
Text: iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,, Font: STIXTwoText, Size: 9.962639808654785
Text: Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. “Efective Gene Expression Prediction from, Font: STIXTwoText, Size: 9.962639808654785
Text: Sequence by Integrating Long-range Interactions”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Nature Methods, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  18.10 (2021), pp. 1196–1203., Font: STIXTwoText, Size: 9.962639808654785
Text: [3], Font: STIXTwoText, Size: 9.962639808654785
Text: Jimmy Ba, Geofrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. “Using Fast Weights to, Font: STIXTwoText, Size: 9.962639808654785
Text: Attend to the Recent Past”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  29 (2016)., Font: STIXTwoText, Size: 9.962639808654785
Text: [4], Font: STIXTwoText, Size: 9.962639808654785
Text: Jimmy Lei Ba, Jamie Ryan Kiros, and Geofrey E Hinton. “Layer Normalization”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXivpreprintarXiv:1607.06450, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: (2016)., Font: STIXTwoText, Size: 9.962639808654785
Text: [5], Font: STIXTwoText, Size: 9.962639808654785
Text: Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural Machine Translation by Jointly Learning to, Font: STIXTwoText, Size: 9.962639808654785
Text: Align and Translate”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2015., Font: STIXTwoText, Size: 9.962639808654785
Text: [6], Font: STIXTwoText, Size: 9.962639808654785
Text: David Balduzzi and Muhammad Ghifary. “Strongly-typed Recurrent Neural Networks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  International Con-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ference on Machine Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2016, pp. 1292–1300., Font: STIXTwoText, Size: 9.962639808654785
Text: [7], Font: STIXTwoText, Size: 9.962639808654785
Text: Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan,, Font: STIXTwoText, Size: 9.962639808654785
Text: Mohammad Afah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raf, et al. “Pythia: A Suite for, Font: STIXTwoText, Size: 9.962639808654785
Text: Analyzing Large Language Models across Training and Scaling”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Machine, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2023, pp. 2397–2430., Font: STIXTwoText, Size: 9.962639808654785
Text: [8], Font: STIXTwoText, Size: 9.962639808654785
Text: Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. “PIQA: Reasoning about Physical Commonsense, Font: STIXTwoText, Size: 9.962639808654785
Text: in Natural Language”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Proceedings of the AAAI conference on Artifcial Intelligence, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . Vol. 34. 05. 2020, pp. 7432–, Font: STIXTwoText, Size: 9.962639808654785
Text: 7439., Font: STIXTwoText, Size: 9.962639808654785
Text: [9], Font: STIXTwoText, Size: 9.962639808654785
Text: Guy E Blelloch. “Prefx Sums and Their Applications”. In: (1990)., Font: STIXTwoText, Size: 9.962639808654785
Text: [10], Font: STIXTwoText, Size: 9.962639808654785
Text: James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. “Quasi-recurrent Neural Networks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text: arXiv preprint arXiv:1611.01576, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2016)., Font: STIXTwoText, Size: 9.962639808654785
Text: 18, Font: STIXTwoText, Size: 9.962639808654785
Text: [11], Font: STIXTwoText, Size: 9.962639808654785
Text: Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-, Font: STIXTwoText, Size: 9.962639808654785
Text: lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language Models are Few-shot Learners”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text: Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  33 (2020), pp. 1877–1901., Font: STIXTwoText, Size: 9.962639808654785
Text: [12], Font: STIXTwoText, Size: 9.962639808654785
Text: Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. “Scaling Transformer to 1M tokens and Beyond with RMT”., Font: STIXTwoText, Size: 9.962639808654785
Text: In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2304.11062, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [13], Font: STIXTwoText, Size: 9.962639808654785
Text: Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. “Generating Long Sequences with Sparse Trans-, Font: STIXTwoText, Size: 9.962639808654785
Text: formers”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:1904.10509, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2019)., Font: STIXTwoText, Size: 9.962639808654785
Text: [14], Font: STIXTwoText, Size: 9.962639808654785
Text: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Pe-, Font: STIXTwoText, Size: 9.962639808654785
Text: ter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. “Rethinking Attention with Performers”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text: The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2021., Font: STIXTwoText, Size: 9.962639808654785
Text: [15], Font: STIXTwoText, Size: 9.962639808654785
Text: Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul, Font: STIXTwoText, Size: 9.962639808654785
Text: Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. “PaLM: Scaling Language Modeling, Font: STIXTwoText, Size: 9.962639808654785
Text: with Pathways”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Journal of Machine Learning Research, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  24.240 (2023), pp. 1–113. url:, Font: STIXTwoText, Size: 9.962639808654785
Text:  http://jmlr.org/, Font: LMMono10-Regular, Size: 9.962639808654785
Text: papers/v24/22-1144.html, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text: [16], Font: STIXTwoText, Size: 9.962639808654785
Text: Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. “Empirical Evaluation of Gated Re-, Font: STIXTwoText, Size: 9.962639808654785
Text: current Neural Networks on Sequence Modeling”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:1412.3555, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2014)., Font: STIXTwoText, Size: 9.962639808654785
Text: [17], Font: STIXTwoText, Size: 9.962639808654785
Text: Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind, Font: STIXTwoText, Size: 9.962639808654785
Text: Tafjord. “Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: preprint arXiv:1803.05457, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2018)., Font: STIXTwoText, Size: 9.962639808654785
Text: [18], Font: STIXTwoText, Size: 9.962639808654785
Text: Tri Dao. “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning”. In: (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [19], Font: STIXTwoText, Size: 9.962639808654785
Text: Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “FlashAttention: Fast and Memory-, Font: STIXTwoText, Size: 9.962639808654785
Text: Efcient Exact Attention with IO-Awareness”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text: 2022., Font: STIXTwoText, Size: 9.962639808654785
Text: [20], Font: STIXTwoText, Size: 9.962639808654785
Text: Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. “Hungry Hungry, Font: STIXTwoText, Size: 9.962639808654785
Text: Hippos: Towards Language Modeling with State Space Models”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [21], Font: STIXTwoText, Size: 9.962639808654785
Text: Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. “Language Modeling with Gated Convolu-, Font: STIXTwoText, Size: 9.962639808654785
Text: tional Networks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2017, pp. 933–941., Font: STIXTwoText, Size: 9.962639808654785
Text: [22], Font: STIXTwoText, Size: 9.962639808654785
Text: DeepSound., Font: STIXTwoText, Size: 9.962639808654785
Text:  SampleRNN, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text:  https://github.com/deepsound-project/samplernn-pytorch, Font: LMMono10-Regular, Size: 9.962639808654785
Text: . 2017., Font: STIXTwoText, Size: 9.962639808654785
Text: [23], Font: STIXTwoText, Size: 9.962639808654785
Text: Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. “LongNet:, Font: STIXTwoText, Size: 9.962639808654785
Text: Scaling Transformers to 1,000,000,000 Tokens”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2307.02486, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [24], Font: STIXTwoText, Size: 9.962639808654785
Text: Chris Donahue, Julian McAuley, and Miller Puckette. “Adversarial Audio Synthesis”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2019., Font: STIXTwoText, Size: 9.962639808654785
Text: [25], Font: STIXTwoText, Size: 9.962639808654785
Text: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,, Font: STIXTwoText, Size: 9.962639808654785
Text: Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. “An Image is Worth 16x16 Words:, Font: STIXTwoText, Size: 9.962639808654785
Text: Transformers for Image Recognition at Scale”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2020., Font: STIXTwoText, Size: 9.962639808654785
Text: [26], Font: STIXTwoText, Size: 9.962639808654785
Text: Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,, Font: STIXTwoText, Size: 9.962639808654785
Text: Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfeld-Dodds, Danny, Font: STIXTwoText, Size: 9.962639808654785
Text: Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack, Font: STIXTwoText, Size: 9.962639808654785
Text: Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. “A Mathematical Framework for Transformer Circuits”., Font: STIXTwoText, Size: 9.962639808654785
Text: In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Transformer Circuits Thread, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2021). https://transformer-circuits.pub/2021/framework/index.html., Font: STIXTwoText, Size: 9.962639808654785
Text: [27], Font: STIXTwoText, Size: 9.962639808654785
Text: Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. “Block-, Font: STIXTwoText, Size: 9.962639808654785
Text: State Transformer”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2306.09539, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [28], Font: STIXTwoText, Size: 9.962639808654785
Text: Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,, Font: STIXTwoText, Size: 9.962639808654785
Text: Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. “Multi-Head State Space Model for Sequence Modeling”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text: INTERSPEECH, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [29], Font: STIXTwoText, Size: 9.962639808654785
Text: Karl J Friston, Lee Harrison, and Will Penny. “Dynamic Causal Modelling”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Neuroimage, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  19.4 (2003), pp. 1273–, Font: STIXTwoText, Size: 9.962639808654785
Text: 1302., Font: STIXTwoText, Size: 9.962639808654785
Text: [30], Font: STIXTwoText, Size: 9.962639808654785
Text: Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christo-, Font: STIXTwoText, Size: 9.962639808654785
Text: pher Ré. “Simple Hardware-efcient Long Convolutions for Sequence Modeling”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Confer-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ence on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [31], Font: STIXTwoText, Size: 9.962639808654785
Text: Ken-ichi Funahashi and Yuichi Nakamura. “Approximation of Dynamical Systems by Continuous Time Recur-, Font: STIXTwoText, Size: 9.962639808654785
Text: rent Neural Networks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Neural Networks, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  6.6 (1993), pp. 801–806., Font: STIXTwoText, Size: 9.962639808654785
Text: 19, Font: STIXTwoText, Size: 9.962639808654785
Text: [32], Font: STIXTwoText, Size: 9.962639808654785
Text: Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,, Font: STIXTwoText, Size: 9.962639808654785
Text: Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. “The Pile: An 800GB Dataset of Diverse Text, Font: STIXTwoText, Size: 9.962639808654785
Text: for Language Modeling”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2101.00027, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2020)., Font: STIXTwoText, Size: 9.962639808654785
Text: [33], Font: STIXTwoText, Size: 9.962639808654785
Text: Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPof, Charles Foster, Laurence Golding, Jefrey, Font: STIXTwoText, Size: 9.962639808654785
Text: Hsu, Kyle McDonell, Niklas Muennighof, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang,, Font: STIXTwoText, Size: 9.962639808654785
Text: Kevin Wang, and Andy Zou., Font: STIXTwoText, Size: 9.962639808654785
Text:  A Framework for Few-shot Language Model Evaluation, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . Version v0.0.1. Sept. 2021., Font: STIXTwoText, Size: 9.962639808654785
Text: doi:, Font: STIXTwoText, Size: 9.962639808654785
Text:  10.5281/zenodo.5371628, Font: LMMono10-Regular, Size: 9.962639808654785
Text: . url:, Font: STIXTwoText, Size: 9.962639808654785
Text:  https://doi.org/10.5281/zenodo.5371628, Font: LMMono10-Regular, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text: [34], Font: STIXTwoText, Size: 9.962639808654785
Text: Karan Goel, Albert Gu, Chris Donahue, and Christopher Ré. “It’s Raw! Audio Generation with State-Space, Font: STIXTwoText, Size: 9.962639808654785
Text: Models”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2022., Font: STIXTwoText, Size: 9.962639808654785
Text: [35], Font: STIXTwoText, Size: 9.962639808654785
Text: Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. “HIPPO: Recurrent Memory with Optimal, Font: STIXTwoText, Size: 9.962639808654785
Text: Polynomial Projections”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2020., Font: STIXTwoText, Size: 9.962639808654785
Text: [36], Font: STIXTwoText, Size: 9.962639808654785
Text: Albert Gu, Karan Goel, and Christopher Ré. “Efciently Modeling Long Sequences with Structured State Spaces”., Font: STIXTwoText, Size: 9.962639808654785
Text: In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2022., Font: STIXTwoText, Size: 9.962639808654785
Text: [37], Font: STIXTwoText, Size: 9.962639808654785
Text: Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hofman, and Razvan Pascanu. “Improving the Gating Mech-, Font: STIXTwoText, Size: 9.962639808654785
Text: anism of Recurrent Neural Networks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2020., Font: STIXTwoText, Size: 9.962639808654785
Text: [38], Font: STIXTwoText, Size: 9.962639808654785
Text: Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. “On the Parameterization and Initialization of Diag-, Font: STIXTwoText, Size: 9.962639808654785
Text: onal State Space Models”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2022., Font: STIXTwoText, Size: 9.962639808654785
Text: [39], Font: STIXTwoText, Size: 9.962639808654785
Text: Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. “Combining Recur-, Font: STIXTwoText, Size: 9.962639808654785
Text: rent, Convolutional, and Continuous-time Models with the Linear State Space Layer”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2021., Font: STIXTwoText, Size: 9.962639808654785
Text: [40], Font: STIXTwoText, Size: 9.962639808654785
Text: Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Ré. “How to Train Your HIPPO: State, Font: STIXTwoText, Size: 9.962639808654785
Text: Space Models with Generalized Basis Projections”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [41], Font: STIXTwoText, Size: 9.962639808654785
Text: Ankit Gupta, Albert Gu, and Jonathan Berant. “Diagonal State Spaces are as Efective as Structured State, Font: STIXTwoText, Size: 9.962639808654785
Text: Spaces”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  35 (2022), pp. 22982–22994., Font: STIXTwoText, Size: 9.962639808654785
Text: [42], Font: STIXTwoText, Size: 9.962639808654785
Text: David Ha, Andrew Dai, and Quoc V. Le. “HyperNetworks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Rep-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: resentations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2017., Font: STIXTwoText, Size: 9.962639808654785
Text: [43], Font: STIXTwoText, Size: 9.962639808654785
Text: Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. “Dream to Control: Learning Behav-, Font: STIXTwoText, Size: 9.962639808654785
Text: iors by Latent Imagination”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2020., Font: STIXTwoText, Size: 9.962639808654785
Text: [44], Font: STIXTwoText, Size: 9.962639808654785
Text: Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus., Font: STIXTwoText, Size: 9.962639808654785
Text: “Liquid Structural State-Space Models”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text: 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [45], Font: STIXTwoText, Size: 9.962639808654785
Text: Mikael Henaf, Arthur Szlam, and Yann LeCun. “Recurrent Orthogonal Networks and Long-Memory Tasks”., Font: STIXTwoText, Size: 9.962639808654785
Text: In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2016., Font: STIXTwoText, Size: 9.962639808654785
Text: [46], Font: STIXTwoText, Size: 9.962639808654785
Text: Dan Hendrycks and Kevin Gimpel. “Gaussian Error Linear Units (GELUs)”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:1606.08415, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: (2016)., Font: STIXTwoText, Size: 9.962639808654785
Text: [47], Font: STIXTwoText, Size: 9.962639808654785
Text: Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Neural Computation, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  9.8 (1997),, Font: STIXTwoText, Size: 9.962639808654785
Text: pp. 1735–1780., Font: STIXTwoText, Size: 9.962639808654785
Text: [48], Font: STIXTwoText, Size: 9.962639808654785
Text: Jordan Hofmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego, Font: STIXTwoText, Size: 9.962639808654785
Text: de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. “An Empirical Analysis of Compute-, Font: STIXTwoText, Size: 9.962639808654785
Text: Optimal Large Language Model Training”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  35, Font: STIXTwoText, Size: 9.962639808654785
Text: (2022), pp. 30016–30030., Font: STIXTwoText, Size: 9.962639808654785
Text: [49], Font: STIXTwoText, Size: 9.962639808654785
Text: Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. “Transformer Quality in Linear Time”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The Interna-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: tional Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2022, pp. 9099–9117., Font: STIXTwoText, Size: 9.962639808654785
Text: [50], Font: STIXTwoText, Size: 9.962639808654785
Text: Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. “Deep, Font: STIXTwoText, Size: 9.962639808654785
Text: Learning for Time Series Classifcation: A Review”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Data Mining and Knowledge Discovery, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  33.4 (2019),, Font: STIXTwoText, Size: 9.962639808654785
Text: pp. 917–963., Font: STIXTwoText, Size: 9.962639808654785
Text: [51], Font: STIXTwoText, Size: 9.962639808654785
Text: Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefer. “Data Movement is All You Need:, Font: STIXTwoText, Size: 9.962639808654785
Text: A Case Study on Optimizing Transformers”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Proceedings of Machine Learning and Systems, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  3 (2021), pp. 711–, Font: STIXTwoText, Size: 9.962639808654785
Text: 732., Font: STIXTwoText, Size: 9.962639808654785
Text: [52], Font: STIXTwoText, Size: 9.962639808654785
Text: Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. “Gated, Font: STIXTwoText, Size: 9.962639808654785
Text: Orthogonal Recurrent Units: On Learning to Forget”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Neural Computation, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  31.4 (2019), pp. 765–783., Font: STIXTwoText, Size: 9.962639808654785
Text: [53], Font: STIXTwoText, Size: 9.962639808654785
Text: Rudolph Emil Kalman. “A New Approach to Linear Filtering and Prediction Problems”. In: (1960)., Font: STIXTwoText, Size: 9.962639808654785
Text: 20, Font: STIXTwoText, Size: 9.962639808654785
Text: [54], Font: STIXTwoText, Size: 9.962639808654785
Text: Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast, Font: STIXTwoText, Size: 9.962639808654785
Text: Autoregressive Transformers with Linear Attention”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  International Conference on Machine Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR., Font: STIXTwoText, Size: 9.962639808654785
Text: 2020, pp. 5156–5165., Font: STIXTwoText, Size: 9.962639808654785
Text: [55], Font: STIXTwoText, Size: 9.962639808654785
Text: Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. “DifWave: A Versatile Difusion Model, Font: STIXTwoText, Size: 9.962639808654785
Text: for Audio Synthesis”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  International Conference on Learning Representations, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2021., Font: STIXTwoText, Size: 9.962639808654785
Text: [56], Font: STIXTwoText, Size: 9.962639808654785
Text: Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. “Time-Parameterized Convolutional Neu-, Font: STIXTwoText, Size: 9.962639808654785
Text: ral Networks for Irregularly Sampled Time Series”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2308.03210, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [57], Font: STIXTwoText, Size: 9.962639808654785
Text: Alex Krizhevsky, Ilya Sutskever, and Geofrey E Hinton. “ImageNet Classifcation with Deep Convolutional, Font: STIXTwoText, Size: 9.962639808654785
Text: Neural Networks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  25 (2012)., Font: STIXTwoText, Size: 9.962639808654785
Text: [58], Font: STIXTwoText, Size: 9.962639808654785
Text: Tao Lei. “When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2021, pp. 7633–7648., Font: STIXTwoText, Size: 9.962639808654785
Text: [59], Font: STIXTwoText, Size: 9.962639808654785
Text: Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. “Simple Recurrent Units for Highly Parallelizable, Font: STIXTwoText, Size: 9.962639808654785
Text: Recurrence”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:1709.02755, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2017)., Font: STIXTwoText, Size: 9.962639808654785
Text: [60], Font: STIXTwoText, Size: 9.962639808654785
Text: Mario Lezcano-Casado and David Martínez-Rubio. “Cheap Orthogonal Constraints in Neural Networks: A, Font: STIXTwoText, Size: 9.962639808654785
Text: Simple Parametrization of the Orthogonal and Unitary Group”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Machine, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2019., Font: STIXTwoText, Size: 9.962639808654785
Text: [61], Font: STIXTwoText, Size: 9.962639808654785
Text: Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. “What Makes Convolutional Models, Font: STIXTwoText, Size: 9.962639808654785
Text: Great on Long Sequence Modeling?” In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [62], Font: STIXTwoText, Size: 9.962639808654785
Text: Vasileios Lioutas and Yuhong Guo. “Time-aware Large Kernel Convolutions”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2020, pp. 6172–6183., Font: STIXTwoText, Size: 9.962639808654785
Text: [63], Font: STIXTwoText, Size: 9.962639808654785
Text: Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behba-, Font: STIXTwoText, Size: 9.962639808654785
Text: hani. “Structured State Space Models for In-Context Reinforcement Learning”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Informa-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: tion Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [64], Font: STIXTwoText, Size: 9.962639808654785
Text: Shahar Lutati, Itamar Zimerman, and Lior Wolf. “Focus Your Attention (with Adaptive IIR Filters)”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: preprint arXiv:2305.14952, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [65], Font: STIXTwoText, Size: 9.962639808654785
Text: Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke, Font: STIXTwoText, Size: 9.962639808654785
Text: Zettlemoyer. “Mega: Moving Average Equipped Gated Attention”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [66], Font: STIXTwoText, Size: 9.962639808654785
Text: Eric Martin and Chris Cundy. “Parallelizing Linear Recurrent Neural Nets Over Sequence Length”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2018., Font: STIXTwoText, Size: 9.962639808654785
Text: [67], Font: STIXTwoText, Size: 9.962639808654785
Text: Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville,, Font: STIXTwoText, Size: 9.962639808654785
Text: and Yoshua Bengio. “SampleRNN: An Unconditional End-to-End Neural Audio Generation Model”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2017., Font: STIXTwoText, Size: 9.962639808654785
Text: [68], Font: STIXTwoText, Size: 9.962639808654785
Text: Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. “Long Range Language Modeling via, Font: STIXTwoText, Size: 9.962639808654785
Text: Gated State Spaces”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [69], Font: STIXTwoText, Size: 9.962639808654785
Text: Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. “Efcient Orthogonal Parametri-, Font: STIXTwoText, Size: 9.962639808654785
Text: sation of Recurrent Neural Networks using Householder Refections”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  International Conference on Machine, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2017, pp. 2401–2409., Font: STIXTwoText, Size: 9.962639808654785
Text: [70], Font: STIXTwoText, Size: 9.962639808654785
Text: Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Ré., Font: STIXTwoText, Size: 9.962639808654785
Text: “S4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2022., Font: STIXTwoText, Size: 9.962639808654785
Text: [71], Font: STIXTwoText, Size: 9.962639808654785
Text: Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Pa-, Font: STIXTwoText, Size: 9.962639808654785
Text: tel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. “HyenaDNA: Long-range Genomic Sequence, Font: STIXTwoText, Size: 9.962639808654785
Text: Modeling at Single Nucleotide Resolution”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text: 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [72], Font: STIXTwoText, Size: 9.962639808654785
Text: Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,, Font: STIXTwoText, Size: 9.962639808654785
Text: Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfeld-Dodds, Danny, Font: STIXTwoText, Size: 9.962639808654785
Text: Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom, Font: STIXTwoText, Size: 9.962639808654785
Text: Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. “In-context Learning and Induction Heads”., Font: STIXTwoText, Size: 9.962639808654785
Text: In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Transformer Circuits Thread, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-, Font: STIXTwoText, Size: 9.962639808654785
Text: heads/index.html., Font: STIXTwoText, Size: 9.962639808654785
Text: [73], Font: STIXTwoText, Size: 9.962639808654785
Text: Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalch-, Font: STIXTwoText, Size: 9.962639808654785
Text: brenner, Andrew Senior, and Koray Kavukcuoglu. “WaveNet: A Generative Model for Raw Audio”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: preprint arXiv:1609.03499, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2016)., Font: STIXTwoText, Size: 9.962639808654785
Text: 21, Font: STIXTwoText, Size: 9.962639808654785
Text: [74], Font: STIXTwoText, Size: 9.962639808654785
Text: Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and So-, Font: STIXTwoText, Size: 9.962639808654785
Text: ham De. “Resurrecting Recurrent Neural Networks for Long Sequences”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [75], Font: STIXTwoText, Size: 9.962639808654785
Text: Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Rafaella Bernardi, Sandro Pezzelle,, Font: STIXTwoText, Size: 9.962639808654785
Text: Marco Baroni, Gemma Boleda, and Raquel Fernández. “The LAMBADA Dataset: Word Prediction Requiring, Font: STIXTwoText, Size: 9.962639808654785
Text: a Broad Discourse Context”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Proceedings of the 54th Annual Meeting of the Association for Computational, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Linguistics, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2016, pp. 1525–1534., Font: STIXTwoText, Size: 9.962639808654785
Text: [76], Font: STIXTwoText, Size: 9.962639808654785
Text: Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. “On the Difculty of Training Recurrent Neural Net-, Font: STIXTwoText, Size: 9.962639808654785
Text: works”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  International Conference on Machine Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2013, pp. 1310–1318., Font: STIXTwoText, Size: 9.962639808654785
Text: [77], Font: STIXTwoText, Size: 9.962639808654785
Text: Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael, Font: STIXTwoText, Size: 9.962639808654785
Text: Chung, Matteo Grella, Kranthi Kiran GV, et al. “RWKV: Reinventing RNNs for the Transformer Era”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: preprint arXiv:2305.13048, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [78], Font: STIXTwoText, Size: 9.962639808654785
Text: Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. “Random, Font: STIXTwoText, Size: 9.962639808654785
Text: Feature Attention”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2021., Font: STIXTwoText, Size: 9.962639808654785
Text: [79], Font: STIXTwoText, Size: 9.962639808654785
Text: Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano, Font: STIXTwoText, Size: 9.962639808654785
Text: Ermon, and Christopher Ré. “Hyena Hierarchy: Towards Larger Convolutional Language Models”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: International Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [80], Font: STIXTwoText, Size: 9.962639808654785
Text: Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and, Font: STIXTwoText, Size: 9.962639808654785
Text: Yiran Zhong. “Toeplitz Neural Network for Sequence Modeling”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [81], Font: STIXTwoText, Size: 9.962639808654785
Text: Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. “The devil, Font: STIXTwoText, Size: 9.962639808654785
Text: in linear transformer”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2210.10340, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2022)., Font: STIXTwoText, Size: 9.962639808654785
Text: [82], Font: STIXTwoText, Size: 9.962639808654785
Text: Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and, Font: STIXTwoText, Size: 9.962639808654785
Text: Yiran Zhong. “CosFormer: Rethinking Softmax in Attention”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2022., Font: STIXTwoText, Size: 9.962639808654785
Text: [83], Font: STIXTwoText, Size: 9.962639808654785
Text: Ali Rahimi and Benjamin Recht. “Random features for large-scale kernel machines”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in neural, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: information processing systems, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  20 (2007)., Font: STIXTwoText, Size: 9.962639808654785
Text: [84], Font: STIXTwoText, Size: 9.962639808654785
Text: Prajit Ramachandran, Barret Zoph, and Quoc V Le. “Swish: A Self-gated Activation Function”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: arXiv:1710.05941, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  7.1 (2017), p. 5., Font: STIXTwoText, Size: 9.962639808654785
Text: [85], Font: STIXTwoText, Size: 9.962639808654785
Text: David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. “CKConv: Con-, Font: STIXTwoText, Size: 9.962639808654785
Text: tinuous Kernel Convolution For Sequential Data”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2102.02611, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2021)., Font: STIXTwoText, Size: 9.962639808654785
Text: [86], Font: STIXTwoText, Size: 9.962639808654785
Text: Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. “Winogrande: An Adversarial Wino-, Font: STIXTwoText, Size: 9.962639808654785
Text: grad Schema Challenge at Scale”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Communications of the ACM, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  64.9 (2021), pp. 99–106., Font: STIXTwoText, Size: 9.962639808654785
Text: [87], Font: STIXTwoText, Size: 9.962639808654785
Text: George Saon, Ankit Gupta, and Xiaodong Cui. “Diagonal State Space Augmented Transformers for Speech, Font: STIXTwoText, Size: 9.962639808654785
Text: Recognition”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: (ICASSP), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . IEEE. 2023, pp. 1–5., Font: STIXTwoText, Size: 9.962639808654785
Text: [88], Font: STIXTwoText, Size: 9.962639808654785
Text: Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. “Linear Transformers are Secretly Fast Weight Program-, Font: STIXTwoText, Size: 9.962639808654785
Text: mers”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2021, pp. 9355–9366., Font: STIXTwoText, Size: 9.962639808654785
Text: [89], Font: STIXTwoText, Size: 9.962639808654785
Text: Noam Shazeer. “GLU Variants Improve Transformer”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2002.05202, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2020)., Font: STIXTwoText, Size: 9.962639808654785
Text: [90], Font: STIXTwoText, Size: 9.962639808654785
Text: Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and, Font: STIXTwoText, Size: 9.962639808654785
Text: Denny Zhou. “Large Language Models can be Easily Distracted by Irrelevant Context”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2023, pp. 31210–31227., Font: STIXTwoText, Size: 9.962639808654785
Text: [91], Font: STIXTwoText, Size: 9.962639808654785
Text: Jiaxin Shi, Ke Alexander Wang, and Emily Fox. “Sequence Modeling with Multiresolution Convolutional Mem-, Font: STIXTwoText, Size: 9.962639808654785
Text: ory”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Machine Learning (ICML), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2023, pp. 31312–31327., Font: STIXTwoText, Size: 9.962639808654785
Text: [92], Font: STIXTwoText, Size: 9.962639808654785
Text: Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. “Simplifed State Space Layers for Sequence, Font: STIXTwoText, Size: 9.962639808654785
Text: Modeling”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [93], Font: STIXTwoText, Size: 9.962639808654785
Text: Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. “Roformer: Enhanced Trans-, Font: STIXTwoText, Size: 9.962639808654785
Text: former with Rotary Position Embedding”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2104.09864, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2021)., Font: STIXTwoText, Size: 9.962639808654785
Text: [94], Font: STIXTwoText, Size: 9.962639808654785
Text: Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei., Font: STIXTwoText, Size: 9.962639808654785
Text: “Retentive network: A successor to transformer for large language models”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2307.08621, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [95], Font: STIXTwoText, Size: 9.962639808654785
Text: Ilya Sutskever, Oriol Vinyals, and Quoc V Le. “Sequence to Sequence Learning with Neural Networks”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text: Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  27 (2014)., Font: STIXTwoText, Size: 9.962639808654785
Text: 22, Font: STIXTwoText, Size: 9.962639808654785
Text: [96], Font: STIXTwoText, Size: 9.962639808654785
Text: Corentin Tallec and Yann Ollivier. “Can Recurrent Neural Networks Warp Time?” In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Con-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2018., Font: STIXTwoText, Size: 9.962639808654785
Text: [97], Font: STIXTwoText, Size: 9.962639808654785
Text: Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Se-, Font: STIXTwoText, Size: 9.962639808654785
Text: bastian Ruder, and Donald Metzler. “Long Range Arena: A Benchmark for Efcient Transformers”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Inter-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: national Conference on Learning Representations (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2021., Font: STIXTwoText, Size: 9.962639808654785
Text: [98], Font: STIXTwoText, Size: 9.962639808654785
Text: Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. “Efcient Transformers: A Survey”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  ACM Com-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: puting Surveys, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  55.6 (2022), pp. 1–28., Font: STIXTwoText, Size: 9.962639808654785
Text: [99], Font: STIXTwoText, Size: 9.962639808654785
Text: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-, Font: STIXTwoText, Size: 9.962639808654785
Text: tiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. “Llama: Open and Efcient Foundation Language, Font: STIXTwoText, Size: 9.962639808654785
Text: Models”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2302.13971, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2023)., Font: STIXTwoText, Size: 9.962639808654785
Text: [100], Font: STIXTwoText, Size: 9.962639808654785
Text: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,, Font: STIXTwoText, Size: 9.962639808654785
Text: and Illia Polosukhin. “Attention Is All You Need”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  AdvancesinNeuralInformationProcessingSystems(NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text: 2017., Font: STIXTwoText, Size: 9.962639808654785
Text: [101], Font: STIXTwoText, Size: 9.962639808654785
Text: Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. “On Orthogonality and Learning Recur-, Font: STIXTwoText, Size: 9.962639808654785
Text: rent Networks with Long Term Dependencies”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  International Conference on Machine Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2017,, Font: STIXTwoText, Size: 9.962639808654785
Text: pp. 3570–3578., Font: STIXTwoText, Size: 9.962639808654785
Text: [102], Font: STIXTwoText, Size: 9.962639808654785
Text: Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Rafay Hamid. “Selective, Font: STIXTwoText, Size: 9.962639808654785
Text: Structured State-Spaces for Long-form Video Understanding”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Proceedings of the IEEE/CVF Conference on, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: Computer Vision and Pattern Recognition, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023, pp. 6387–6397., Font: STIXTwoText, Size: 9.962639808654785
Text: [103], Font: STIXTwoText, Size: 9.962639808654785
Text: Pete Warden. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  ArXiv, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  abs/1804.03209, Font: STIXTwoText, Size: 9.962639808654785
Text: (2018)., Font: STIXTwoText, Size: 9.962639808654785
Text: [104], Font: STIXTwoText, Size: 9.962639808654785
Text: Samuel Williams, Andrew Waterman, and David Patterson. “Roofine: An Insightful Visual Performance Model, Font: STIXTwoText, Size: 9.962639808654785
Text: for Multicore Architectures”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Communications of the ACM, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  52.4 (2009), pp. 65–76., Font: STIXTwoText, Size: 9.962639808654785
Text: [105], Font: STIXTwoText, Size: 9.962639808654785
Text: Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. “CondConv: Conditionally Parameterized Con-, Font: STIXTwoText, Size: 9.962639808654785
Text: volutions for Efcient Inference”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Advances in Neural Information Processing Systems (NeurIPS), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  32 (2019)., Font: STIXTwoText, Size: 9.962639808654785
Text: [106], Font: STIXTwoText, Size: 9.962639808654785
Text: Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. “HellaSwag: Can a Machine Really, Font: STIXTwoText, Size: 9.962639808654785
Text: Finish Your Sentence?” In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: tics, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2019., Font: STIXTwoText, Size: 9.962639808654785
Text: [107], Font: STIXTwoText, Size: 9.962639808654785
Text: Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind., Font: STIXTwoText, Size: 9.962639808654785
Text: “An Attention Free Transformer”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2105.14103, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2021)., Font: STIXTwoText, Size: 9.962639808654785
Text: [108], Font: STIXTwoText, Size: 9.962639808654785
Text: Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Ré. “Efectively Modeling, Font: STIXTwoText, Size: 9.962639808654785
Text: Time Series with Simple Discrete State Spaces”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  The International Conference on Learning Representations, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: (ICLR), Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . 2023., Font: STIXTwoText, Size: 9.962639808654785
Text: [109], Font: STIXTwoText, Size: 9.962639808654785
Text: Lin Zheng, Chong Wang, and Lingpeng Kong. “Linear complexity randomized self-attention mechanism”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text: International Conference on Machine Learning, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: . PMLR. 2022, pp. 27011–27041., Font: STIXTwoText, Size: 9.962639808654785
Text: [110], Font: STIXTwoText, Size: 9.962639808654785
Text: Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. “Efcient, Font: STIXTwoText, Size: 9.962639808654785
Text: Long Sequence Modeling via State Space Augmented Transformer”. In:, Font: STIXTwoText, Size: 9.962639808654785
Text:  arXiv preprint arXiv:2212.08136, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  (2022)., Font: STIXTwoText, Size: 9.962639808654785
Text: 23, Font: STIXTwoText, Size: 9.962639808654785
Text: A, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Discussion: Selection Mechanism, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Our selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence., Font: SFRM1000, Size: 9.862509727478027
Text: It can also be viewed as related to “fast weights” (J. Ba et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2016, Font: SFRM1000, Size: 10.061773300170898
Text: ), which connects classical RNNs with the, Font: SFRM1000, Size: 10.061773300170898
Text: mechanism of linear attention (Schlag, Irie, and Schmidhuber, Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ). However, we believe that it is a distinct, Font: SFRM1000, Size: 10.061773300170898
Text: concept that is worth clarifying., Font: SFRM1000, Size: 9.962639808654785
Text: Gating., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and, Font: SFRM1000, Size: 10.061773300170898
Text: Schmidhuber, Font: SFRM1000, Size: 9.882617950439453
Text:  1997, Font: SFRM1000, Size: 9.882617950439453
Text: ) and GRU (J. Chung et al., Font: SFRM1000, Size: 9.882617950439453
Text:  2014, Font: SFRM1000, Size: 9.882617950439453
Text: ), or the gated equation, Font: SFRM1000, Size: 9.882617950439453
Text:  (, Font: STIXTwoText, Size: 9.962639808654785
Text: 5, Font: STIXTwoText, Size: 9.962639808654785
Text: ), Font: STIXTwoText, Size: 9.962639808654785
Text: n Theorem, Font: SFRM1000, Size: 9.882617950439453
Text:  1, Font: SFRM1000, Size: 9.882617950439453
Text: . This was interpreted, Font: SFRM1000, Size: 9.882617950439453
Text: as a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular,, Font: SFRM1000, Size: 9.932706832885742
Text: this aﬀects the propagation of signal through time and causes inputs to interact along the sequence length, Font: SFRM1000, Size: 10.061773300170898
Text: dimension., Font: SFRM1000, Size: 9.962639808654785
Text: However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative, Font: SFRM1000, Size: 10.061773300170898
Text: interaction (often with an activation function). For example, elementwise multiplicative components of neural, Font: SFRM1000, Size: 10.061773300170898
Text: network architectures (that do not interact along sequence length) are now commonly referred to as gated, Font: SFRM1000, Size: 10.061773300170898
Text: architectures (Hua et al., Font: SFRM1000, Size: 9.957657814025879
Text:  2022, Font: SFRM1000, Size: 9.957657814025879
Text: ; Mehta et al., Font: SFRM1000, Size: 9.957657814025879
Text:  2023, Font: SFRM1000, Size: 9.957657814025879
Text: ), despite a very diﬀerent meaning than the original RNN sense., Font: SFRM1000, Size: 9.957657814025879
Text: Thus we believe the original concept of RNN gating versus the popular usage of multiplicative gating actually, Font: SFRM1000, Size: 10.056839942932129
Text: have a very diﬀerent semantic meaning., Font: SFRM1000, Size: 9.962639808654785
Text: Hypernetworks., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Hypernetworks refer to neural networks whose parameters are themselves generated by smaller, Font: SFRM1000, Size: 9.862509727478027
Text: neural networks. The original idea (Ha, Dai, and Quoc V. Le, Font: SFRM1000, Size: 10.061773300170898
Text:  2017, Font: SFRM1000, Size: 10.061773300170898
Text: ) used it in a narrow sense to deﬁne a large, Font: SFRM1000, Size: 10.061773300170898
Text: RNN whose recurrent parameters are generated by a smaller RNN., Font: SFRM1000, Size: 9.962639808654785
Text: Data-dependence., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Similar to hypernetworks, data-dependence can refer to any notion where some parameters, Font: SFRM1000, Size: 9.942694664001465
Text: of the model depend on the data (Poli et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Example: GLU Activation., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: To illustrate the issues with these concepts, consider a simple diagonal linear, Font: SFRM1000, Size: 10.061773300170898
Text: layer, Font: SFRM1000, Size: 10.061773300170898
Text:  푦 =, Font: STIXTwoMath, Size: 9.962639808654785
Text:  D, Font: CMMIB10, Size: 9.962639808654785
Text: 푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: , where, Font: SFRM1000, Size: 10.061773300170898
Text:  D, Font: CMMIB10, Size: 9.962639808654785
Text:  is a diagonal weight parameter. Now suppose that, Font: SFRM1000, Size: 10.061773300170898
Text:  D, Font: CMMIB10, Size: 9.962639808654785
Text:  is itself generated from a linear, Font: SFRM1000, Size: 10.061773300170898
Text: transformation of, Font: SFRM1000, Size: 9.922708511352539
Text:  푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: , with an optional nonlinearity:, Font: SFRM1000, Size: 9.922708511352539
Text:  D, Font: CMMIB10, Size: 9.962639808654785
Text:  = 휎(, Font: STIXTwoMath, Size: 9.962639808654785
Text: W, Font: CMMIB10, Size: 9.962639808654785
Text:  푥), Font: STIXTwoMath, Size: 9.962639808654785
Text: . Since it is diagonal, the multiplication becomes, Font: SFRM1000, Size: 9.922708511352539
Text: an elementwise product:, Font: SFRM1000, Size: 9.962639808654785
Text:  푦 = 휎(, Font: STIXTwoMath, Size: 9.962639808654785
Text: W, Font: CMMIB10, Size: 9.962639808654785
Text:  푥)◦푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: This is a rather trivial transformation, yet it technically satisﬁes the common meanings of gating (since it has a, Font: SFRM1000, Size: 9.987515449523926
Text: multiplicative “branch”), hypernetworks (since the parameter, Font: SFRM1000, Size: 9.862509727478027
Text:  D, Font: CMMIB10, Size: 9.962639808654785
Text:  is generated by another layer), and data-dependent, Font: SFRM1000, Size: 9.862509727478027
Text: (since, Font: SFRM1000, Size: 10.042024612426758
Text:  D, Font: CMMIB10, Size: 9.962639808654785
Text:  depends on the data, Font: SFRM1000, Size: 10.042024612426758
Text:  푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: ). However, this in fact simply deﬁnes a GLU function, which is so simple that, Font: SFRM1000, Size: 10.042024612426758
Text: it is often considered just an activation function (Dauphin et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2017, Font: SFRM1000, Size: 10.061773300170898
Text: ; Shazeer, Font: SFRM1000, Size: 10.061773300170898
Text:  2020, Font: SFRM1000, Size: 10.061773300170898
Text: ) instead of a meaningful, Font: SFRM1000, Size: 10.061773300170898
Text: layer., Font: SFRM1000, Size: 9.962639808654785
Text: Selection., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Thus, while selection mechanisms could be considered a special case of ideas such as architectural, Font: SFRM1000, Size: 10.061773300170898
Text: gating, hypernetworks, or data-dependence, so can an enormous range of other constructions—essentially anything, Font: SFRM1000, Size: 9.862509727478027
Text: with a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio, Font: SFRM1000, Size: 9.89767074584961
Text:  2015, Font: SFRM1000, Size: 9.89767074584961
Text: ; Vaswani et al., Font: SFRM1000, Size: 9.89767074584961
Text: 2017, Font: SFRM1000, Size: 9.962639808654785
Text: ) as well—and we ﬁnd it uninformative to think of them as such., Font: SFRM1000, Size: 9.962639808654785
Text: Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case, Font: SFRM1000, Size: 9.977572441101074
Text: (Theorem, Font: SFRM1000, Size: 9.862509727478027
Text:  1, Font: SFRM1000, Size: 9.862509727478027
Text: ) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization, Font: SFRM1000, Size: 9.862509727478027
Text: of, Font: SFRM1000, Size: 10.032135963439941
Text:  ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  (Funahashi and Nakamura, Font: SFRM1000, Size: 10.032135963439941
Text:  1993, Font: SFRM1000, Size: 10.032135963439941
Text: ; Gu, Dao, et al., Font: SFRM1000, Size: 10.032135963439941
Text:  2020, Font: SFRM1000, Size: 10.032135963439941
Text: ; Tallec and Ollivier, Font: SFRM1000, Size: 10.032135963439941
Text:  2018, Font: SFRM1000, Size: 10.032135963439941
Text: ). We also eschew the term, Font: SFRM1000, Size: 10.032135963439941
Text: “gating” in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to, Font: SFRM1000, Size: 10.0123291015625
Text: the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence, Font: SFRM1000, Size: 10.061773300170898
Text: length (Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.1, Font: SFRM1000, Size: 10.061773300170898
Text: ). Beyond selective SSMs and gated RNNs, other examples may include input-dependent, Font: SFRM1000, Size: 10.061773300170898
Text: convolutions (Kosma, Nikolentzos, and Vazirgiannis, Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ; Lioutas and Guo, Font: SFRM1000, Size: 10.061773300170898
Text:  2020, Font: SFRM1000, Size: 10.061773300170898
Text: ; Lutati, Zimerman, and Wolf, Font: SFRM1000, Size: 10.061773300170898
Text: 2023, Font: SFRM1000, Size: 9.962639808654785
Text: ; Yang et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2019, Font: SFRM1000, Size: 9.962639808654785
Text: ) and even attention., Font: SFRM1000, Size: 9.962639808654785
Text: 24, Font: STIXTwoText, Size: 9.962639808654785
Text: B, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Related Work, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: We overview several prior works related to our methods. We mention that some of the most closely related models, Font: SFRM1000, Size: 9.862509727478027
Text: include recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet,, Font: SFRM1000, Size: 9.957657814025879
Text: and RWKV., Font: SFRM1000, Size: 9.962639808654785
Text: B.1, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: S4 Variants and Derivatives, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: We describe a brief overview of some structured SSMs from past work, particularly those that have a relation to, Font: SFRM1000, Size: 9.957657814025879
Text: our method., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  S4 (Gu, Goel, and Ré, Font: SFRM1000, Size: 10.061773300170898
Text:  2022, Font: SFRM1000, Size: 10.061773300170898
Text: ; Gu, Johnson, Goel, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ) introduced the ﬁrst structured SSM, describing, Font: SFRM1000, Size: 10.061773300170898
Text: diagonal structure and diagonal plus low-rank (DPLR). It focused on eﬃcient convolutional algorithms for, Font: SFRM1000, Size: 10.061773300170898
Text: DPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2020, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  DSS (Gupta, Gu, and Berant, Font: SFRM1000, Size: 9.957657814025879
Text:  2022, Font: SFRM1000, Size: 9.957657814025879
Text: ) ﬁrst discovered the empirical eﬀectiveness of diagonal structured SSMs by, Font: SFRM1000, Size: 9.957657814025879
Text: approximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2022, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  S5 (Smith, Warrington, and Linderman, Font: SFRM1000, Size: 9.952672004699707
Text:  2023, Font: SFRM1000, Size: 9.952672004699707
Text: ) independently discovered the diagonal SSM approximation, and, Font: SFRM1000, Size: 9.952672004699707
Text: is the ﬁrst S4 model to be computed recurrently with the parallel scan. However, this required lowering the, Font: SFRM1000, Size: 10.061773300170898
Text: eﬀective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input, Font: SFRM1000, Size: 9.8876371383667
Text: single-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but diﬀers, Font: SFRM1000, Size: 9.962639808654785
Text: by (i) keeping the SISO dimensions, which provides a larger eﬀective recurrent state, (ii) using a hardware-aware, Font: SFRM1000, Size: 9.862509727478027
Text: algorithm to overcome the computation issue, (iii) adding the selection mechanism., Font: SFRM1000, Size: 9.962639808654785
Text: Lu et al. (, Font: SFRM1000, Size: 9.89767074584961
Text: 2023, Font: SFRM1000, Size: 9.89767074584961
Text: ) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories., Font: SFRM1000, Size: 9.89767074584961
Text: Their mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where, Font: SFRM1000, Size: 10.061773300170898
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  is, Font: SFRM1000, Size: 10.061773300170898
Text: manually set to, Font: SFRM1000, Size: 10.061773300170898
Text:  0, Font: STIXTwoMath, Size: 9.962639808654785
Text: , instead of our learnable mechanism that depends on the input. It would be interesting to, Font: SFRM1000, Size: 10.061773300170898
Text: apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its, Font: SFRM1000, Size: 10.042024612426758
Text: state on episode boundaries., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Mega (Ma et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ) introduced a simpliﬁcation of S4 to be real- instead of complex- valued, giving it an, Font: SFRM1000, Size: 10.061773300170898
Text: interpretation of being an exponential moving average (EMA). They additionally make an interesting connection, Font: SFRM1000, Size: 9.862509727478027
Text: of the discretization step of SSMs to an EMA damping term. Contrary to ﬁndings in the original S4 papers, this, Font: SFRM1000, Size: 9.862509727478027
Text: was the ﬁrst model to show that real-valued SSMs are empirically eﬀective in certain settings or when combined, Font: SFRM1000, Size: 9.862509727478027
Text: with diﬀerent architectural components., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Liquid S4 (Hasani et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ) is also motivated by augmenting S4 with an input-dependent state transition., Font: SFRM1000, Size: 10.061773300170898
Text: From this perspective it shares similarity to selection mechanisms, although in a limited form which is still, Font: SFRM1000, Size: 10.061773300170898
Text: computed convolutionally and close to LTI., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  SGConv (Y. Li et al., Font: SFRM1000, Size: 10.017284393310547
Text:  2023, Font: SFRM1000, Size: 10.017284393310547
Text: ), Hyena (Poli et al., Font: SFRM1000, Size: 10.017284393310547
Text:  2023, Font: SFRM1000, Size: 10.017284393310547
Text: ), LongConv (Fu et al., Font: SFRM1000, Size: 10.017284393310547
Text:  2023, Font: SFRM1000, Size: 10.017284393310547
Text: ), MultiresConv (J. Shi, K. A., Font: SFRM1000, Size: 10.017284393310547
Text: Wang, and Fox, Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ), and Toeplitz Neural Network (Qin, Han, W. Sun, He, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ) all focus on the, Font: SFRM1000, Size: 10.061773300170898
Text: convolutional representation of S4 and create global or long convolution kernels with diﬀerent parameterizations., Font: SFRM1000, Size: 9.862509727478027
Text: However, these methods cannot do fast autoregressive inference directly., Font: SFRM1000, Size: 9.962639808654785
Text: Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and, Font: SFRM1000, Size: 10.0123291015625
Text: usually strictly LTI (linear time invariant)., Font: SFRM1000, Size: 9.962639808654785
Text: B.2, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: SSM Architectures, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: We use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures, Font: SFRM1000, Size: 10.032135963439941
Text: incorporating one of the previous SSMs as a black box layer., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  GSS (Mehta et al., Font: SFRM1000, Size: 9.87256908416748
Text:  2023, Font: SFRM1000, Size: 9.87256908416748
Text: ) was the ﬁrst gated neural network architecture incorporating SSMs. It is motivated by, Font: SFRM1000, Size: 9.87256908416748
Text: the gated attention unit (GAU) of Hua et al. (, Font: SFRM1000, Size: 9.862509727478027
Text: 2022, Font: SFRM1000, Size: 9.862509727478027
Text: ) and looks quite similar to our block, except with additional, Font: SFRM1000, Size: 9.862509727478027
Text: projections. Most importantly, its projection contracts the model dimension to reduce the state size of the, Font: SFRM1000, Size: 10.061773300170898
Text: SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in, Font: SFRM1000, Size: 10.032135963439941
Text: Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3.1, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: 25, Font: STIXTwoText, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Mega (Ma et al., Font: SFRM1000, Size: 9.892655372619629
Text:  2023, Font: SFRM1000, Size: 9.892655372619629
Text: ) combined the EMA simpliﬁcation of S4 described above into a hybrid architecture using, Font: SFRM1000, Size: 9.892655372619629
Text: an eﬃcient attention approximation., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  H3 (Dao, Fu, Saab, et al., Font: SFRM1000, Size: 9.882617950439453
Text:  2023, Font: SFRM1000, Size: 9.882617950439453
Text: ) is motivated by combining S4 with linear attention (Katharopoulos et al., Font: SFRM1000, Size: 9.882617950439453
Text:  2020, Font: SFRM1000, Size: 9.882617950439453
Text: )., Font: SFRM1000, Size: 9.882617950439453
Text: It is the ﬁrst to generalize this formulation of linear attention to more general recurrences, which is also the, Font: SFRM1000, Size: 10.061773300170898
Text: basis of later architectures., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Selective S4 (J. Wang et al., Font: SFRM1000, Size: 9.882617950439453
Text:  2023, Font: SFRM1000, Size: 9.882617950439453
Text: ) incorporates S4 as a black box to generate a binary mask which is multiplied, Font: SFRM1000, Size: 9.882617950439453
Text: on the input. While sharing the “selection” name, we consider this an architectural modiﬁcation that is closer to, Font: SFRM1000, Size: 9.862509727478027
Text: architectural gating than a selection mechanism (Appendix, Font: SFRM1000, Size: 9.937702178955078
Text:  A, Font: SFRM1000, Size: 9.937702178955078
Text: ). For example, we hypothesize that it would not, Font: SFRM1000, Size: 9.937702178955078
Text: solve the Selective Copying task because simply masking out the irrelevant inputs does not aﬀect the spacing, Font: SFRM1000, Size: 9.997447967529297
Text: between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the, Font: SFRM1000, Size: 9.942694664001465
Text: noise tokens are embedded to 0)., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  RetNet (Y. Sun et al., Font: SFRM1000, Size: 9.917706489562988
Text:  2023, Font: SFRM1000, Size: 9.917706489562988
Text: ) is also based on Linear Attention and very similar to H3, but reduces the inner S4, Font: SFRM1000, Size: 9.917706489562988
Text: layer to a special case where the state dimension is, Font: SFRM1000, Size: 9.942694664001465
Text:  푁 = 1, Font: STIXTwoMath, Size: 9.962639808654785
Text: . Although not framed as such, its recurrence can be, Font: SFRM1000, Size: 9.942694664001465
Text: viewed as a special case of a linear SSM., Font: SFRM1000, Size: 9.962639808654785
Text: Its primary source of improvement is using a linear attention with large head dimension, which can be viewed as, Font: SFRM1000, Size: 9.862509727478027
Text: another method to perform input-dependent state expansion. Using a larger head dimension in the context, Font: SFRM1000, Size: 10.061773300170898
Text: of linear attention variants was ﬁrst done by H3, but not extensively used since this requires a proportional, Font: SFRM1000, Size: 10.061773300170898
Text: amount of extra computation. RetNet avoids this with an alternate way to parallelize the computation with a, Font: SFRM1000, Size: 9.957657814025879
Text: variant of standard multi-head attention instead of convolutions, made feasible by their particular special case, Font: SFRM1000, Size: 9.952672004699707
Text: of SSMs which acts as a simple EMA., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  RWKV (B. Peng et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ) is another recent RNN designed for language modeling. It is based on AFT, Font: SFRM1000, Size: 10.061773300170898
Text: (attention-free Transformer (S. Zhai et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2021, Font: SFRM1000, Size: 9.862509727478027
Text: )), another variant of linear attention. Its main “WKV” mechanism, Font: SFRM1000, Size: 9.862509727478027
Text: involves LTI recurrences and can be seen as the ratio of two SSMs., Font: SFRM1000, Size: 9.962639808654785
Text: We also highlight the gated attention unit (GAU) from Hua et al. (, Font: SFRM1000, Size: 9.912701606750488
Text: 2022, Font: SFRM1000, Size: 9.912701606750488
Text: ), which was motivated by combining the, Font: SFRM1000, Size: 9.912701606750488
Text: Transformer’s MHA and MLP blocks together and was an inspiration for our architecture (Section, Font: SFRM1000, Size: 9.867541313171387
Text:  3.4, Font: SFRM1000, Size: 9.867541313171387
Text: ) combining, Font: SFRM1000, Size: 9.867541313171387
Text: the H3 and MLP blocks., Font: SFRM1000, Size: 9.962639808654785
Text: B.3, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Relationship to RNNs, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: RNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state., Font: SFRM1000, Size: 9.962639808654785
Text: Several older RNNs such as the strongly typed RNN (Balduzzi and Ghifary, Font: SFRM1000, Size: 9.862509727478027
Text:  2016, Font: SFRM1000, Size: 9.862509727478027
Text: ), quasi-RNN (QRNN) (Bradbury, Font: SFRM1000, Size: 9.862509727478027
Text: et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2016, Font: SFRM1000, Size: 10.061773300170898
Text: ), and simple recurrent unit (SRU) (Lei, Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ; Lei et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2017, Font: SFRM1000, Size: 10.061773300170898
Text: ) involve forms of gated RNNs without, Font: SFRM1000, Size: 10.061773300170898
Text: time-wise nonlinearities. Because of the connections of gating mechanisms and selection mechanisms, these can be, Font: SFRM1000, Size: 9.862509727478027
Text: viewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs, Font: SFRM1000, Size: 9.917706489562988
Text: above. The main diﬀerences are:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  They do not use state expansion (, Font: SFRM1000, Size: 10.061773300170898
Text: 푁 = 1, Font: STIXTwoMath, Size: 9.962639808654785
Text: ) or selective, Font: SFRM1000, Size: 10.061773300170898
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text:  parameters, both of which are important for, Font: SFRM1000, Size: 10.061773300170898
Text: performance (Section, Font: SFRM1000, Size: 9.962639808654785
Text:  4.6, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +, Font: SFRM1000, Size: 9.927709579467773
Text: discretization (Theorem, Font: SFRM1000, Size: 10.042024612426758
Text:  1, Font: SFRM1000, Size: 10.042024612426758
Text: ). The connections to principled SSM theory provides better parameterizations, Font: SFRM1000, Size: 10.042024612426758
Text: and initializations (Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3.6, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: Additionally, older RNNs famously suﬀered from eﬃciency issues and the vanishing gradients problem (Pascanu,, Font: SFRM1000, Size: 9.962639808654785
Text: Mikolov, and Bengio, Font: SFRM1000, Size: 10.061773300170898
Text:  2013, Font: SFRM1000, Size: 10.061773300170898
Text: ), both caused by their sequential nature. The latter could be solved for some of the, Font: SFRM1000, Size: 10.061773300170898
Text: above RNNs by leveraging the parallel scan (Martin and Cundy, Font: SFRM1000, Size: 9.862509727478027
Text:  2018, Font: SFRM1000, Size: 9.862509727478027
Text: ), but the former was diﬃcult without theory, Font: SFRM1000, Size: 9.862509727478027
Text: later developed for SSMs. For example, modern structured SSMs diﬀer in more careful parameterization of the, Font: SFRM1000, Size: 10.022237777709961
Text: recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson, Goel, et al., Font: SFRM1000, Size: 9.937702178955078
Text:  2021, Font: SFRM1000, Size: 9.937702178955078
Text: ;, Font: SFRM1000, Size: 9.937702178955078
Text: Gu, Johnson, Timalsina, et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: )), or direct analysis (Orvieto et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ))., Font: SFRM1000, Size: 9.962639808654785
Text: We also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio, Font: SFRM1000, Size: 10.061773300170898
Text:  2016, Font: SFRM1000, Size: 10.061773300170898
Text: ; Henaﬀ,, Font: SFRM1000, Size: 10.061773300170898
Text: Szlam, and LeCun, Font: SFRM1000, Size: 9.862509727478027
Text:  2016, Font: SFRM1000, Size: 9.862509727478027
Text: ; Lezcano-Casado and Martínez-Rubio, Font: SFRM1000, Size: 9.862509727478027
Text:  2019, Font: SFRM1000, Size: 9.862509727478027
Text: ; Mhammedi et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2017, Font: SFRM1000, Size: 9.862509727478027
Text: ; Vorontsov et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2017, Font: SFRM1000, Size: 9.862509727478027
Text: ), Font: SFRM1000, Size: 9.862509727478027
Text: 26, Font: STIXTwoText, Size: 9.962639808654785
Text: which are motivated by constraining the, Font: SFRM1000, Size: 10.061773300170898
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  transition matrix to be orthogonal or unitary, in order to control, Font: SFRM1000, Size: 10.061773300170898
Text: its eigenvalues and prevent the vanishing gradient problem. However, these had other limitations; we believe, Font: SFRM1000, Size: 10.061773300170898
Text: that these stem from the fact that orthogonal/unitary RNNs are also LTI. For example, they are almost always, Font: SFRM1000, Size: 9.987515449523926
Text: evaluated on the Copying task which they can solve perfectly, but observed to struggle on the Selective Copying, Font: SFRM1000, Size: 9.957657814025879
Text: task (Jing et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2019, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: B.4, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Linear Attention, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: The Linear Attention (LA) (Katharopoulos et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2020, Font: SFRM1000, Size: 10.061773300170898
Text: ) framework is an important result popularizing kernel, Font: SFRM1000, Size: 10.061773300170898
Text: attention and showing how it relates to recurrent autoregressive models. Many variants have proposed alternative, Font: SFRM1000, Size: 9.882617950439453
Text: kernels and other modiﬁcations. Random Feature Attention (RFA) (H. Peng et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2021, Font: SFRM1000, Size: 9.862509727478027
Text: ) chooses the kernel feature, Font: SFRM1000, Size: 9.862509727478027
Text: map to approximate softmax attention (i.e. the, Font: SFRM1000, Size: 9.862509727478027
Text:  exp, Font: STIXTwoMath, Size: 9.962639808654785
Text:  feature map) using the random Fourier feature approximation, Font: SFRM1000, Size: 9.862509727478027
Text: of Gaussian kernels (Rahimi and Recht, Font: SFRM1000, Size: 10.061773300170898
Text:  2007, Font: SFRM1000, Size: 10.061773300170898
Text: ). Performer (Choromanski et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ) ﬁnds an approximation, Font: SFRM1000, Size: 10.061773300170898
Text: to the exponential kernel involving only positive features, which also allows the softmax normalization term., Font: SFRM1000, Size: 10.061773300170898
Text: TransNormer (Qin, Han, W. Sun, D. Li, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2022, Font: SFRM1000, Size: 10.061773300170898
Text: ) showed that the LA denominator term can be unstable, Font: SFRM1000, Size: 10.061773300170898
Text: and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2022, Font: SFRM1000, Size: 10.061773300170898
Text: ) augments RFA with a, Font: SFRM1000, Size: 10.061773300170898
Text: cosine reweighting mechanism that incorporates positional information to emphasize locality. Linear Randomized, Font: SFRM1000, Size: 9.907693862915039
Text: Attention (Zheng, C. Wang, and L. Kong, Font: SFRM1000, Size: 10.061773300170898
Text:  2022, Font: SFRM1000, Size: 10.061773300170898
Text: ) generalize RFA from the perspective of importance sampling,, Font: SFRM1000, Size: 10.061773300170898
Text: and generalize it to provide better estimates of the full softmax kernel (rather than just the, Font: SFRM1000, Size: 10.061773300170898
Text:  exp, Font: STIXTwoMath, Size: 9.962639808654785
Text: -transformed, Font: SFRM1000, Size: 10.061773300170898
Text: numerator)., Font: SFRM1000, Size: 9.962639808654785
Text: Aside from kernel attention, many other variants of eﬃcient attention exist; the survey Tay, Dehghani, Bahri,, Font: SFRM1000, Size: 10.061773300170898
Text: et al. (, Font: SFRM1000, Size: 9.962639808654785
Text: 2022, Font: SFRM1000, Size: 9.962639808654785
Text: ) oﬀers an extensive categorization of many of these., Font: SFRM1000, Size: 9.962639808654785
Text: B.5, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Long Context Models, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Long context has become a popular subject, and several recent models have claimed to scale to longer and longer, Font: SFRM1000, Size: 9.89767074584961
Text: sequences. However, these are often from a computational standpoint and have not been extensively validated., Font: SFRM1000, Size: 10.05190372467041
Text: These include:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev, Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ), a lightweight wrapper around a, Font: SFRM1000, Size: 10.061773300170898
Text: Transformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization, Font: SFRM1000, Size: 9.922708511352539
Text: tasks; their main result is similar to our Induction Heads extrapolation experiment (Table, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  LongNet (Ding et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ), which claimed to scale to 1B length but only evaluated on length, Font: SFRM1000, Size: 9.862509727478027
Text:  < 100퐾, Font: STIXTwoMath, Size: 9.962639808654785
Text:  for actual, Font: SFRM1000, Size: 9.862509727478027
Text: tasks., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Hyena and HyenaDNA (Nguyen, Poli, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ; Poli et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ), which claimed to leverage up to 1M context., Font: SFRM1000, Size: 9.862509727478027
Text: However, their experiments trained on proportionally more data at longer contexts, making it hard to conclude, Font: SFRM1000, Size: 9.89767074584961
Text: if quality improvements at 1M context are due to context length or due to more data and computation., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Sparse Transformer (Child et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2019, Font: SFRM1000, Size: 9.862509727478027
Text: ) showed a proof-of-concept of using a strided sparse attention Transformer, Font: SFRM1000, Size: 9.862509727478027
Text: to model audio waveforms of length, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 1048576, Font: STIXTwoMath, Size: 9.962639808654785
Text: , although did not discuss performance tradeoﬀs when, Font: SFRM1000, Size: 10.061773300170898
Text: controlling for computation and model size., Font: SFRM1000, Size: 9.962639808654785
Text: In contrast, we believe this work presents one of the ﬁrst approaches to meaningfully demonstrate increasing, Font: SFRM1000, Size: 10.061773300170898
Text: performance with longer context., Font: SFRM1000, Size: 9.962639808654785
Text: C, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Mechanics of Selective SSMs, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Proof of Theorem, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  1, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ., Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  Consider a selective SSM (Algorithm, Font: STIXTwoText, Size: 9.962639808654785
Text:  2, Font: STIXTwoText, Size: 9.962639808654785
Text: ) with, Font: STIXTwoText, Size: 9.962639808654785
Text:  푁 = 1,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text:  = −1,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text:  = 1, 푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = 햫헂헇햾햺헋(푥), 휏, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = 헌허햿헍헉헅헎헌, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text: The corresponding continuous-time SSM (, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoText, Size: 9.962639808654785
Text: ) is, Font: STIXTwoText, Size: 9.962639808654785
Text: ℎ(푡) = −ℎ(푡) + 푥(푡), Font: STIXTwoMath, Size: 9.962639808654785
Text: which is also called a, Font: STIXTwoText, Size: 9.962639808654785
Text:  leaky integrator, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ., Font: STIXTwoText, Size: 9.962639808654785
Text: 27, Font: STIXTwoText, Size: 9.962639808654785
Text: The discretization step size is, Font: STIXTwoText, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = 휏, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text: (햯햺헋햺헆햾헍햾헋 + 푠, Font: STIXTwoMath, Size: 9.962639808654785
Text: ∆, Font: STIXTwoMath, Size: 7.471980094909668
Text: (푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )), Font: STIXTwoMath, Size: 9.962639808654785
Text: = 헌허햿헍헉헅헎헌(햯햺헋햺헆햾헍햾헋 + 햫헂헇햾햺헋(푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )), Font: STIXTwoMath, Size: 9.962639808654785
Text: = 헌허햿헍헉헅헎헌(햫헂헇햾햺헋(푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )), Font: STIXTwoMath, Size: 9.962639808654785
Text: where we observe that the parameter can be viewed as a learnable bias and folded into the linear projection., Font: STIXTwoText, Size: 9.962639808654785
Text: Now applying the zero-order hold (ZOH) discretization formulas:, Font: STIXTwoText, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = exp(∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ) =, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1 + exp(햫헂헇햾햺헋(푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: ) , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 휎(−햫헂헇햾햺헋(푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )), Font: STIXTwoMath, Size: 9.962639808654785
Text: = 1 − 휎(햫헂헇햾햺헋(푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )), Font: STIXTwoMath, Size: 9.962639808654785
Text: B, Font: CMMIB10, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = (∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: −1, Font: STIXTwoMath, Size: 7.471980094909668
Text: (exp(∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ) −, Font: STIXTwoMath, Size: 9.962639808654785
Text:  I, Font: CMMIB10, Size: 9.962639808654785
Text: ) ⋅ ∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: B, Font: CMMIB10, Size: 9.962639808654785
Text:  = −(exp(∆, Font: STIXTwoMath, Size: 9.962639808654785
Text: A, Font: CMMIB10, Size: 9.962639808654785
Text: ) −, Font: STIXTwoMath, Size: 9.962639808654785
Text:  I, Font: CMMIB10, Size: 9.962639808654785
Text: ) = 1 −, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: = 휎(햫헂헇햾햺헋(푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: ))., Font: STIXTwoMath, Size: 9.962639808654785
Text: Thus the fnal discrete recurrence (, Font: STIXTwoText, Size: 9.962639808654785
Text: 2a, Font: STIXTwoText, Size: 9.962639808654785
Text: ) is, Font: STIXTwoText, Size: 9.962639808654785
Text: 푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = 휎(햫헂헇햾햺헋(푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )), Font: STIXTwoMath, Size: 9.962639808654785
Text: ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text:  = (1 − 푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: )ℎ, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡−1, Font: STIXTwoMath, Size: 7.471980094909668
Text:  + 푔, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: 푥, Font: STIXTwoMath, Size: 9.962639808654785
Text: 푡, Font: STIXTwoMath, Size: 7.471980094909668
Text: as desired., Font: STIXTwoText, Size: 9.962639808654785
Text: D, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Hardware-aware Algorithm For Selective SSMs, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Without input-dependent selectivity, SSMs can be eﬃciently implemented as a convolution (Dao, Fu, Saab, et al., Font: SFRM1000, Size: 9.932706832885742
Text: 2023, Font: SFRM1000, Size: 10.037081718444824
Text: ; Gu, Goel, and Ré, Font: SFRM1000, Size: 10.037081718444824
Text:  2022, Font: SFRM1000, Size: 10.037081718444824
Text: ), which leverages the fast Fourier transform (FFT) as primitive. With selectivity,, Font: SFRM1000, Size: 10.037081718444824
Text: SSMs are no-longer equivalent to convolution, but we leverage the parallel associative scan. While SSM scans, Font: SFRM1000, Size: 10.061773300170898
Text: are theoretically eﬃcient (, Font: SFRM1000, Size: 9.932706832885742
Text: 푂(퐵퐿퐷푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  FLOPs, scaling linear in, Font: SFRM1000, Size: 9.932706832885742
Text:  퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text: ), training foundation models with selective SSMs, Font: SFRM1000, Size: 9.932706832885742
Text: requires them to be eﬃcient on modern hardware (GPUs) as well. We describe how we use kernel fusion and, Font: SFRM1000, Size: 10.061773300170898
Text: recomputation to make SSM scan fast and memory-eﬃcient. We evaluate the speed of our scan implementation, Font: SFRM1000, Size: 9.982544898986816
Text: compared to convolution and attention in Section, Font: SFRM1000, Size: 9.977572441101074
Text:  4.5, Font: SFRM1000, Size: 9.977572441101074
Text: , showing that it is up to 7, Font: SFRM1000, Size: 9.977572441101074
Text: ×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  times faster than attention at, Font: SFRM1000, Size: 9.977572441101074
Text: sequence length 32K, and is as memory-eﬃcient as the best attention implementation (FlashAttention)., Font: SFRM1000, Size: 9.962639808654785
Text: Speed., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by, Font: SFRM1000, Size: 10.061773300170898
Text: memory-bandwidth (Dao, Fu, Ermon, et al., Font: SFRM1000, Size: 9.967619895935059
Text:  2022, Font: SFRM1000, Size: 9.967619895935059
Text: ; Ivanov et al., Font: SFRM1000, Size: 9.967619895935059
Text:  2021, Font: SFRM1000, Size: 9.967619895935059
Text: ; Williams, Waterman, and Patterson, Font: SFRM1000, Size: 9.967619895935059
Text:  2009, Font: SFRM1000, Size: 9.967619895935059
Text: )., Font: SFRM1000, Size: 9.967619895935059
Text: This the case with our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to, Font: SFRM1000, Size: 9.937702178955078
Text: signiﬁcant speedup compared to a standard implementation., Font: SFRM1000, Size: 9.962639808654785
Text: The standard way to implement the scan algorithm in Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.2, Font: SFRM1000, Size: 10.061773300170898
Text:  is to prepare the scan input, Font: SFRM1000, Size: 10.061773300170898
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text:  of size, Font: SFRM1000, Size: 10.061773300170898
Text: (퐵, 퐿, 퐷, 푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  in GPU HBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel, Font: SFRM1000, Size: 10.061773300170898
Text: associative scan implementation to write the scan output of size, Font: SFRM1000, Size: 9.862509727478027
Text:  (퐵, 퐿, 퐷, 푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  to GPU HBM, then multiply that scan, Font: SFRM1000, Size: 9.862509727478027
Text: output with, Font: SFRM1000, Size: 9.917706489562988
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text:  to produce an output of size, Font: SFRM1000, Size: 9.917706489562988
Text:  (퐵, 퐿, 퐷), Font: STIXTwoMath, Size: 9.962639808654785
Text: . However, this requires the number of memory reads/writes, Font: SFRM1000, Size: 9.917706489562988
Text: on the order of, Font: SFRM1000, Size: 10.037081718444824
Text:  푂(퐵퐿퐷푁), Font: STIXTwoMath, Size: 9.962639808654785
Text: . We can instead fuse the discretization step, the scan, and the multiplication with, Font: SFRM1000, Size: 10.037081718444824
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: into one kernel:, Font: SFRM1000, Size: 9.962639808654785
Text: 1. We read in, Font: SFRM1000, Size: 9.962639808654785
Text:  푂(퐵퐿퐷 + 퐷푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  bytes of memory (, Font: SFRM1000, Size: 9.962639808654785
Text: ∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: ) from slow HBM to fast SRAM., Font: SFRM1000, Size: 9.962639808654785
Text: 2. We discretize to produce, Font: SFRM1000, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text:  of size, Font: SFRM1000, Size: 9.962639808654785
Text:  (퐵, 퐿, 퐷, 푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  in SRAM., Font: SFRM1000, Size: 9.962639808654785
Text: 3. We perform a parallel associative scan, yielding intermediate states of size, Font: SFRM1000, Size: 9.962639808654785
Text:  (퐵, 퐿, 퐷, 푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  in SRAM., Font: SFRM1000, Size: 9.962639808654785
Text: 4. We multiply and sum with, Font: SFRM1000, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text: , producing outputs of size, Font: SFRM1000, Size: 9.962639808654785
Text:  (퐵, 퐿, 퐷), Font: STIXTwoMath, Size: 9.962639808654785
Text:  and write it to HBM., Font: SFRM1000, Size: 9.962639808654785
Text: This way, we reduce IOs by a factor of, Font: SFRM1000, Size: 9.937702178955078
Text:  푂(푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  (the state dimension), which in practice speeds up the operation by, Font: SFRM1000, Size: 9.937702178955078
Text: 20-40 times (Section, Font: SFRM1000, Size: 9.962639808654785
Text:  4.5, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: 28, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 11: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Induction heads, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Models are trained on sequence length, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 8, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 256, Font: STIXTwoMath, Size: 8.96638011932373
Text: , and tested on various sequence lengths of, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 6, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 64, Font: STIXTwoMath, Size: 8.96638011932373
Text: up to, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 20, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 1048576, Font: STIXTwoMath, Size: 8.96638011932373
Text: ., Font: STIXTwoText, Size: 8.96638011932373
Text:  , Font: Dingbats, Size: 8.96638011932373
Text:  denotes perfect generalization accuracy, while, Font: STIXTwoText, Size: 8.96638011932373
Text:  , Font: Dingbats, Size: 8.96638011932373
Text:  denotes out of memory., Font: STIXTwoText, Size: 8.96638011932373
Text: Model, Font: STIXTwoText, Size: 7.970109939575195
Text: Params, Font: STIXTwoText, Size: 7.970109939575195
Text: Test Accuracy (%) at Sequence Length, Font: STIXTwoText, Size: 7.970109939575195
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 6, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 7, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: CMBX8, Size: 7.970109939575195
Text: 8, Font: CMBX6, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 9, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 10, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 11, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 12, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 13, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 14, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 15, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 16, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 17, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 18, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 19, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 2, Font: STIXTwoMath, Size: 7.970109939575195
Text: 20, Font: STIXTwoMath, Size: 5.9775800704956055
Text: MHA-Abs, Font: STIXTwoText, Size: 7.970109939575195
Text: 137K, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: 99.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 100.0, Font: STIXTwoText, Size: 7.970109939575195
Text: 58.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 26.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 18.8, Font: STIXTwoText, Size: 7.970109939575195
Text: 9.8, Font: STIXTwoText, Size: 7.970109939575195
Text: 10.9, Font: STIXTwoText, Size: 7.970109939575195
Text: 7.8, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: MHA-RoPE, Font: STIXTwoText, Size: 7.970109939575195
Text: 137K, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: 100.0, Font: STIXTwoText, Size: 7.970109939575195
Text: 83.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 31.3, Font: STIXTwoText, Size: 7.970109939575195
Text: 18.4, Font: STIXTwoText, Size: 7.970109939575195
Text: 8.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 9.0, Font: STIXTwoText, Size: 7.970109939575195
Text: 5.5, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: MHA-xPos, Font: STIXTwoText, Size: 7.970109939575195
Text: 137K, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: 100.0, Font: STIXTwoText, Size: 7.970109939575195
Text: 99.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 67.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 25.4, Font: STIXTwoText, Size: 7.970109939575195
Text: 7.0, Font: STIXTwoText, Size: 7.970109939575195
Text: 9.0, Font: STIXTwoText, Size: 7.970109939575195
Text: 7.8, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: H3, Font: STIXTwoText, Size: 7.970109939575195
Text: 153K, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: 100.0, Font: STIXTwoText, Size: 7.970109939575195
Text: 80.9, Font: STIXTwoText, Size: 7.970109939575195
Text: 39.5, Font: STIXTwoText, Size: 7.970109939575195
Text: 23.8, Font: STIXTwoText, Size: 7.970109939575195
Text: 14.8, Font: STIXTwoText, Size: 7.970109939575195
Text: 8.2, Font: STIXTwoText, Size: 7.970109939575195
Text: 5.9, Font: STIXTwoText, Size: 7.970109939575195
Text: 6.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 8.2, Font: STIXTwoText, Size: 7.970109939575195
Text: 4.7, Font: STIXTwoText, Size: 7.970109939575195
Text: 8.2, Font: STIXTwoText, Size: 7.970109939575195
Text: 6.3, Font: STIXTwoText, Size: 7.970109939575195
Text: 7.4, Font: STIXTwoText, Size: 7.970109939575195
Text: Hyena, Font: STIXTwoText, Size: 7.970109939575195
Text: 69M, Font: STIXTwoText, Size: 7.970109939575195
Text: ∗, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 97.7, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: 100.0, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: 44.1, Font: STIXTwoText, Size: 7.970109939575195
Text: 12.5, Font: STIXTwoText, Size: 7.970109939575195
Text: 6.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 5.1, Font: STIXTwoText, Size: 7.970109939575195
Text: 7.0, Font: STIXTwoText, Size: 7.970109939575195
Text: 5.9, Font: STIXTwoText, Size: 7.970109939575195
Text: 6.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 6.6, Font: STIXTwoText, Size: 7.970109939575195
Text: 5.9, Font: STIXTwoText, Size: 7.970109939575195
Text: 6.3, Font: STIXTwoText, Size: 7.970109939575195
Text: 9.8, Font: STIXTwoText, Size: 7.970109939575195
Text: Mamba, Font: STIXTwoText, Size: 7.970109939575195
Text: 74K, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: 100.0, Font: STIXTwoText, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: , Font: Dingbats, Size: 7.970109939575195
Text: ∗, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  Most of the parameters are in learnable positional encodings., Font: STIXTwoText, Size: 7.970109939575195
Text: For sequence length, Font: SFRM1000, Size: 9.862509727478027
Text:  퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text:  too long where we cannot ﬁt the sequence in SRAM (which is much smaller than HBM), we, Font: SFRM1000, Size: 9.862509727478027
Text: split the sequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate, Font: SFRM1000, Size: 9.992483139038086
Text: scan states, we can continue the scan with the next chunk., Font: SFRM1000, Size: 9.962639808654785
Text: Memory., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We describe how we use the classical technique of recomputation to reduce the total amount of memory, Font: SFRM1000, Size: 9.862509727478027
Text: required to train selective SSM layers., Font: SFRM1000, Size: 9.962639808654785
Text: From the way we fuse the forward pass, we do not save the intermediate states of size, Font: SFRM1000, Size: 9.862509727478027
Text:  (퐵, 퐿, 퐷, 푁), Font: STIXTwoMath, Size: 9.962639808654785
Text:  to avoid memory, Font: SFRM1000, Size: 9.862509727478027
Text: blowup. However, these intermediate states are necessary for the backward pass to compute gradients. We instead, Font: SFRM1000, Size: 9.862509727478027
Text: recompute those intermediate states in the backward pass. Since the inputs, Font: SFRM1000, Size: 10.061773300170898
Text:  ∆,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text:  and output gradient, Font: SFRM1000, Size: 10.061773300170898
Text: read from HBM to SRAM are of size, Font: SFRM1000, Size: 10.061773300170898
Text:  푂(퐵퐿푁 + 퐷푁), Font: STIXTwoMath, Size: 9.962639808654785
Text: , and the input gradients are also of size, Font: SFRM1000, Size: 10.061773300170898
Text:  푂(퐵퐿푁 + 퐷푁), Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.061773300170898
Text: recomputation avoids the cost of reading, Font: SFRM1000, Size: 9.997447967529297
Text:  푂(퐵퐿푁퐷), Font: STIXTwoMath, Size: 9.962639808654785
Text:  elements from HBM. This means that recomputation of the, Font: SFRM1000, Size: 9.997447967529297
Text: SSM states in the backward pass speeds up the computation compared to storing them and reading them from, Font: SFRM1000, Size: 10.017284393310547
Text: HBM., Font: SFRM1000, Size: 9.962639808654785
Text: Beyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize, Font: SFRM1000, Size: 9.882617950439453
Text: the memory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output, Font: SFRM1000, Size: 9.882617950439453
Text: projection). In particular, we do not save intermediate activations that take a lot of memory but are fast to, Font: SFRM1000, Size: 10.061773300170898
Text: recompute (e.g. output of activation function or short convolution). As a result, the selective SSM layer has the, Font: SFRM1000, Size: 9.967619895935059
Text: same memory requirement as an optimized Transformer implementation with FlashAttention. In particular, each, Font: SFRM1000, Size: 9.912701606750488
Text: attention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around, Font: SFRM1000, Size: 9.862509727478027
Text: 20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or BF16))., Font: SFRM1000, Size: 9.977572441101074
Text: Each selective SSM stores around 16 bytes of activations per token. Hence two layers of selective SSMs have, Font: SFRM1000, Size: 10.061773300170898
Text: around the same activation memory as an attention layer and an MLP layer., Font: SFRM1000, Size: 9.962639808654785
Text: E, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: Experimental Details and Additional Results, Font: STIXTwoText-Bold, Size: 14.346199989318848
Text: E.1, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Synthetic Tasks, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Selective Copying., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Our setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including, Font: SFRM1000, Size: 9.862509727478027
Text: the white “noise” token from Figure, Font: SFRM1000, Size: 9.862509727478027
Text:  2, Font: SFRM1000, Size: 9.862509727478027
Text: ) and requiring models to memorize 16 “data” tokens. We use 2 layer models, Font: SFRM1000, Size: 9.862509727478027
Text: with a model dimension of, Font: SFRM1000, Size: 9.962639808654785
Text:  퐷 = 64, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: Models are trained for 400K steps at a constant learning rate of, Font: SFRM1000, Size: 9.962639808654785
Text:  0.0001, Font: STIXTwoMath, Size: 9.962639808654785
Text:  with a batch size of, Font: SFRM1000, Size: 9.962639808654785
Text:  64, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: Induction Heads., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Training consists of randomly generating data every step, with a batch size of, Font: SFRM1000, Size: 9.937702178955078
Text:  8, Font: STIXTwoMath, Size: 9.962639808654785
Text: . We choose, Font: SFRM1000, Size: 9.937702178955078
Text: an “epoch” size of 8192 steps, and track the accuracy on ﬁxed validation sets (also randomly generated) of, Font: SFRM1000, Size: 10.061773300170898
Text: each target sequence length. For the MHA-Abs and Mamba models, results are reported after the 25th epoch, Font: SFRM1000, Size: 10.061773300170898
Text: (, Font: SFRM1000, Size: 9.862509727478027
Text: 8192 × 25 = 204800, Font: STIXTwoMath, Size: 9.962639808654785
Text:  steps). For the MHA-RoPE and MHA-xPos models, results are reported after the 50th epoch, Font: SFRM1000, Size: 9.862509727478027
Text: (, Font: SFRM1000, Size: 9.987515449523926
Text: 8192 × 50 = 409600, Font: STIXTwoMath, Size: 9.962639808654785
Text:  steps). For the LTI H3 and Hyena models, results are reported after the 10th epoch (, Font: SFRM1000, Size: 9.987515449523926
Text: 81920, Font: STIXTwoMath, Size: 9.962639808654785
Text: steps) because they had converged by then and failed to improve further., Font: SFRM1000, Size: 9.962639808654785
Text: 29, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 12: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Scaling Law Model Sizes, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Our model sizes and hyperparameters for scaling experiments. (Model dimension and, Font: STIXTwoText, Size: 8.96638011932373
Text: number of heads applies only to Transformer models.), Font: STIXTwoText, Size: 8.96638011932373
Text: Params, Font: STIXTwoText, Size: 8.96638011932373
Text: 횗, Font: STIXTwoMath, Size: 8.96638011932373
Text: _, Font: STIXTwoText, Size: 8.96638011932373
Text: 횕횊횢횎횛횜, Font: STIXTwoMath, Size: 8.96638011932373
Text: 획, Font: STIXTwoMath, Size: 8.96638011932373
Text: _, Font: STIXTwoText, Size: 8.96638011932373
Text: 횖횘획횎횕, Font: STIXTwoMath, Size: 8.96638011932373
Text: 횗, Font: STIXTwoMath, Size: 8.96638011932373
Text: _, Font: STIXTwoText, Size: 8.96638011932373
Text: 횑횎횊획횜, Font: STIXTwoMath, Size: 8.96638011932373
Text:  /, Font: STIXTwoText, Size: 8.96638011932373
Text:  획, Font: STIXTwoMath, Size: 8.96638011932373
Text: _, Font: STIXTwoText, Size: 8.96638011932373
Text: 횑횎횊획, Font: STIXTwoMath, Size: 8.96638011932373
Text: Training steps, Font: STIXTwoText, Size: 8.96638011932373
Text: Learning Rate, Font: STIXTwoText, Size: 8.96638011932373
Text: Batch Size, Font: STIXTwoText, Size: 8.96638011932373
Text: Tokens, Font: STIXTwoText, Size: 8.96638011932373
Text: 125M, Font: STIXTwoText, Size: 8.96638011932373
Text: 12, Font: STIXTwoText, Size: 8.96638011932373
Text: 768, Font: STIXTwoText, Size: 8.96638011932373
Text: 12 / 64, Font: STIXTwoText, Size: 8.96638011932373
Text: 4800, Font: STIXTwoText, Size: 8.96638011932373
Text: 6e-4, Font: STIXTwoText, Size: 8.96638011932373
Text: 0.5M tokens, Font: STIXTwoText, Size: 8.96638011932373
Text: 2.5B, Font: STIXTwoText, Size: 8.96638011932373
Text: 350M, Font: STIXTwoText, Size: 8.96638011932373
Text: 24, Font: STIXTwoText, Size: 8.96638011932373
Text: 1024, Font: STIXTwoText, Size: 8.96638011932373
Text: 16 / 64, Font: STIXTwoText, Size: 8.96638011932373
Text: 13500, Font: STIXTwoText, Size: 8.96638011932373
Text: 3e-4, Font: STIXTwoText, Size: 8.96638011932373
Text: 0.5M tokens, Font: STIXTwoText, Size: 8.96638011932373
Text: 7B, Font: STIXTwoText, Size: 8.96638011932373
Text: 760M, Font: STIXTwoText, Size: 8.96638011932373
Text: 24, Font: STIXTwoText, Size: 8.96638011932373
Text: 1536, Font: STIXTwoText, Size: 8.96638011932373
Text: 16 / 96, Font: STIXTwoText, Size: 8.96638011932373
Text: 29000, Font: STIXTwoText, Size: 8.96638011932373
Text: 2.5e-4, Font: STIXTwoText, Size: 8.96638011932373
Text: 0.5M tokens, Font: STIXTwoText, Size: 8.96638011932373
Text: 15B, Font: STIXTwoText, Size: 8.96638011932373
Text: 1.3B, Font: STIXTwoText, Size: 8.96638011932373
Text: 24, Font: STIXTwoText, Size: 8.96638011932373
Text: 2048, Font: STIXTwoText, Size: 8.96638011932373
Text: 32 / 64, Font: STIXTwoText, Size: 8.96638011932373
Text: 50000, Font: STIXTwoText, Size: 8.96638011932373
Text: 2e-4, Font: STIXTwoText, Size: 8.96638011932373
Text: 0.5M tokens, Font: STIXTwoText, Size: 8.96638011932373
Text: 26B, Font: STIXTwoText, Size: 8.96638011932373
Text: We use the Adam optimizer with no weight decay. All models are trained at constant learning rates, Font: SFRM1000, Size: 10.027188301086426
Text:  2푒 − 4, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and, Font: SFRM1000, Size: 10.027188301086426
Text: 1푒 − 3, Font: STIXTwoMath, Size: 9.962639808654785
Text: , and the better results are reported for each model (, Font: SFRM1000, Size: 10.027188301086426
Text: 2푒 − 4, Font: STIXTwoMath, Size: 9.962639808654785
Text:  for all models except Mamba). The attention, Font: SFRM1000, Size: 10.027188301086426
Text: and Hyena models did not learn at LR, Font: SFRM1000, Size: 10.061773300170898
Text:  1푒 − 3, Font: STIXTwoMath, Size: 9.962639808654785
Text: . H3 learned at both LRs, but interestingly generalized better to, Font: SFRM1000, Size: 10.061773300170898
Text: shorter sequences at the smaller LR of, Font: SFRM1000, Size: 9.90268325805664
Text:  2푒 − 4, Font: STIXTwoMath, Size: 9.962639808654785
Text: . Mamba learned at both LRs, but extrapolated better at the larger, Font: SFRM1000, Size: 9.90268325805664
Text: LR of, Font: SFRM1000, Size: 9.962639808654785
Text:  1푒 − 3, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: E.2, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Language Modeling, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: E.2.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Scaling Law Details, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: All models were trained on the Pile., Font: SFRM1000, Size: 9.962639808654785
Text: Model Sizes., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 10.017284393310547
Text:  12, Font: SFRM1000, Size: 10.017284393310547
Text:  speciﬁes the model sizes we use for scaling laws. This is taken directly from the GPT3, Font: SFRM1000, Size: 10.017284393310547
Text: speciﬁcations (Brown et al., Font: SFRM1000, Size: 10.05190372467041
Text:  2020, Font: SFRM1000, Size: 10.05190372467041
Text: ), with very minor modiﬁcations. First, we changed the batch size of the 1.3B, Font: SFRM1000, Size: 10.05190372467041
Text: model from 1M tokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch, Font: SFRM1000, Size: 10.061773300170898
Text: size. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling, Font: SFRM1000, Size: 10.061773300170898
Text: laws (Hoﬀmann et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2022, Font: SFRM1000, Size: 9.862509727478027
Text: ), which specify that training tokens should increase proportionally to model size., Font: SFRM1000, Size: 9.862509727478027
Text: Training Recipes., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: All models used the AdamW optimizer with, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  gradient clip value, Font: SFRM1000, Size: 9.962639808654785
Text:  1.0, Font: STIXTwoMath, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  weight decay, Font: SFRM1000, Size: 9.962639808654785
Text:  0.1, Font: STIXTwoMath, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  no dropout, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  linear learning rate warmup with cosine decay, Font: SFRM1000, Size: 9.962639808654785
Text: By default, the peak learning rate is the GPT3 speciﬁcation., Font: SFRM1000, Size: 9.962639808654785
Text: We give several models an “improved recipe”, inspired by changes adopted by popular large language models such, Font: SFRM1000, Size: 9.90268325805664
Text: as PaLM (Chowdhery et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ) and LLaMa (Touvron et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ). These include:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  linear learning rate warmup with cosine decay to, Font: SFRM1000, Size: 9.962639808654785
Text:  1푒 − 5, Font: STIXTwoMath, Size: 9.962639808654785
Text: , with a peak value of, Font: SFRM1000, Size: 9.962639808654785
Text:  5×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  the GPT3 value, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  no linear bias terms, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  RMSNorm instead of LayerNorm, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  AdamW hyperparameter, Font: SFRM1000, Size: 9.962639808654785
Text:  훽 = (.9, .95), Font: STIXTwoMath, Size: 9.962639808654785
Text:  (the GPT3 value) instead of the PyTorch default of, Font: SFRM1000, Size: 9.962639808654785
Text:  훽 = (.9, .999), Font: STIXTwoMath, Size: 9.962639808654785
Text: Architecture and Training Details., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Our models are:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Transformer: The standard Transformer based on GPT3 (Table, Font: SFRM1000, Size: 9.962639808654785
Text:  12, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al., Font: SFRM1000, Size: 10.017284393310547
Text: 2021, Font: SFRM1000, Size: 9.962639808654785
Text: ) and SwiGLU MLP (Shazeer, Font: SFRM1000, Size: 9.962639808654785
Text:  2020, Font: SFRM1000, Size: 9.962639808654785
Text: ), and the improved training recipe above., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an, Font: SFRM1000, Size: 9.87256908416748
Text: MLP) with standard MLP blocks. The MLP blocks have expansion factor, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text:  instead of, Font: SFRM1000, Size: 10.061773300170898
Text:  4, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and the number of, Font: SFRM1000, Size: 10.061773300170898
Text: layers is correspondingly increased by, Font: SFRM1000, Size: 9.962639808654785
Text:  1.5×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  to preserve parameter count., Font: SFRM1000, Size: 9.962639808654785
Text: 30, Font: STIXTwoText, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  H3++: The H3 architecture with a few modiﬁcations, including (i) using the same “thin” Hyena dimensions, Font: SFRM1000, Size: 10.061773300170898
Text: above (ii) the improved training recipe above (iii) a linear attention head dimension of 8., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  RWKV: The default RWKV model from B. Peng et al. (, Font: SFRM1000, Size: 9.882617950439453
Text: 2023, Font: SFRM1000, Size: 9.882617950439453
Text: ), including its modiﬁed MLP block. We also used, Font: SFRM1000, Size: 9.882617950439453
Text: as much of its speciﬁed training recipe as possible, such as increasing the learning rates by, Font: SFRM1000, Size: 9.917706489562988
Text:  2×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  or, Font: SFRM1000, Size: 9.917706489562988
Text:  3×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  on certain, Font: SFRM1000, Size: 9.917706489562988
Text: parameters., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  RetNet: The default RetNet model from Y. Sun et al. (, Font: SFRM1000, Size: 10.061773300170898
Text: 2023, Font: SFRM1000, Size: 10.061773300170898
Text: ). We also gave it the improved training recipe, Font: SFRM1000, Size: 10.061773300170898
Text: above., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Mamba: The standard Mamba architecture, with the improved training recipe., Font: SFRM1000, Size: 9.962639808654785
Text: E.2.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Additional Scaling Law Ablations, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws, Font: SFRM1000, Size: 9.87256908416748
Text: in Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  4, Font: SFRM1000, Size: 9.962639808654785
Text:  (Left)., Font: SFRM1000, Size: 9.962639808654785
Text: Mamba Architecture: Interleaving Blocks., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We test the eﬀect of diﬀerent architectural blocks combined with, Font: SFRM1000, Size: 9.952672004699707
Text: the Mamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with, Font: SFRM1000, Size: 9.957657814025879
Text: an extra, Font: SFRM1000, Size: 9.962639808654785
Text:  햼허헇헏 → 햲햲햬, Font: STIXTwoMath, Size: 9.962639808654785
Text:  path added. This leads to two natural ablations:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This, Font: SFRM1000, Size: 10.032135963439941
Text: can also be interpreted as taking Mamba and removing half of the SSMs., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted, Font: SFRM1000, Size: 9.89767074584961
Text: as taking a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the, Font: SFRM1000, Size: 9.862509727478027
Text: MLP blocks., Font: SFRM1000, Size: 9.962639808654785
Text: Figure, Font: SFRM1000, Size: 9.937702178955078
Text:  9, Font: SFRM1000, Size: 9.937702178955078
Text:  (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly,, Font: SFRM1000, Size: 9.937702178955078
Text: neither change matters too much. The Mamba-MLP architecture is only slightly worse, and still better than, Font: SFRM1000, Size: 10.061773300170898
Text: all models except Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat, Font: SFRM1000, Size: 10.061773300170898
Text: surprising in light of the fact that many recent works have found that combining (LTI) SSMs with Attention can, Font: SFRM1000, Size: 9.922708511352539
Text: lead to substantial improvements (Dao, Fu, Saab, et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ; Fathi et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ; Fathullah et al., Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: ; Saon, Gupta,, Font: SFRM1000, Size: 9.862509727478027
Text: and Cui, Font: SFRM1000, Size: 9.962639808654785
Text:  2023, Font: SFRM1000, Size: 9.962639808654785
Text: ; Zuo et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2022, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: H3 Architecture: Training Recipes., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Next we ablate diﬀerences between the Hyena and H3++ models, our, Font: SFRM1000, Size: 10.061773300170898
Text: weakest and strongest models outside of Transformer++ and Mamba, particularly to isolate the eﬀect of training, Font: SFRM1000, Size: 9.907693862915039
Text: recipes., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  4, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Hyena+: The same architecture but with the improved training recipe described above., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution, Font: SFRM1000, Size: 9.862509727478027
Text: kernel., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside, Font: SFRM1000, Size: 9.952672004699707
Text: the SSM recurrence but does not increase parameters., Font: SFRM1000, Size: 9.962639808654785
Text: Our general convention is that “Model+” represents the base model with the improved training recipe, and, Font: SFRM1000, Size: 10.061773300170898
Text: “Model++” also allows for architectural changes., Font: SFRM1000, Size: 9.962639808654785
Text: Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  9, Font: SFRM1000, Size: 9.962639808654785
Text:  (Right) shows that, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  A large improvement is achieved by the improved training recipe, which was used for many of the models in the, Font: SFRM1000, Size: 9.862509727478027
Text: main Figure, Font: SFRM1000, Size: 9.962639808654785
Text:  4, Font: SFRM1000, Size: 9.962639808654785
Text:  (RetNet, H3++, Transformer++, Mamba)., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with ﬁndings throughout this, Font: SFRM1000, Size: 9.932706832885742
Text: paper., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  The head dimension expansion improves performance, consistent with one of our main themes that expanded, Font: SFRM1000, Size: 10.002410888671875
Text: state dimension improves performance for SSMs (Section, Font: SFRM1000, Size: 9.962639808654785
Text:  3, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: 31, Font: STIXTwoText, Size: 9.962639808654785
Text: 10, Font: Roboto-Bold, Size: 4.895880222320557
Text: 19, Font: Roboto-Bold, Size: 3.4271159172058105
Text: 10, Font: Roboto-Bold, Size: 4.895880222320557
Text: 20, Font: Roboto-Bold, Size: 3.4271159172058105
Text: FLOPs (log scale), Font: Roboto-Bold, Size: 6.2311201095581055
Text: 10, Font: Roboto-Bold, Size: 4.895880222320557
Text: 1, Font: Roboto-Bold, Size: 3.4271159172058105
Text: 7 × 10, Font: Roboto-Bold, Size: 4.895880222320557
Text: 0, Font: Roboto-Bold, Size: 3.4271159172058105
Text: 8 × 10, Font: Roboto-Bold, Size: 4.895880222320557
Text: 0, Font: Roboto-Bold, Size: 3.4271159172058105
Text: 9 × 10, Font: Roboto-Bold, Size: 4.895880222320557
Text: 0, Font: Roboto-Bold, Size: 3.4271159172058105
Text: Perplexity (log scale), Font: Roboto-Bold, Size: 6.2311201095581055
Text: Scaling Laws on The Pile (Sequence Length 2048), Font: Roboto-Bold, Size: 6.2311201095581055
Text: Mamba, Font: Roboto-Bold, Size: 4.895880222320557
Text: Mamba-MLP, Font: Roboto-Bold, Size: 4.895880222320557
Text: Mamba-MHA, Font: Roboto-Bold, Size: 4.895880222320557
Text: 10, Font: Roboto-Bold, Size: 4.882349967956543
Text: 19, Font: Roboto-Bold, Size: 3.41764497756958
Text: 10, Font: Roboto-Bold, Size: 4.882349967956543
Text: 20, Font: Roboto-Bold, Size: 3.41764497756958
Text: FLOPs (log scale), Font: Roboto-Bold, Size: 6.213900089263916
Text: 10, Font: Roboto-Bold, Size: 4.882349967956543
Text: 1, Font: Roboto-Bold, Size: 3.41764497756958
Text: Perplexity (log scale), Font: Roboto-Bold, Size: 6.213900089263916
Text: Scaling Laws on The Pile (Sequence Length 2048), Font: Roboto-Bold, Size: 6.213900089263916
Text: Hyena, Font: Roboto-Bold, Size: 4.882349967956543
Text: Hyena+, Font: Roboto-Bold, Size: 4.882349967956543
Text: H3+, Font: Roboto-Bold, Size: 4.882349967956543
Text: H3++, Font: Roboto-Bold, Size: 4.882349967956543
Text: Figure 9: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Scaling laws: extra ablations, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) (, Font: STIXTwoText, Size: 8.96638011932373
Text: Left, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Instead of (, Font: STIXTwoText, Size: 8.96638011932373
Text: Right, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Instead of, Font: STIXTwoText, Size: 8.96638011932373
Text: E.2.3, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Downstream Evaluation Details, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: This pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens. For the 1.3B, Font: SFRM1000, Size: 10.05190372467041
Text: model, we use a batch size of 1M tokens to be consistent with the GPT3 speciﬁcations. We report the perplexity, Font: SFRM1000, Size: 9.922708511352539
Text: on the Pile validation set, and for this metric only compare to models trained on the same dataset and with the, Font: SFRM1000, Size: 9.967619895935059
Text: same tokenizer, in particular Pythia and RWKV., Font: SFRM1000, Size: 9.962639808654785
Text: For downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2021, Font: SFRM1000, Size: 10.061773300170898
Text: ),, Font: SFRM1000, Size: 10.061773300170898
Text: as done by most work in this area. We evaluate on the following tasks/datasets that measure common sense, Font: SFRM1000, Size: 10.061773300170898
Text: reasoning:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  LAMBADA (Paperno et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2016, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  HellaSwag (Zellers et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2019, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  PIQA (Bisk et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2020, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  ARC-challenge (P. Clark et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2018, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  ARC-easy: an easy subset of ARC-challenge., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  WinoGrande (Sakaguchi et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2021, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: We report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence, Font: SFRM1000, Size: 9.997447967529297
Text: length for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these, Font: SFRM1000, Size: 10.061773300170898
Text: task)., Font: SFRM1000, Size: 9.962639808654785
Text: E.3, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: DNA Modeling, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: E.3.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Pretraining Details, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We describe the dataset and training procedure of the HG38 pretraining task in more detail., Font: SFRM1000, Size: 9.962639808654785
Text: The dataset follows the splits from the prior Enformer work on genomics (Avsec et al., Font: SFRM1000, Size: 10.056839942932129
Text:  2021, Font: SFRM1000, Size: 10.056839942932129
Text: ); the training split, Font: SFRM1000, Size: 10.056839942932129
Text: contains a total of, Font: SFRM1000, Size: 9.877593994140625
Text:  푆 = 34021, Font: STIXTwoMath, Size: 9.962639808654785
Text:  segments of length, Font: SFRM1000, Size: 9.877593994140625
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 17, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 131072, Font: STIXTwoMath, Size: 9.962639808654785
Text:  that cover the genome, for a total of approximately, Font: SFRM1000, Size: 9.877593994140625
Text: 4.5 billion tokens (DNA base pairs). These segments are pairs of (chromosome number, starting index, ending, Font: SFRM1000, Size: 10.061773300170898
Text: index), and can be extended if necessary (e.g. to get longer segments)., Font: SFRM1000, Size: 9.962639808654785
Text: We deviate from HyenaDNA when the training sequence length is not, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 17, Font: STIXTwoMath, Size: 7.471980094909668
Text: . HyenaDNA always takes a ﬁxed, Font: SFRM1000, Size: 10.061773300170898
Text: sub-segment (e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length, Font: SFRM1000, Size: 9.947684288024902
Text: each epoch is ﬁxed to, Font: SFRM1000, Size: 9.862509727478027
Text:  34021, Font: STIXTwoMath, Size: 9.962639808654785
Text:  samples and doesn’t necessarily go through the whole genome. On the other hand, we, Font: SFRM1000, Size: 9.862509727478027
Text: use the entire training data:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  When the context length, Font: SFRM1000, Size: 10.061773300170898
Text:  퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text:  is less than (or equal to), Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 17, Font: STIXTwoMath, Size: 7.471980094909668
Text: , we divide up each segment into non-overlapping, Font: SFRM1000, Size: 10.061773300170898
Text: sub-segments of length, Font: SFRM1000, Size: 9.962639808654785
Text:  퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text: , so that there are, Font: SFRM1000, Size: 9.962639808654785
Text:  푆 ×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  , Font: STIXTwoMath, Size: 7.471980094909668
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text: 17, Font: STIXTwoMath, Size: 5.9775800704956055
Text: 퐿, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.962639808654785
Text: total samples and, Font: SFRM1000, Size: 9.962639808654785
Text:  푆 × 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 17, Font: STIXTwoMath, Size: 7.471980094909668
Text:  ≈ 4.5퐵, Font: STIXTwoMath, Size: 9.962639808654785
Text:  tokens per epoch., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  When the context length, Font: SFRM1000, Size: 9.862509727478027
Text:  퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text:  is greater than, Font: SFRM1000, Size: 9.862509727478027
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 17, Font: STIXTwoMath, Size: 7.471980094909668
Text: , we turn each segment into two samples, one that begins with the, Font: SFRM1000, Size: 9.862509727478027
Text: prescribed segment and one that ends with the prescribed segment. Thus each epoch has, Font: SFRM1000, Size: 10.061773300170898
Text:  2푆, Font: STIXTwoMath, Size: 9.962639808654785
Text:  items and, Font: SFRM1000, Size: 10.061773300170898
Text:  2푆퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text: 32, Font: STIXTwoText, Size: 9.962639808654785
Text: tokens per epoch. For example, at sequence length, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 18, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 262144, Font: STIXTwoMath, Size: 9.962639808654785
Text:  there are, Font: SFRM1000, Size: 10.061773300170898
Text:  4×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  as many tokens as the default,, Font: SFRM1000, Size: 10.061773300170898
Text: and at sequence length, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.962639808654785
Text: there are, Font: SFRM1000, Size: 9.962639808654785
Text:  16×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  as many tokens., Font: SFRM1000, Size: 9.962639808654785
Text: Other training details generally follow the same protocol as our language modeling experiments (Appendix, Font: SFRM1000, Size: 9.967619895935059
Text:  E.2, Font: SFRM1000, Size: 9.967619895935059
Text: )., Font: SFRM1000, Size: 9.967619895935059
Text: For example, we use the AdamW with, Font: SFRM1000, Size: 9.862509727478027
Text:  (훽, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1, Font: STIXTwoMath, Size: 7.471980094909668
Text: , 훽, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text: ) = (0.9, 0.95), Font: STIXTwoMath, Size: 9.962639808654785
Text: , no dropout, weight decay, Font: SFRM1000, Size: 9.862509727478027
Text:  0.1, Font: STIXTwoMath, Size: 9.962639808654785
Text: . We use a cosine learning, Font: SFRM1000, Size: 9.862509727478027
Text: rate scheduler with linear warmup for 10% of total steps., Font: SFRM1000, Size: 9.962639808654785
Text: E.3.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Scaling: Model Size Details, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Models., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: The models we consider are:, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su, Font: SFRM1000, Size: 9.862509727478027
Text: et al., Font: SFRM1000, Size: 9.87256908416748
Text:  2021, Font: SFRM1000, Size: 9.87256908416748
Text: ). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani, Font: SFRM1000, Size: 9.87256908416748
Text: et al., Font: SFRM1000, Size: 9.962639808654785
Text:  2017, Font: SFRM1000, Size: 9.962639808654785
Text: )., Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  HyenaDNA: the Hyena model from Nguyen, Poli, et al. (, Font: SFRM1000, Size: 10.061773300170898
Text: 2023, Font: SFRM1000, Size: 10.061773300170898
Text: ) and Poli et al. (, Font: SFRM1000, Size: 10.061773300170898
Text: 2023, Font: SFRM1000, Size: 10.061773300170898
Text: ), which is roughly a, Font: SFRM1000, Size: 10.061773300170898
Text: Transformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP., Font: SFRM1000, Size: 9.862509727478027
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Mamba: the standard Mamba architecture., Font: SFRM1000, Size: 9.962639808654785
Text: Model Sizes., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We use the following model sizes., Font: SFRM1000, Size: 9.962639808654785
Text: Blocks, Font: STIXTwoText, Size: 9.962639808654785
Text: 4, Font: SFRM1000, Size: 9.962639808654785
Text: 5, Font: SFRM1000, Size: 9.962639808654785
Text: 6, Font: SFRM1000, Size: 9.962639808654785
Text: 7, Font: SFRM1000, Size: 9.962639808654785
Text: 8, Font: SFRM1000, Size: 9.962639808654785
Text: 10, Font: SFRM1000, Size: 9.962639808654785
Text: 12, Font: SFRM1000, Size: 9.962639808654785
Text: Model Dimension, Font: STIXTwoText, Size: 9.962639808654785
Text: 64, Font: SFRM1000, Size: 9.962639808654785
Text: 96, Font: SFRM1000, Size: 9.962639808654785
Text: 128, Font: SFRM1000, Size: 9.962639808654785
Text: 192, Font: SFRM1000, Size: 9.962639808654785
Text: 256, Font: SFRM1000, Size: 9.962639808654785
Text: 384, Font: SFRM1000, Size: 9.962639808654785
Text: 512, Font: SFRM1000, Size: 9.962639808654785
Text: Params (Approx.), Font: STIXTwoText, Size: 9.962639808654785
Text: 250K, Font: SFRM1000, Size: 9.962639808654785
Text: 700K, Font: SFRM1000, Size: 9.962639808654785
Text: 1.4M, Font: SFRM1000, Size: 9.962639808654785
Text: 3.5M, Font: SFRM1000, Size: 9.962639808654785
Text: 7.0M, Font: SFRM1000, Size: 9.962639808654785
Text: 19.3M, Font: SFRM1000, Size: 9.962639808654785
Text: 40.7M, Font: SFRM1000, Size: 9.962639808654785
Text: Note that the number of blocks for Mamba is doubled, because one Transformer “layer” includes both the MHA and, Font: SFRM1000, Size: 9.862509727478027
Text: MLP blocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section, Font: SFRM1000, Size: 9.862509727478027
Text:  3.4, Font: SFRM1000, Size: 9.862509727478027
Text: )., Font: SFRM1000, Size: 9.862509727478027
Text: Training., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: For each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across, Font: SFRM1000, Size: 9.862509727478027
Text:  {1푒 − 3, 2푒 −, Font: STIXTwoMath, Size: 9.962639808654785
Text: 3, 4푒 − 3, 8푒 − 3}, Font: STIXTwoMath, Size: 9.962639808654785
Text: . The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal, Font: SFRM1000, Size: 9.927709579467773
Text: Mamba learning rate was 8e-3; note that Mamba performed better than baselines with matched learning rates, Font: SFRM1000, Size: 10.056839942932129
Text: (2e-3), but was more stable and improved even more at higher learning rates. (Furthermore, as this LR is on the, Font: SFRM1000, Size: 9.927709579467773
Text: upper range of the sweep, it is possible that our results are still suboptimal.), Font: SFRM1000, Size: 9.962639808654785
Text: Note that, in contrast to standard LM scaling laws (Table, Font: SFRM1000, Size: 10.061773300170898
Text:  12, Font: SFRM1000, Size: 10.061773300170898
Text: ), our LR held constant across model sizes for, Font: SFRM1000, Size: 10.061773300170898
Text: simplicity. The optimal LR should go down for larger models, but we didn’t ﬁnd a noticeable eﬀect at the small, Font: SFRM1000, Size: 9.967619895935059
Text: model sizes (at most a few million parameters) we considered., Font: SFRM1000, Size: 9.962639808654785
Text: E.3.3, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Scaling: Context Length Details, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We use a total batch size of, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 24, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: ≈ 16푀, Font: STIXTwoMath, Size: 9.962639808654785
Text:  tokens per training step, for every sequence length (e.g. at length, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text: there are, Font: SFRM1000, Size: 9.90268325805664
Text:  16, Font: STIXTwoMath, Size: 9.962639808654785
Text:  segments per batch and at length, Font: SFRM1000, Size: 9.90268325805664
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 10, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.90268325805664
Text: there are, Font: SFRM1000, Size: 9.90268325805664
Text:  16384, Font: STIXTwoMath, Size: 9.962639808654785
Text:  segments per batch). This is a large batch size, Font: SFRM1000, Size: 9.90268325805664
Text: relative to the model size by usual LM standards, but note that a batch size of, Font: SFRM1000, Size: 9.932706832885742
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 23, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.932706832885742
Text: is the minimum possible on a, Font: SFRM1000, Size: 9.932706832885742
Text: machine with 8 GPUs and sequence length of, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text: 0, Font: STIXTwoMath, Size: 9.962639808654785
Text: , and that HyenaDNA used much larger batches of, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 28, Font: STIXTwoMath, Size: 7.471980094909668
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: The learning rate used was, Font: SFRM1000, Size: 10.056839942932129
Text:  0.008, Font: STIXTwoMath, Size: 9.962639808654785
Text:  for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same, Font: SFRM1000, Size: 10.056839942932129
Text: learning rate of, Font: SFRM1000, Size: 10.061773300170898
Text:  0.002, Font: STIXTwoMath, Size: 9.962639808654785
Text:  from the previous section for HyenaDNA, but found that it was unstable at the longest, Font: SFRM1000, Size: 10.061773300170898
Text: context length., Font: SFRM1000, Size: 9.962639808654785
Text: Sequence Length Warmup., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Following (Nguyen, Poli, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ), we use sequence length warmup (SLW), Font: SFRM1000, Size: 10.061773300170898
Text: during pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from, Font: SFRM1000, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 10, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 1024, Font: STIXTwoMath, Size: 9.962639808654785
Text: . (Note that because of how data is curated, at the longest sequence lengths more steps and tokens are, Font: SFRM1000, Size: 9.89767074584961
Text: spent proportionally. In particular, each stage up to length, Font: SFRM1000, Size: 10.007370948791504
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 17, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 10.007370948791504
Text: processes the same number of tokens, but, Font: SFRM1000, Size: 10.007370948791504
Text:  4×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  as, Font: SFRM1000, Size: 10.007370948791504
Text: many tokens are processed at length, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 18, Font: STIXTwoMath, Size: 7.471980094909668
Text: ,, Font: SFRM1000, Size: 9.962639808654785
Text:  8×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  as many at length, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 19, Font: STIXTwoMath, Size: 7.471980094909668
Text: , and, Font: SFRM1000, Size: 9.962639808654785
Text:  16×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  as many at length, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text: .), Font: SFRM1000, Size: 9.962639808654785
Text: Unlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively, Font: SFRM1000, Size: 9.862509727478027
Text: halved as the sequence lengths are doubled in each stage., Font: SFRM1000, Size: 9.962639808654785
Text: 33, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 13: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Great Apes DNA Classifcation, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Accuracy after fne-tuning on sequences of length, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 10, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 1024, Font: STIXTwoMath, Size: 8.96638011932373
Text:  up to, Font: STIXTwoText, Size: 8.96638011932373
Text:  2, Font: STIXTwoMath, Size: 8.96638011932373
Text: 20, Font: STIXTwoMath, Size: 5.9775800704956055
Text:  , Font: STIXTwoMath, Size: 8.96638011932373
Text: = 1048576, Font: STIXTwoMath, Size: 8.96638011932373
Text: using pretrained models of the same context length. Random guessing is 20%., Font: STIXTwoText, Size: 8.96638011932373
Text: Model, Font: STIXTwoText, Size: 9.962639808654785
Text: Params, Font: STIXTwoText, Size: 9.962639808654785
Text: Accuracy (%) at Sequence Length, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 10, Font: STIXTwoMath, Size: 7.471980094909668
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 12, Font: STIXTwoMath, Size: 7.471980094909668
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 14, Font: STIXTwoMath, Size: 7.471980094909668
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 16, Font: STIXTwoMath, Size: 7.471980094909668
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 18, Font: STIXTwoMath, Size: 7.471980094909668
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text: HyenaDNA, Font: STIXTwoText, Size: 9.962639808654785
Text: 1.4M, Font: STIXTwoText, Size: 9.962639808654785
Text: 28.04, Font: STIXTwoText, Size: 9.962639808654785
Text: 28.43, Font: STIXTwoText, Size: 9.962639808654785
Text: 41.17, Font: STIXTwoText, Size: 9.962639808654785
Text: 42.22, Font: STIXTwoText, Size: 9.962639808654785
Text: 31.10, Font: STIXTwoText, Size: 9.962639808654785
Text: 54.87, Font: STIXTwoText, Size: 9.962639808654785
Text: Mamba, Font: STIXTwoText, Size: 9.962639808654785
Text: 1.4M, Font: STIXTwoText, Size: 9.962639808654785
Text: 31.47, Font: STIXTwoText, Size: 9.962639808654785
Text: 27.50, Font: STIXTwoText, Size: 9.962639808654785
Text: 27.66, Font: STIXTwoText, Size: 9.962639808654785
Text: 40.72, Font: STIXTwoText, Size: 9.962639808654785
Text: 42.41, Font: STIXTwoText, Size: 9.962639808654785
Text: 71.67, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Mamba, Font: STIXTwoText, Size: 9.962639808654785
Text: 7M, Font: STIXTwoText, Size: 9.962639808654785
Text: 30.00, Font: STIXTwoText, Size: 9.962639808654785
Text: 29.01, Font: STIXTwoText, Size: 9.962639808654785
Text: 31.48, Font: STIXTwoText, Size: 9.962639808654785
Text: 43.73, Font: STIXTwoText, Size: 9.962639808654785
Text: 56.60, Font: STIXTwoText, Size: 9.962639808654785
Text: 81.31, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Remark E.1., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text:  We also note that the schedule was not tuned, and we never experimented with turning of sequence length, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: warmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: similar lengths (Section, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text:  4.4, Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: ), and it is possible that it is not necessary for DNA pretraining either., Font: STIXTwoText-Italic, Size: 9.962639808654785
Text: E.3.4, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Species (Great Apes) Classifcation, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Models are causal and therefore only the last element (across the sequence length) of the model’s output is used for, Font: SFRM1000, Size: 9.862509727478027
Text: the classiﬁcation head. Note that we control for the total number of elements in the loss function per gradient step., Font: SFRM1000, Size: 9.862509727478027
Text: The pretraining objective includes all positions across the sequence length, so that, Font: SFRM1000, Size: 9.862509727478027
Text:  횋횊횝회횑, Font: STIXTwoMath, Size: 9.962639808654785
Text: _, Font: SFRM1000, Size: 9.962639808654785
Text: 횜횒횣횎×횜횎횚횞횎횗회횎, Font: STIXTwoMath, Size: 9.962639808654785
Text: _, Font: SFRM1000, Size: 9.962639808654785
Text: 횕횎횗횐횝횑, Font: STIXTwoMath, Size: 9.962639808654785
Text: is held constant; in other words, the batch size decreases as the sequence length increases. However, for a, Font: SFRM1000, Size: 10.061773300170898
Text: classiﬁcation task, since only the last position enters the loss, the batch size itself is held constant. Note that this, Font: SFRM1000, Size: 9.892655372619629
Text: also means that ﬁne-tuning models with longer sequence lengths is more computationally expensive., Font: SFRM1000, Size: 9.962639808654785
Text: Training consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which, Font: SFRM1000, Size: 9.862509727478027
Text: are all independently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then, Font: SFRM1000, Size: 9.917706489562988
Text: uniformly picking a contiguous segment of DNA., Font: SFRM1000, Size: 9.962639808654785
Text: Following (Nguyen, Poli, et al., Font: SFRM1000, Size: 10.061773300170898
Text:  2023, Font: SFRM1000, Size: 10.061773300170898
Text: ), models with a maximum context length greater than, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 14, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 16384, Font: STIXTwoMath, Size: 9.962639808654785
Text:  use, Font: SFRM1000, Size: 10.061773300170898
Text: sequence length warmup with 1 epoch at length, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 14, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 16384, Font: STIXTwoMath, Size: 9.962639808654785
Text: , 1 epoch at length, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 15, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 32768, Font: STIXTwoMath, Size: 9.962639808654785
Text: , 1 epoch at length, Font: SFRM1000, Size: 10.061773300170898
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 16, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 65536, Font: STIXTwoMath, Size: 9.962639808654785
Text: , and so on up to the maximum sequence length. For example, the model with, Font: SFRM1000, Size: 9.927709579467773
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 1048576, Font: STIXTwoMath, Size: 9.962639808654785
Text:  context, Font: SFRM1000, Size: 9.927709579467773
Text: undergoes, Font: SFRM1000, Size: 9.962639808654785
Text:  6, Font: STIXTwoMath, Size: 9.962639808654785
Text:  epochs of sequence length warmup before, Font: SFRM1000, Size: 9.962639808654785
Text:  4, Font: STIXTwoMath, Size: 9.962639808654785
Text:  more epochs at its maximum sequence length., Font: SFRM1000, Size: 9.962639808654785
Text: The learning rate for all Hyena models is, Font: SFRM1000, Size: 10.002410888671875
Text:  ퟺ횎 − ퟻ, Font: STIXTwoMath, Size: 9.962639808654785
Text: , while the learning rate for all Mamba models is, Font: SFRM1000, Size: 10.002410888671875
Text:  ퟷ횎 − ퟺ, Font: STIXTwoMath, Size: 9.962639808654785
Text: . These, Font: SFRM1000, Size: 10.002410888671875
Text: were found by performing learning rate sweeps for each model among, Font: SFRM1000, Size: 10.061773300170898
Text:  {1푒 − 5, 2푒 − 5, 4푒 − 5, 1푒 − 4, 2푒 − 4}, Font: STIXTwoMath, Size: 9.962639808654785
Text:  for, Font: SFRM1000, Size: 10.061773300170898
Text: the smaller sequence lengths, Font: SFRM1000, Size: 10.061773300170898
Text:  (2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 10, Font: STIXTwoMath, Size: 7.471980094909668
Text: , 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 12, Font: STIXTwoMath, Size: 7.471980094909668
Text: , 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 14, Font: STIXTwoMath, Size: 7.471980094909668
Text: , 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 16, Font: STIXTwoMath, Size: 7.471980094909668
Text: ), Font: STIXTwoMath, Size: 9.962639808654785
Text: , and these values were consistently found to be the best for each, Font: SFRM1000, Size: 10.061773300170898
Text: model. An abridged learning rate sweep was done at length, Font: SFRM1000, Size: 9.917706489562988
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 18, Font: STIXTwoMath, Size: 7.471980094909668
Text: , which agreed with these values, and a single run, Font: SFRM1000, Size: 9.917706489562988
Text: at length, Font: SFRM1000, Size: 9.967619895935059
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.967619895935059
Text: was performed (as described above, the computational cost of these experiments is proportional to, Font: SFRM1000, Size: 9.967619895935059
Text: the sequence length). The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear, Font: SFRM1000, Size: 10.061773300170898
Text: warmup to the maximum learning rate, and 5 epochs of cosine decay down to, Font: SFRM1000, Size: 9.87256908416748
Text:  1푒 − 6, Font: STIXTwoMath, Size: 9.962639808654785
Text: . The unusually long learning, Font: SFRM1000, Size: 9.87256908416748
Text: rate warmup schedule was chosen because the sequence length warmup was also long (e.g. comprising 6 out of 10, Font: SFRM1000, Size: 9.912701606750488
Text: epochs for the model with context length, Font: SFRM1000, Size: 9.962639808654785
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text: ); we did not experiment with this choice., Font: SFRM1000, Size: 9.962639808654785
Text: Results for the Species classiﬁcation task are in Table, Font: SFRM1000, Size: 9.962639808654785
Text:  13, Font: SFRM1000, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: E.4, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Audio Details, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: E.4.1, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: YouTubeMix Audio Pretraining, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Model., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We use a model with 3 blocks per stage (, Font: SFRM1000, Size: 10.061773300170898
Text: 3 × 5 = 15, Font: STIXTwoMath, Size: 9.962639808654785
Text:  total Mamba blocks), pooling factor, Font: SFRM1000, Size: 10.061773300170898
Text:  푝 = 16, Font: STIXTwoMath, Size: 9.962639808654785
Text: , and, Font: SFRM1000, Size: 10.061773300170898
Text: outer dimension, Font: SFRM1000, Size: 9.962639808654785
Text:  퐷 = 64, Font: STIXTwoMath, Size: 9.962639808654785
Text: , for about 3.5M parameters., Font: SFRM1000, Size: 9.962639808654785
Text: Dataset., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of, Font: SFRM1000, Size: 10.05190372467041
Text: 256, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: The dataset consists of clips of up to 1 minute long, or length, Font: SFRM1000, Size: 10.061773300170898
Text:  960000, Font: STIXTwoMath, Size: 9.962639808654785
Text: , which is subsampled and divided into, Font: SFRM1000, Size: 10.061773300170898
Text: segments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of, Font: SFRM1000, Size: 10.007370948791504
Text:  16, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.007370948791504
Text: 34, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 14: YouTubeMix length scaling sequence lengths and batch sizes., Font: STIXTwoText, Size: 8.96638011932373
Text: Sequence length, Font: STIXTwoText, Size: 9.962639808654785
Text: Batch size, Font: STIXTwoText, Size: 9.962639808654785
Text: Tokens / batch, Font: STIXTwoText, Size: 9.962639808654785
Text: 468 × 2048 = 958464, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1, Font: STIXTwoMath, Size: 9.962639808654785
Text: 958464, Font: STIXTwoMath, Size: 9.962639808654785
Text: 234 × 2048 = 479232, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 958464, Font: STIXTwoMath, Size: 9.962639808654785
Text: 117 × 2048 = 239616, Font: STIXTwoMath, Size: 9.962639808654785
Text: 4, Font: STIXTwoMath, Size: 9.962639808654785
Text: 958464, Font: STIXTwoMath, Size: 9.962639808654785
Text: 59 × 2048 = 120832, Font: STIXTwoMath, Size: 9.962639808654785
Text: 8, Font: STIXTwoMath, Size: 9.962639808654785
Text: 966656, Font: STIXTwoMath, Size: 9.962639808654785
Text: 30 × 2048 = 61440, Font: STIXTwoMath, Size: 9.962639808654785
Text: 16, Font: STIXTwoMath, Size: 9.962639808654785
Text: 983040, Font: STIXTwoMath, Size: 9.962639808654785
Text: 15 × 2048 = 30720, Font: STIXTwoMath, Size: 9.962639808654785
Text: 32, Font: STIXTwoMath, Size: 9.962639808654785
Text: 983040, Font: STIXTwoMath, Size: 9.962639808654785
Text: 8 × 2048 = 16384, Font: STIXTwoMath, Size: 9.962639808654785
Text: 64, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1048576, Font: STIXTwoMath, Size: 9.962639808654785
Text: 4 × 2048 = 8192, Font: STIXTwoMath, Size: 9.962639808654785
Text: 128, Font: STIXTwoMath, Size: 9.962639808654785
Text: 1048576, Font: STIXTwoMath, Size: 9.962639808654785
Text: 10, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 4, Font: Roboto-Bold, Size: 3.430349826812744
Text: 10, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 5, Font: Roboto-Bold, Size: 3.430349826812744
Text: Sequence Length, Font: Roboto-Bold, Size: 6.236999988555908
Text: 1.25, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.30, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.35, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.40, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.45, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.50, Font: Roboto-Bold, Size: 4.9004998207092285
Text: Bits Per Byte, Font: Roboto-Bold, Size: 6.236999988555908
Text: Audio Waveforms - SSM Parameterization, Font: Roboto-Bold, Size: 6.236999988555908
Text: S4+MLP, Font: Roboto-Bold, Size: 4.9004998207092285
Text: Mamba (S6), Font: Roboto-Bold, Size: 4.9004998207092285
Text: +complex, Font: Roboto-Bold, Size: 4.9004998207092285
Text: -selective B/C, Font: Roboto-Bold, Size: 4.9004998207092285
Text: -selective , Font: Roboto-Bold, Size: 4.9004998207092285
Text:    (Mamba-S4), Font: Roboto-Bold, Size: 4.9004998207092285
Text: 10, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 4, Font: Roboto-Bold, Size: 3.430349826812744
Text: 10, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 5, Font: Roboto-Bold, Size: 3.430349826812744
Text: Sequence Length, Font: Roboto-Bold, Size: 6.236999988555908
Text: 1.25, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.30, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.35, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.40, Font: Roboto-Bold, Size: 4.9004998207092285
Text: 1.45, Font: Roboto-Bold, Size: 4.9004998207092285
Text: Bits Per Byte, Font: Roboto-Bold, Size: 6.236999988555908
Text: Audio Waveforms - SSM Parameterization, Font: Roboto-Bold, Size: 6.236999988555908
Text: Mamba (S6), Font: Roboto-Bold, Size: 4.9004998207092285
Text: +complex, Font: Roboto-Bold, Size: 4.9004998207092285
Text: -selective B/C, Font: Roboto-Bold, Size: 4.9004998207092285
Text: -selective , Font: Roboto-Bold, Size: 4.9004998207092285
Text:    (Mamba-S4), Font: Roboto-Bold, Size: 4.9004998207092285
Text: Figure 10: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Audio Pretraining (YouTubeMix) Ablations, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) As a uniformly-sampled “continuous” signal modality, audio wave-, Font: STIXTwoText, Size: 8.96638011932373
Text: forms actually beneft from LTI models which have matching inductive bias. (, Font: STIXTwoText, Size: 8.96638011932373
Text: Left, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Homogenous models (all blocks have the same, Font: STIXTwoText, Size: 8.96638011932373
Text: parameterization) (, Font: STIXTwoText, Size: 8.96638011932373
Text: Right, Font: STIXTwoText-Italic, Size: 8.96638011932373
Text: ) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as fgure, Font: STIXTwoText, Size: 8.96638011932373
Text: on left., Font: STIXTwoText, Size: 8.96638011932373
Text: and we want the resulting sequence length to be a a multiple of, Font: SFRM1000, Size: 10.061773300170898
Text:  8, Font: STIXTwoMath, Size: 9.962639808654785
Text:  for hardware eﬃciency, the longest possible, Font: SFRM1000, Size: 10.061773300170898
Text: sequence is, Font: SFRM1000, Size: 10.061773300170898
Text:  468 × 2048 = 958464, Font: STIXTwoMath, Size: 9.962639808654785
Text: . The rest of our sequence lengths are deﬁned by successively halving this and, Font: SFRM1000, Size: 10.061773300170898
Text: rounding up to the nearest multiple of, Font: SFRM1000, Size: 9.962639808654785
Text:  2048, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: Table, Font: SFRM1000, Size: 9.867541313171387
Text:  14, Font: SFRM1000, Size: 9.867541313171387
Text:  lists the speciﬁcations used in Figure, Font: SFRM1000, Size: 9.867541313171387
Text:  7, Font: SFRM1000, Size: 9.867541313171387
Text: . Beyond the varying batch sizes, the number of valid segments in, Font: SFRM1000, Size: 9.867541313171387
Text: the training set varied between diﬀerent sequence lengths (e.g. the number of training steps per epoch was not, Font: SFRM1000, Size: 10.046965599060059
Text: constant for diﬀerent points in the graph), which may have contributed to kinks in the scaling curves., Font: SFRM1000, Size: 9.962639808654785
Text: Training., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Models were trained for, Font: SFRM1000, Size: 10.061773300170898
Text:  200퐾, Font: STIXTwoMath, Size: 9.962639808654785
Text:  training steps with a maximum learning rate of, Font: SFRM1000, Size: 10.061773300170898
Text:  0.002, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.061773300170898
Text:  20퐾, Font: STIXTwoMath, Size: 9.962639808654785
Text:  (10%), Font: SFRM1000, Size: 10.061773300170898
Text: warmup steps, and weight decay, Font: SFRM1000, Size: 9.962639808654785
Text:  0.1, Font: STIXTwoMath, Size: 9.962639808654785
Text:  (similar to our general pretraining recipe across domains)., Font: SFRM1000, Size: 9.962639808654785
Text: Additional Ablations: SSM Parameterizations., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We investigate SSM parameterizations on long-form audio, Font: SFRM1000, Size: 10.061773300170898
Text: waveform pretraining in the setting of Figure, Font: SFRM1000, Size: 9.862509727478027
Text:  7, Font: SFRM1000, Size: 9.862509727478027
Text: . The setting is modiﬁed slightly to use larger models (, Font: SFRM1000, Size: 9.862509727478027
Text: 8, Font: STIXTwoMath, Size: 9.962639808654785
Text:  layers and, Font: SFRM1000, Size: 9.862509727478027
Text: 퐷 = 64, Font: STIXTwoMath, Size: 9.962639808654785
Text:  for 6M params, the SaShiMi default), shorter sequences (, Font: SFRM1000, Size: 9.87256908416748
Text: 2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 11, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 2048, Font: STIXTwoMath, Size: 9.962639808654785
Text:  to, Font: SFRM1000, Size: 9.87256908416748
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 18, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 262144, Font: STIXTwoMath, Size: 9.962639808654785
Text:  instead of, Font: SFRM1000, Size: 9.87256908416748
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 13, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.87256908416748
Text: to, Font: SFRM1000, Size: 9.87256908416748
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 20, Font: STIXTwoMath, Size: 7.471980094909668
Text: ),, Font: SFRM1000, Size: 9.87256908416748
Text: lower LR (, Font: SFRM1000, Size: 9.962639808654785
Text: 0.001, Font: STIXTwoMath, Size: 9.962639808654785
Text:  from, Font: SFRM1000, Size: 9.962639808654785
Text:  0.002, Font: STIXTwoMath, Size: 9.962639808654785
Text: ), and shorter training cycles (100K instead of 200K steps)., Font: SFRM1000, Size: 9.962639808654785
Text: Figure, Font: SFRM1000, Size: 9.862509727478027
Text:  10, Font: SFRM1000, Size: 9.862509727478027
Text:  shows that the change from S4, Font: SFRM1000, Size: 9.862509727478027
Text:  →, Font: STIXTwoMath, Size: 9.962639808654785
Text:  S6 (i.e. the selection mechanism) is not always beneﬁcial. On long-form, Font: SFRM1000, Size: 9.862509727478027
Text: audio waveforms, it in fact signiﬁcantly hampers performance, which may be intuitive from the point of view, Font: SFRM1000, Size: 10.061773300170898
Text: that audio is uniformly sampled and very smooth, and therefore beneﬁts from continuous linear time-invariant, Font: SFRM1000, Size: 10.042024612426758
Text: (LTI) methods. After ablating away the selection mechanism, note that the resulting model is the S4 layer, Font: SFRM1000, Size: 10.061773300170898
Text: inside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture, Font: SFRM1000, Size: 10.061773300170898
Text: Mamba-S6., Font: SFRM1000, Size: 9.962639808654785
Text: However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers., Font: SFRM1000, Size: 10.061773300170898
Text: The performance diﬀerences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio, Font: SFRM1000, Size: 9.882617950439453
Text: signal should be LTI, but once they are “tokenized” and compressed by the outer layers, the inner layers no longer, Font: SFRM1000, Size: 9.862509727478027
Text: need to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one., Font: SFRM1000, Size: 9.962639808654785
Text: 35, Font: STIXTwoText, Size: 9.962639808654785
Text: E.4.2, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: SC09 Speech Generation, Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: Autoregressive training largely followed the autoregressive language modeling protocol, such as, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Weight decay, Font: SFRM1000, Size: 9.962639808654785
Text:  0.1, Font: STIXTwoMath, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Learning rate warmup for 10% of total steps, Font: SFRM1000, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  AdamW optimizer with, Font: SFRM1000, Size: 9.962639808654785
Text:  훽 = (0.9, 0.95), Font: STIXTwoMath, Size: 9.962639808654785
Text: •, Font: STIXTwoText, Size: 9.962639808654785
Text:  Gradient clip value, Font: SFRM1000, Size: 9.962639808654785
Text:  0.1, Font: STIXTwoMath, Size: 9.962639808654785
Text: We used a learning rate of, Font: SFRM1000, Size: 9.962639808654785
Text:  0.002, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and, Font: SFRM1000, Size: 9.962639808654785
Text:  200000, Font: STIXTwoMath, Size: 9.962639808654785
Text:  training steps at a batch size of, Font: SFRM1000, Size: 9.962639808654785
Text:  16, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: The large Mamba model in Table, Font: SFRM1000, Size: 9.912701606750488
Text:  4, Font: SFRM1000, Size: 9.912701606750488
Text:  has 15 layers per stage with an outer dimension of, Font: SFRM1000, Size: 9.912701606750488
Text:  퐷 = 96, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and pooling factor, Font: SFRM1000, Size: 9.912701606750488
Text: 4, Font: STIXTwoMath, Size: 9.962639808654785
Text: . We note that this dataset is small (training went through 100 epochs) and for this large model, there was, Font: SFRM1000, Size: 10.061773300170898
Text: signiﬁcant overﬁtting of the BPB or NLL. However, automated metrics of generated samples continually improving, Font: SFRM1000, Size: 9.862509727478027
Text: throughout training., Font: SFRM1000, Size: 9.962639808654785
Text: The models in the architecture ablations in Table, Font: SFRM1000, Size: 9.927709579467773
Text:  5, Font: SFRM1000, Size: 9.927709579467773
Text:  all have 8 layers per stage with an outer dimension of, Font: SFRM1000, Size: 9.927709579467773
Text:  홳 = 64, Font: STIXTwoMath, Size: 9.962639808654785
Text: and pooling factor, Font: SFRM1000, Size: 10.061773300170898
Text:  4, Font: STIXTwoMath, Size: 9.962639808654785
Text: . The S4+MLP block has roughly, Font: SFRM1000, Size: 10.061773300170898
Text:  2퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: + 4퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 10.061773300170898
Text: parameters (expansion factor, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text:  in the MLP)., Font: SFRM1000, Size: 10.061773300170898
Text: The Transformer block has, Font: SFRM1000, Size: 10.056839942932129
Text:  4퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: + 2퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 10.056839942932129
Text: parameters (expansion factor, Font: SFRM1000, Size: 10.056839942932129
Text:  1, Font: STIXTwoMath, Size: 9.962639808654785
Text:  in the MLP). The Mamba block has the, Font: SFRM1000, Size: 10.056839942932129
Text: usual, Font: SFRM1000, Size: 9.962639808654785
Text:  ≈ 6퐷, Font: STIXTwoMath, Size: 9.962639808654785
Text: 2, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: SFRM1000, Size: 9.962639808654785
Text: parameters. All models have roughly 6M total parameters., Font: SFRM1000, Size: 9.962639808654785
Text: E.5, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Efciency Benchmark, Font: STIXTwoText-Bold, Size: 11.955169677734375
Text: Scan Operation., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We compare the core operation of selective SSMs, which is the parallel scan (Section, Font: SFRM1000, Size: 10.061773300170898
Text:  3.3, Font: SFRM1000, Size: 10.061773300170898
Text: ),, Font: SFRM1000, Size: 10.061773300170898
Text: against convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost, Font: SFRM1000, Size: 9.862509727478027
Text: of other operations outside of this core operation, such as computing the convolutional kernel in global-convolution, Font: SFRM1000, Size: 9.862509727478027
Text: models, or computing the QKV projections in attention., Font: SFRM1000, Size: 9.962639808654785
Text: As a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing, Font: SFRM1000, Size: 9.862509727478027
Text: the parameters, Font: SFRM1000, Size: 9.962639808654785
Text:  A, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  B, Font: CMMIB10, Size: 9.962639808654785
Text: ,, Font: STIXTwoMath, Size: 9.962639808654785
Text:  C, Font: CMMIB10, Size: 9.962639808654785
Text:  in HBM., Font: SFRM1000, Size: 9.962639808654785
Text: Our scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all, Font: SFRM1000, Size: 9.917706489562988
Text: the large parameters in HBM., Font: SFRM1000, Size: 9.962639808654785
Text: For convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs, Font: SFRM1000, Size: 9.927709579467773
Text: and the ﬁlters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The, Font: SFRM1000, Size: 10.061773300170898
Text: theoretical complexity is, Font: SFRM1000, Size: 9.962639808654785
Text:  푂(퐿 log(퐿)), Font: STIXTwoMath, Size: 9.962639808654785
Text:  for sequence length, Font: SFRM1000, Size: 9.962639808654785
Text:  퐿, Font: STIXTwoMath, Size: 9.962639808654785
Text: ., Font: SFRM1000, Size: 9.962639808654785
Text: For attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao, Font: SFRM1000, Size: 9.862509727478027
Text:  2023, Font: SFRM1000, Size: 9.862509727478027
Text: )),, Font: SFRM1000, Size: 9.862509727478027
Text: with causal mask. Note that FlashAttention-2 with causal mask is about 1.7, Font: SFRM1000, Size: 10.007370948791504
Text: ×, Font: STIXTwoMath, Size: 9.962639808654785
Text:  faster than without causal mask,, Font: SFRM1000, Size: 10.007370948791504
Text: since approximately only half of the attention entries are computed., Font: SFRM1000, Size: 9.962639808654785
Text: We use batch size of 1 and increase the sequence length from, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 9, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: = 512, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 10, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: ≈ 1퐾, Font: STIXTwoMath, Size: 9.962639808654785
Text: ,, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 11, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: ≈ 2퐾, Font: STIXTwoMath, Size: 9.962639808654785
Text: , up to, Font: SFRM1000, Size: 10.061773300170898
Text:  2, Font: STIXTwoMath, Size: 9.962639808654785
Text: 19, Font: STIXTwoMath, Size: 7.471980094909668
Text:  , Font: STIXTwoMath, Size: 9.962639808654785
Text: ≈ 500퐾, Font: STIXTwoMath, Size: 9.962639808654785
Text: (some of the baselines run out of memory before reaching 500K). We use a model dimension of, Font: SFRM1000, Size: 9.862509727478027
Text:  퐷 = 1024, Font: STIXTwoMath, Size: 9.962639808654785
Text:  and state, Font: SFRM1000, Size: 9.862509727478027
Text: dimension, Font: SFRM1000, Size: 10.061773300170898
Text:  푁 = 16, Font: STIXTwoMath, Size: 9.962639808654785
Text: . We measure with BF16 inputs, which is the data type most commonly used for large scale, Font: SFRM1000, Size: 10.061773300170898
Text: training., Font: SFRM1000, Size: 9.962639808654785
Text: End-to-end Inference., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba, Font: SFRM1000, Size: 9.862509727478027
Text: 6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard, Font: SFRM1000, Size: 10.061773300170898
Text: Transformer implementation in the Huggingface, Font: SFRM1000, Size: 9.962639808654785
Text:  transformers, Font: LMMono10-Regular, Size: 9.962639808654785
Text:  library., Font: SFRM1000, Size: 9.962639808654785
Text: We set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,, Font: SFRM1000, Size: 9.862509727478027
Text: 32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s), Font: SFRM1000, Size: 9.892655372619629
Text: as, Font: SFRM1000, Size: 10.061773300170898
Text:  batch size, Font: SFRM1000, Size: 9.962639808654785
Text:  × 128∕, Font: STIXTwoMath, Size: 9.962639808654785
Text: time taken, Font: SFRM1000, Size: 9.962639808654785
Text: . We repeat the measurements 3 times and take the average. Measurements are, Font: SFRM1000, Size: 10.061773300170898
Text: done on an A100 80GB PCIe GPU., Font: SFRM1000, Size: 9.962639808654785
Text: Memory Benchmark., Font: STIXTwoText-Bold, Size: 9.962639808654785
Text: The memory usage simply scales proportionally to the size of the activation tensors, as, Font: SFRM1000, Size: 9.967619895935059
Text: with most deep sequence models. We report measurements of the training memory requirements of 125M models, Font: SFRM1000, Size: 9.917706489562988
Text: 36, Font: STIXTwoText, Size: 9.962639808654785
Text: Table 15: (, Font: STIXTwoText, Size: 8.96638011932373
Text: Memory benchmark, Font: STIXTwoText-Bold, Size: 8.96638011932373
Text: .) Mamba’s memory footprint is comparable to the most optimized Transformer. Results for 125M, Font: STIXTwoText, Size: 8.96638011932373
Text: models., Font: STIXTwoText, Size: 8.96638011932373
Text: Batch size, Font: STIXTwoText, Size: 9.962639808654785
Text: Transformer (w/ FlashAttention-2), Font: STIXTwoText, Size: 9.962639808654785
Text: Mamba, Font: STIXTwoText, Size: 9.962639808654785
Text: 1, Font: STIXTwoText, Size: 9.962639808654785
Text: 4.6GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 4.8GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 2, Font: STIXTwoText, Size: 9.962639808654785
Text: 5.2GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 5.8GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 4, Font: STIXTwoText, Size: 9.962639808654785
Text: 6.9GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 7.3GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 8, Font: STIXTwoText, Size: 9.962639808654785
Text: 11.5GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 12.3GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 16, Font: STIXTwoText, Size: 9.962639808654785
Text: 20.7GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 23.1GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 32, Font: STIXTwoText, Size: 9.962639808654785
Text: 34.5GB, Font: STIXTwoText, Size: 9.962639808654785
Text: 38.2GB, Font: STIXTwoText, Size: 9.962639808654785
Text: on 1 A100 80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-eﬃcient, Font: SFRM1000, Size: 9.862509727478027
Text: Transformer implementation we are aware of (with kernel fusion from, Font: SFRM1000, Size: 9.862509727478027
Text:  torch.compile, Font: LMMono10-Regular, Size: 9.962639808654785
Text:  and with FlashAttention-2)., Font: SFRM1000, Size: 9.862509727478027
Text: Table, Font: SFRM1000, Size: 9.862509727478027
Text:  15, Font: SFRM1000, Size: 9.862509727478027
Text:  shows that Mamba’s memory requirement is comparable to a similar-sized Transformer with an extremely, Font: SFRM1000, Size: 9.862509727478027
Text: optimized implementation, and we expect further improvement in Mamba’s memory footprint in the future., Font: SFRM1000, Size: 9.862509727478027
Text: 37, Font: STIXTwoText, Size: 9.962639808654785